{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "nlu_jointbert_dl21.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzZzR2-U43r4"
      },
      "source": [
        "# Joint Intent Classification and Slot filling with BERT\n",
        "This notebook is based on the paper __BERT for Joint Intent Classification and Slot Filling__ by Chen et al. (2019), https://arxiv.org/abs/1902.10909 but on a different dataset made for a class project.\n",
        "\n",
        "Ideas were also taken from https://github.com/monologg/JointBERT, which is a PyTorch implementation of the paper with the original dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjRqJ7sS5vsQ"
      },
      "source": [
        "## Install transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OO2HRXgV5HA",
        "outputId": "f2da277a-198d-4efc-d76f-77bd605352c2"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbLDTz7m5_rs"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj_x96qa6C20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b156f43c-de21-48c3-9824-01d6c6119d01"
      },
      "source": [
        "!wget https://github.com/ShawonAshraf/nlu-jointbert-dl2021/raw/main/data/nlu_traindev/train.json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-05 16:11:27--  https://github.com/ShawonAshraf/nlu-jointbert-dl2021/raw/main/data/nlu_traindev/train.json\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ShawonAshraf/nlu-jointbert-dl2021/main/data/nlu_traindev/train.json [following]\n",
            "--2023-12-05 16:11:27--  https://raw.githubusercontent.com/ShawonAshraf/nlu-jointbert-dl2021/main/data/nlu_traindev/train.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5055766 (4.8M) [text/plain]\n",
            "Saving to: ‘train.json.1’\n",
            "\n",
            "train.json.1        100%[===================>]   4.82M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-12-05 16:11:27 (145 MB/s) - ‘train.json.1’ saved [5055766/5055766]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-T_1uc46O6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12cb464e-dbe4-4370-d268-0dd85fbbe415"
      },
      "source": [
        "!wget https://github.com/ShawonAshraf/nlu-jointbert-dl2021/raw/main/data/nlu_traindev/dev.json"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-05 16:11:27--  https://github.com/ShawonAshraf/nlu-jointbert-dl2021/raw/main/data/nlu_traindev/dev.json\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ShawonAshraf/nlu-jointbert-dl2021/main/data/nlu_traindev/dev.json [following]\n",
            "--2023-12-05 16:11:28--  https://raw.githubusercontent.com/ShawonAshraf/nlu-jointbert-dl2021/main/data/nlu_traindev/dev.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248459 (243K) [text/plain]\n",
            "Saving to: ‘dev.json.1’\n",
            "\n",
            "\rdev.json.1            0%[                    ]       0  --.-KB/s               \rdev.json.1          100%[===================>] 242.64K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-12-05 16:11:28 (18.7 MB/s) - ‘dev.json.1’ saved [248459/248459]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connection à drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "v7MXPT0AnVx1",
        "outputId": "78b89377-8a13-4a8d-ed1a-aed8dc616dbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpJIziCh6oy8"
      },
      "source": [
        "## Read data from json files\n",
        "\n",
        "Data is of the following format\n",
        "````json5\n",
        "{\n",
        "  \"text\": \"\",\n",
        "  \"positions\": [{}],\n",
        "  \"slots\": [{}],\n",
        "  \"intent\": \"\"\n",
        "}\n",
        "````\n",
        "\n",
        "We will be using `text` as the input and `slots` and `intent` as lables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqYLzmWhVT84"
      },
      "source": [
        "# import json\n",
        "# import os\n",
        "\n",
        "# class RawData(object):\n",
        "#     def __init__(self, id, intent, positions, slots, text):\n",
        "#         self.id = id\n",
        "#         self.intent = intent\n",
        "#         self.positions = positions\n",
        "#         self.slots = slots\n",
        "#         self.text = text\n",
        "\n",
        "#     def __repr__(self):\n",
        "#         return str(json.dumps(self.__dict__, indent=2))\n",
        "\n",
        "\n",
        "# \"\"\"\n",
        "# reads json from data file\n",
        "# returns a list containing DataInstance objects\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "# def read_train_json_file(filename):\n",
        "#     if os.path.exists(filename):\n",
        "#         intents = []\n",
        "\n",
        "#         with open(filename, \"r\", encoding=\"utf-8\") as json_file:\n",
        "#             data = json.load(json_file)\n",
        "\n",
        "#             for k in data.keys():\n",
        "#                 intent = data[k][\"intent\"]\n",
        "#                 positions = data[k][\"positions\"]\n",
        "#                 slots = data[k][\"slots\"]\n",
        "#                 text = data[k][\"text\"]\n",
        "\n",
        "#                 temp = RawData(k, intent, positions, slots, text)\n",
        "#                 intents.append(temp)\n",
        "\n",
        "#         return intents\n",
        "#     else:\n",
        "#         raise FileNotFoundError(\"No file found with that path!\")\n",
        "\n",
        "# # read from json file\n",
        "# train_data = read_train_json_file(\"train.json\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "train_data_path = '/content/drive/MyDrive/ULaval/dev_examples.json'\n",
        "new_exemples_path = '/content/drive/MyDrive/ULaval/new_examples.json'\n",
        "test_data_path = '/content/drive/MyDrive/ULaval/test_examples.json'\n",
        "\n",
        "\n",
        "def load_incident_dataset(filename):\n",
        "    with open(filename, 'r') as fp:\n",
        "        incident_list = json.load(fp)\n",
        "\n",
        "    return incident_list\n",
        "\n",
        "# Load datasets\n",
        "train_data = load_incident_dataset(train_data_path)\n",
        "new_examples = load_incident_dataset(new_exemples_path)\n",
        "test_data_path = load_incident_dataset(test_data_path)"
      ],
      "metadata": {
        "id": "1aSV641hnj5m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgnD-4G3VT84",
        "outputId": "da5186ea-236e-4513-bfd1-7bcb7c912224"
      },
      "source": [
        "example = train_data[0]\n",
        "example"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ' At around 10:00 p.m. on November 10  2013  Employee #1  with Villager  Construction Inc.  with a coworker  were using an asphalt milling machine  (Wirtgen; Model Number: W2100) to grind out existing asphalt from an  interstate at a railroad bridge overpass. Employee # 1 was standing on the  ground  checking the depth of the cut into the asphalt  using a handheld  pendant attached to the machine. The pedant could stretch out from ten to 15  ft. This allowed Employee #1 to walk back and forth  checking the cut. The  operator was on the top of the milling machine  controlling the operation of  the machine and ensuring that the milling machine and dump truck (driven by a  second coworker  who worked for an independent trucking service) kept a safe  working distance. A different company  Protective Services Inc. (PSI)  was  responsible for the traffic control of the job site and had shut down the  inside lane of a three lane section of the interstate  so that work could be  conducted on that lane. The entire work zone was approximately two miles long   from start to finish. Employee #1 and the operator of the milling machine had  completed milling four sections (eight total passes) of the inside lane at the  bridge overpasses and were waiting for PSI to shut down the center lane. Dual  lane shut down of the inside and center lanes of the interstate was completed  around 9:30 p.m.  and Employee #1 and the milling machine operator milled two  sections (four total passes) of the center lane. Once both sides of the  overpass were milled out  approximately 200 ft on each side  Employee #1 and  the operator of the milling machine moved the milling machine down the  interstate  approximately1 000 ft  to a railroad overpass and began setting up  to mill the center lane sections. The truck driver backed his truck into  position and remained in the truck to move the truck slowly forward as milling  took place. Employee #1 was positioned between the milling machine and the  concrete median dividers  inside the coned off work zone. The lanes of travel  were approximately 12 ft wide  so the milling machine made two passes  since  it can only cut seven ft wide on each section to cover the entire lane.  Employee #1 was standing approximately three ft in the far inside lane on the  ground between milling machine and interior median wall inside of the approved  traffic control set up  and approximately midway up the machine and 17 ft from  the traffic control devices and flow of traffic. The milling machine was  approximately nine ft wide by 50 ft long  while operating. Employee #1 was  guarded by the machine from the flow of traffic. Approximately five to ten  minutes into the first pass  the milling machine operator noticed lights  hitting the reflectors on the inside wall and turned briefly to see a vehicle  coming. The operator thought it was the Project manager coming to check on the  status of the project. Then  the operator realized that the oncoming vehicle  was not equipped with a strobe  as required in work zones. The operator turned  and yelled for Employee # 1 to run for safety  as a Chevrolet Tahoe came down  the inside lane where Employee #1 was standing. The driver of the Tahoe  continued traveling in the far inside lane of the work zone  where Employee #1  was struck and thrown some 100 ft from where he was originally standing. The  vehicle was moving approximately 45 mph per hour. As he was transferred to a  hospital by emergency personnel  Employee #1 was treated for severe trauma   lacerations  fractures  and contusions to the body and head. Employee #1 was  pronounced dead at the hospital. The driver of the vehicle disregarded the  traffic control set up  all warning lights on the rear of the milling machine   and cone spacing of 100 ft. The construction work zone was set up correctly  with all signage  cone spacing  tapering  attenuators  and lighting; all of  the traffic control set up was approved by MUTCD for this type of tr ',\n",
              " 'arguments': {'EVENT': ['Employee #1  was struck and thrown'],\n",
              "  'ACTIVITY': ['checking the depth of the cut into the asphalt',\n",
              "   'grind out existing asphalt from an  interstate at a railroad bridge overpass'],\n",
              "  'WHO': ['Employee #1', 'Employee #1  with Villager  Construction Inc.'],\n",
              "  'WHERE': ['railroad bridge overpass'],\n",
              "  'WHEN': ['November 10  2013'],\n",
              "  'CAUSE': ['The driver of the Tahoe  continued traveling in the far inside lane of the work zone'],\n",
              "  'EQUIPMENT': ['asphalt milling machine',\n",
              "   'Wirtgen; Model Number: W2100',\n",
              "   'handheld  pendant',\n",
              "   'a Chevrolet Tahoe'],\n",
              "  'INJURY': ['severe trauma   lacerations  fractures  and contusions'],\n",
              "  'INJURED': ['Employee #1'],\n",
              "  'BODY-PARTS': ['body and head'],\n",
              "  'DEATH': ['Employee #1']}}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_VahZ-O7JXU"
      },
      "source": [
        "## Load Tokenizer from transformers\n",
        "\n",
        "We will use a pretrained bert model `bert-base-cased` for both Tokenizer and our classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA8sh4pLVT84"
      },
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQFs80Rg7jj4"
      },
      "source": [
        "# Encode texts from the dataset\n",
        "\n",
        "We have to encode the texts using the tokenizer to create tensors for training the classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpUnhqA2VT84",
        "outputId": "db54777b-9c1e-4b94-f1df-5321751b53ee"
      },
      "source": [
        "# https://huggingface.co/transformers/preprocessing.html\n",
        "\n",
        "def encode_texts(tokenizer, texts):\n",
        "    return tokenizer(texts, padding=True, truncation=True, return_tensors=\"tf\", max_length=512)\n",
        "\n",
        "texts = [d[\"text\"] for d in train_data]\n",
        "tds = encode_texts(tokenizer, texts)\n",
        "tds.keys()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjaIbB7vVT84"
      },
      "source": [
        "encoded_texts = tds\n",
        "\n",
        "for t in encoded_texts[\"input_ids\"]:\n",
        "  if t.shape != (512,):\n",
        "    print(t.shape)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8iBMHa577iQ"
      },
      "source": [
        "## Encode labels\n",
        "### Intents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "P1eTfXPQVT84"
      },
      "source": [
        "\n",
        "# intents = [d.intent for d in train_data]\n",
        "# intent_names = list(set(intents))\n",
        "# intent_names"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDCv98XHVT84"
      },
      "source": [
        "# intent_map = dict() # index -> intent\n",
        "# for idx, ui in enumerate(intent_names):\n",
        "#     intent_map[ui] = idx\n",
        "# intent_map"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-PxXNWcVT84"
      },
      "source": [
        "# # map to train_data values\n",
        "# def encode_intents(intents, intent_map):\n",
        "#     encoded = []\n",
        "#     for i in intents:\n",
        "#         encoded.append(intent_map[i])\n",
        "#     # convert to tf tensor\n",
        "#     return tf.convert_to_tensor(encoded, dtype=\"int32\")\n",
        "\n",
        "# encoded_intents = encode_intents(intents, intent_map)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j0i00AW8gjA"
      },
      "source": [
        "### Slots\n",
        "\n",
        "To padd all the texts to the same length, the tokenizer will use special characters. To handle those we need to add <PAD> to slots_names. It can be some other symbol as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNW91LSgVT84",
        "outputId": "6aefebe9-a391-4dc8-b089-cf0c527f269a"
      },
      "source": [
        "# encode slots\n",
        "slot_names = set()\n",
        "for td in train_data:\n",
        "    slots = td[\"arguments\"]\n",
        "    for slot in slots:\n",
        "        slot_names.add(slot)\n",
        "slot_names = list(slot_names)\n",
        "slot_names.insert(0, \"<PAD>\")\n",
        "slot_names"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<PAD>',\n",
              " 'WHEN',\n",
              " 'DEATH',\n",
              " 'EVENT',\n",
              " 'ACTIVITY',\n",
              " 'WHERE',\n",
              " 'CAUSE',\n",
              " 'SUBSTANCE',\n",
              " 'INJURED',\n",
              " 'EQUIPMENT',\n",
              " 'BODY-PARTS',\n",
              " 'WHO',\n",
              " 'INJURY']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Fv7aVEoVT84",
        "outputId": "58ef24f7-2eed-4047-8548-419523298ee0"
      },
      "source": [
        "slot_map = dict() # slot -> index\n",
        "for idx, us in enumerate(slot_names):\n",
        "    slot_map[us] = idx\n",
        "slot_map"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<PAD>': 0,\n",
              " 'WHEN': 1,\n",
              " 'DEATH': 2,\n",
              " 'EVENT': 3,\n",
              " 'ACTIVITY': 4,\n",
              " 'WHERE': 5,\n",
              " 'CAUSE': 6,\n",
              " 'SUBSTANCE': 7,\n",
              " 'INJURED': 8,\n",
              " 'EQUIPMENT': 9,\n",
              " 'BODY-PARTS': 10,\n",
              " 'WHO': 11,\n",
              " 'INJURY': 12}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62lsuEZoVT84",
        "outputId": "213293d0-8a31-447e-af36-e88273b53fc5"
      },
      "source": [
        "# gets slot name from its values\n",
        "def get_slot_from_word(word, slot_dict):\n",
        "    for slot_label, value in slot_dict.items():\n",
        "        for slot_element in value:\n",
        "          if word in slot_element.split():\n",
        "              return slot_label\n",
        "    return None\n",
        "\n",
        "print(train_data[0][\"text\"])\n",
        "print(train_data[0][\"arguments\"])\n",
        "print(\"slot_name for struck is : \", get_slot_from_word(\"struck\", train_data[0][\"arguments\"]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " At around 10:00 p.m. on November 10  2013  Employee #1  with Villager  Construction Inc.  with a coworker  were using an asphalt milling machine  (Wirtgen; Model Number: W2100) to grind out existing asphalt from an  interstate at a railroad bridge overpass. Employee # 1 was standing on the  ground  checking the depth of the cut into the asphalt  using a handheld  pendant attached to the machine. The pedant could stretch out from ten to 15  ft. This allowed Employee #1 to walk back and forth  checking the cut. The  operator was on the top of the milling machine  controlling the operation of  the machine and ensuring that the milling machine and dump truck (driven by a  second coworker  who worked for an independent trucking service) kept a safe  working distance. A different company  Protective Services Inc. (PSI)  was  responsible for the traffic control of the job site and had shut down the  inside lane of a three lane section of the interstate  so that work could be  conducted on that lane. The entire work zone was approximately two miles long   from start to finish. Employee #1 and the operator of the milling machine had  completed milling four sections (eight total passes) of the inside lane at the  bridge overpasses and were waiting for PSI to shut down the center lane. Dual  lane shut down of the inside and center lanes of the interstate was completed  around 9:30 p.m.  and Employee #1 and the milling machine operator milled two  sections (four total passes) of the center lane. Once both sides of the  overpass were milled out  approximately 200 ft on each side  Employee #1 and  the operator of the milling machine moved the milling machine down the  interstate  approximately1 000 ft  to a railroad overpass and began setting up  to mill the center lane sections. The truck driver backed his truck into  position and remained in the truck to move the truck slowly forward as milling  took place. Employee #1 was positioned between the milling machine and the  concrete median dividers  inside the coned off work zone. The lanes of travel  were approximately 12 ft wide  so the milling machine made two passes  since  it can only cut seven ft wide on each section to cover the entire lane.  Employee #1 was standing approximately three ft in the far inside lane on the  ground between milling machine and interior median wall inside of the approved  traffic control set up  and approximately midway up the machine and 17 ft from  the traffic control devices and flow of traffic. The milling machine was  approximately nine ft wide by 50 ft long  while operating. Employee #1 was  guarded by the machine from the flow of traffic. Approximately five to ten  minutes into the first pass  the milling machine operator noticed lights  hitting the reflectors on the inside wall and turned briefly to see a vehicle  coming. The operator thought it was the Project manager coming to check on the  status of the project. Then  the operator realized that the oncoming vehicle  was not equipped with a strobe  as required in work zones. The operator turned  and yelled for Employee # 1 to run for safety  as a Chevrolet Tahoe came down  the inside lane where Employee #1 was standing. The driver of the Tahoe  continued traveling in the far inside lane of the work zone  where Employee #1  was struck and thrown some 100 ft from where he was originally standing. The  vehicle was moving approximately 45 mph per hour. As he was transferred to a  hospital by emergency personnel  Employee #1 was treated for severe trauma   lacerations  fractures  and contusions to the body and head. Employee #1 was  pronounced dead at the hospital. The driver of the vehicle disregarded the  traffic control set up  all warning lights on the rear of the milling machine   and cone spacing of 100 ft. The construction work zone was set up correctly  with all signage  cone spacing  tapering  attenuators  and lighting; all of  the traffic control set up was approved by MUTCD for this type of tr \n",
            "{'EVENT': ['Employee #1  was struck and thrown'], 'ACTIVITY': ['checking the depth of the cut into the asphalt', 'grind out existing asphalt from an  interstate at a railroad bridge overpass'], 'WHO': ['Employee #1', 'Employee #1  with Villager  Construction Inc.'], 'WHERE': ['railroad bridge overpass'], 'WHEN': ['November 10  2013'], 'CAUSE': ['The driver of the Tahoe  continued traveling in the far inside lane of the work zone'], 'EQUIPMENT': ['asphalt milling machine', 'Wirtgen; Model Number: W2100', 'handheld  pendant', 'a Chevrolet Tahoe'], 'INJURY': ['severe trauma   lacerations  fractures  and contusions'], 'INJURED': ['Employee #1'], 'BODY-PARTS': ['body and head'], 'DEATH': ['Employee #1']}\n",
            "slot_name for struck is :  EVENT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VUnAC8-VT84"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# find the max encoded test length\n",
        "# tokenizer pads all texts to same length anyway so\n",
        "# just get the length of the first one's input_ids\n",
        "max_len = len(encoded_texts[\"input_ids\"][0])\n",
        "\n",
        "def encode_slots(all_slots, all_texts, tokenizer, slot_map, max_len=512):\n",
        "    encoded_slots = np.zeros(shape=(len(all_texts), max_len), dtype=np.int32)\n",
        "\n",
        "    for idx, text in enumerate(all_texts):\n",
        "        enc = [] # for this idx, to be added at the end to encoded_slots\n",
        "        bert_token_count = 0  # Track the number of BERT tokens\n",
        "\n",
        "        raw_tokens = text.split()\n",
        "        for rt in raw_tokens:\n",
        "            bert_tokens = tokenizer.tokenize(rt)\n",
        "            bert_token_count += len(bert_tokens)\n",
        "\n",
        "            if bert_token_count > max_len - 2:  # Account for [CLS] and [SEP]\n",
        "                break  # Stop processing if max length is reached\n",
        "\n",
        "            rt_slot_name = get_slot_from_word(rt, all_slots[idx])\n",
        "            if rt_slot_name is not None:\n",
        "                enc.extend([slot_map[rt_slot_name]] * len(bert_tokens))\n",
        "            else:\n",
        "                enc.extend([0] * len(bert_tokens))\n",
        "\n",
        "        # Truncate or pad the enc to fit into encoded_slots\n",
        "        enc = enc[:max_len - 2]  # Truncate if necessary\n",
        "        enc_length = len(enc)\n",
        "        if enc_length < max_len - 2:\n",
        "            enc.extend([0] * (max_len - 2 - enc_length))  # Pad with zeros if shorter\n",
        "\n",
        "        encoded_slots[idx, 1:len(enc) + 1] = enc\n",
        "\n",
        "    return encoded_slots\n",
        "\n",
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1Hj-St4VT84",
        "outputId": "24f00526-5917-44b9-ea79-a8fe3e3e25d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "all_slots = [td[\"arguments\"] for td in train_data]\n",
        "all_texts = [td[\"text\"] for td in train_data]\n",
        "\n",
        "print(len(all_slots))\n",
        "print(len(all_texts))\n",
        "print(slot_map)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "100\n",
            "{'<PAD>': 0, 'WHEN': 1, 'DEATH': 2, 'EVENT': 3, 'ACTIVITY': 4, 'WHERE': 5, 'CAUSE': 6, 'SUBSTANCE': 7, 'INJURED': 8, 'EQUIPMENT': 9, 'BODY-PARTS': 10, 'WHO': 11, 'INJURY': 12}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIEsMzT8VT84"
      },
      "source": [
        "encoded_slots = encode_slots(all_slots, all_texts, tokenizer, slot_map)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw9aA2_gVT84",
        "outputId": "a5d8bdde-25ff-4cf8-9089-a88e89bbbbd6"
      },
      "source": [
        "encoded_slots[0]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  3,  3,  3,\n",
              "        3,  3,  3, 11, 11, 11, 11, 11, 11, 11,  4,  0,  0,  0,  0,  0,  4,\n",
              "        4,  9,  9,  0,  0,  0,  0,  0,  9,  9,  9,  0,  0,  0,  0,  0,  4,\n",
              "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  0,  0,  0,  3,  3,  3,\n",
              "        3,  0,  0,  3,  0,  0,  4,  0,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
              "        0,  4,  9,  9,  9,  0,  0,  4,  0,  0,  6,  0,  0,  0,  0,  0,  4,\n",
              "        4,  0,  0,  0,  0,  0,  0,  0,  3,  3,  3,  3,  3,  3,  0,  0,  0,\n",
              "        3,  0,  4,  4,  0,  0,  6,  0,  3,  0,  4,  0,  4,  4,  9,  9,  0,\n",
              "        4,  0,  4,  4,  9,  3,  0,  0,  4,  9,  9,  3,  0,  0,  0,  0,  0,\n",
              "        4,  0,  0,  0,  0,  0,  0,  0,  4,  0,  0,  0,  0,  0,  0,  4,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 11, 11,  0,  0,  0,  0,  3,\n",
              "        0,  0,  4,  0,  0,  4,  4,  0,  0,  3,  0,  0,  0,  4,  6,  6,  4,\n",
              "        4,  0,  6,  0,  4,  4,  4,  0,  0,  6,  0,  0,  0,  0,  0,  0,  0,\n",
              "        6,  0,  6,  6,  3,  0,  0,  0,  0,  4,  0,  0,  0,  0,  3,  3,  3,\n",
              "        3,  3,  3,  3,  4,  0,  4,  4,  9,  9,  0,  0,  9,  0,  0,  0,  0,\n",
              "        0,  0,  0,  4,  4,  6,  6,  4,  4,  4,  0,  0,  0,  3,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  4,  0,  0,  0,  0,  6,  0,  0,  4,  4,  6,  3,\n",
              "        0,  0,  4,  4,  4,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  3,\n",
              "        3,  3,  3,  3,  3,  3,  4,  9,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  4,  4,  0,  0,  0,  0,  0,  0,  4,  4,  4,  4,  0,  0,  0,\n",
              "        4,  0,  0,  0,  0,  0,  0,  3,  3,  3,  3,  3,  3,  3,  4,  0,  4,\n",
              "        4,  9,  9,  0,  4,  9,  9,  0,  4,  4,  0,  0,  0,  0,  0,  4,  4,\n",
              "        4,  4,  3,  0,  0,  0,  0,  0,  4,  0,  6,  0,  0,  6,  0,  6,  0,\n",
              "        0,  0,  4,  0,  3,  0,  6,  4,  0,  0,  0,  4,  0,  0,  0,  0,  9,\n",
              "        0,  0,  0,  3,  3,  3,  3,  3,  3,  3,  0,  0,  4,  9,  9,  3,  4,\n",
              "        0,  0,  0,  0,  6,  4,  0,  0,  0,  6,  0,  0,  6,  0,  4,  0,  0,\n",
              "        0,  0,  0,  0,  0,  4,  9,  9,  0,  0,  0,  0,  0,  0,  0,  4,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  4,  0,  0,  0,  3,  3,  3,  3,  3,  3,\n",
              "        3,  0,  0,  0,  0,  6,  4,  6,  6,  6,  0,  4,  0,  0,  9,  9,  3,\n",
              "        0,  0,  0,  6,  4,  4,  0,  0,  0,  0,  0,  3,  0,  0,  0,  4,  9,\n",
              "        3,  0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Oahd4cP90ms"
      },
      "source": [
        "## Classifier Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8zokKeJ-RGI"
      },
      "source": [
        "### Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcmMMQaLVT84"
      },
      "source": [
        "from transformers import TFBertModel\n",
        "from tensorflow.keras.layers import Dropout, Dense, GlobalAveragePooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "\n",
        "class JointIntentAndSlotFillingModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, slot_num_labels=None,\n",
        "                 model_name=model_name, dropout_prob=0.1):\n",
        "        super().__init__(name=\"joint_intent_slot\")\n",
        "        self.bert = TFBertModel.from_pretrained(model_name)\n",
        "        self.dropout = Dropout(dropout_prob)\n",
        "        self.slot_classifier = Dense(slot_num_labels,\n",
        "                                     name=\"slot_classifier\")\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        # two outputs from BERT\n",
        "        trained_bert = self.bert(inputs, **kwargs)\n",
        "        sequence_output = trained_bert.last_hidden_state\n",
        "\n",
        "        # sequence_output will be used for slot_filling / classification\n",
        "        sequence_output = self.dropout(sequence_output,\n",
        "                                       training=kwargs.get(\"training\", False))\n",
        "        slot_logits = self.slot_classifier(sequence_output)\n",
        "\n",
        "        return slot_logits"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMpHQUuwVT84",
        "outputId": "c2e98d3b-d338-43a1-acb7-385f5059dda3"
      },
      "source": [
        "joint_model = JointIntentAndSlotFillingModel(slot_num_labels=len(slot_map))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaI_qVxI-Xo4"
      },
      "source": [
        "### Hyperparams, Optimizer and Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMZeZf3JVT84"
      },
      "source": [
        "# Configure the optimizer\n",
        "opt = Adam(learning_rate=3e-5, epsilon=1e-08)\n",
        "\n",
        "# Since the model only outputs slots, use one loss function and one metric\n",
        "loss = SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = SparseCategoricalAccuracy(\"accuracy\")\n",
        "\n",
        "# Compile the model\n",
        "joint_model.compile(optimizer=opt, loss=loss, metrics=[metric])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogJCgyMG-o4A"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uae9vd77VT84",
        "outputId": "bd83fa5a-3177-49f2-8f90-da833c100d3c"
      },
      "source": [
        "x = {\n",
        "    \"input_ids\": encoded_texts[\"input_ids\"],\n",
        "    \"token_type_ids\": encoded_texts[\"token_type_ids\"],\n",
        "    \"attention_mask\": encoded_texts[\"attention_mask\"]\n",
        "}\n",
        "\n",
        "history = joint_model.fit(\n",
        "    x,\n",
        "    encoded_slots,  # Target slot labels\n",
        "    epochs=2,\n",
        "    batch_size=8,\n",
        "    shuffle=True\n",
        ")\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 49s 314ms/step - loss: 0.9390 - accuracy: 0.7811\n",
            "Epoch 2/2\n",
            "13/13 [==============================] - 4s 312ms/step - loss: 0.4954 - accuracy: 0.8355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOSkQ6mZ-sMg"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZqMiC_gZM5s"
      },
      "source": [
        "def nlu(text, tokenizer, model, slot_names):\n",
        "    inputs = tf.constant(tokenizer.encode(text))[None, :]  # batch_size = 1\n",
        "    outputs = model(inputs)\n",
        "    slot_logits = outputs\n",
        "\n",
        "    slot_ids = slot_logits.numpy().argmax(axis=-1)[0, :]\n",
        "\n",
        "    info = {\"slots\": {}}\n",
        "\n",
        "    out_dict = {}\n",
        "    # get all slot names and add to out_dict as keys\n",
        "    predicted_slots = set([slot_names[s] for s in slot_ids if s != 0])\n",
        "    for ps in predicted_slots:\n",
        "      out_dict[ps] = []\n",
        "\n",
        "    # check if the text starts with a small letter\n",
        "    if text[0].islower():\n",
        "      tokens = tokenizer.tokenize(text, add_special_tokens=True)\n",
        "    else:\n",
        "      tokens = tokenizer.tokenize(text)\n",
        "    for token, slot_id in zip(tokens, slot_ids):\n",
        "        # add all to out_dict\n",
        "        slot_name = slot_names[slot_id]\n",
        "\n",
        "        if slot_name == \"<PAD>\":\n",
        "            continue\n",
        "\n",
        "        # collect tokens\n",
        "        collected_tokens = [token]\n",
        "        idx = tokens.index(token)\n",
        "\n",
        "        # see if it starts with ##\n",
        "        # then it belongs to the previous token\n",
        "        if token.startswith(\"##\"):\n",
        "          # check if the token already exists or not\n",
        "          if tokens[idx - 1] not in out_dict[slot_name]:\n",
        "            collected_tokens.insert(0, tokens[idx - 1])\n",
        "\n",
        "        # add collected tokens to slots\n",
        "        out_dict[slot_name].extend(collected_tokens)\n",
        "\n",
        "    # process out_dict\n",
        "    for slot_name in out_dict:\n",
        "        tokens = out_dict[slot_name]\n",
        "        slot_value = tokenizer.convert_tokens_to_string(tokens)\n",
        "\n",
        "        info[\"slots\"][slot_name] = slot_value.strip()\n",
        "\n",
        "    return info\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzuZBVpu_K9Q",
        "outputId": "160e1224-b534-48a6-a76a-dbd68aa92ce7"
      },
      "source": [
        "nlu(\"On April 5  2010  an employee and a coworker of a utility contractor were  involved with the replacement of natural gas line risers at single family  homes. A 3-ft deep hole was hand dug  approximately 18-in. in diameter  to  access the main 1-in. gas line. A footage squeeze tool was clamped onto the  1-in. main gas line and the old riser assembly was removed. During the process  of installing the new riser  the clamp was removed causing the flow of natural  gas to enter the excavated hole. The employee was found by the coworker face  down in the hole overcome by the gas. The employee was killed.\", tokenizer, joint_model, slot_names)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'slots': {'ACTIVITY': 'the replacement of natural risers approximately main old new c excavated hole',\n",
              "  'WHO': 'employee coworker',\n",
              "  'WHEN': '5 2010',\n",
              "  'EVENT': '1'}}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXSXyLFV_fAE",
        "outputId": "fcf3bc8b-1437-4a98-f73b-fde9d7139c40"
      },
      "source": [
        "nlu(\"add Brian May to my Reggae Infusions list\", tokenizer, joint_model, slot_names)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'slots': {'EVENT': 'Regga'}}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs6x7wmDVT84"
      },
      "source": [
        "import calendar\n",
        "import time\n",
        "\n",
        "# to generate timestamps for prediction file\n",
        "def get_time_stamp():\n",
        "    ts = calendar.timegm(time.gmtime())\n",
        "    return ts\n",
        "\n",
        "get_time_stamp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWCPwzLP_rQc"
      },
      "source": [
        "## Generate prediction.json\n",
        "\n",
        "This section creates a file containing all the prediction results for inputs from dev.json"
      ]
    }
  ]
}