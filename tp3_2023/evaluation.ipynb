{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "638f4af0",
   "metadata": {},
   "source": [
    "# Du code pour vous aider à faire une évaluation automatique\n",
    "\n",
    "Si cela peut vous être utile, du code pour estimer les valeurs de F1 et de correspondance exacte (*exact match*) entre une réponse et son texte de référence. Les fonctions sont empruntées du code accompagnant le jeu de données SQuAD. \n",
    "\n",
    "À noter que cette évaluation repose sur la correspondance entre des sous-chaînes de caractères entre 2 textes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d38a24",
   "metadata": {},
   "source": [
    "## 1. Les fonctions de base pour comparer 2 passages de textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb1f4c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Mettre en minuscule et retirer la ponctuation, des déterminants and les espaces.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    \"\"\"Normalise les 2 textes, trouve ce qu'il y a en comment et estime précision, rappel et F1.\"\"\"\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if len(ground_truth_tokens) == 0 or len(prediction_tokens) == 0:\n",
    "        return int(ground_truth_tokens == prediction_tokens)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth): \n",
    "    \"\"\"Vérifie si les 2 textes sont quasi-identiques.\"\"\"\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    \"\"\"La fonction princiaple. Important de noter que ground_truths est une liste \n",
    "       parce qu'il peut y avoir plusieurs réponses possibles.\"\"\"\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47702494",
   "metadata": {},
   "source": [
    "## 2. Quelques exemples pour illustrer l'évaluation avec des passages de texte: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c59445de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_demo(prediction, ground_truths): \n",
    "    \"\"\"Fonction utilitaire pour illuster l'utilisation de metric_max_over_ground_truths.\n",
    "       Vous pouvez créer votre propre fonction selon vos besoins. \"\"\"\n",
    "    exact_match = metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\n",
    "    f1_value = metric_max_over_ground_truths(f1_score, prediction, ground_truths)\n",
    "    print('Exact match:', exact_match, '\\nF1:', f1_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39986220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match: True \n",
      "F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "prediction = \"his left foot was struck by and run over by a loader\"\n",
    "ground_truths = [\"his left foot was struck by and run over by a loader\"]\n",
    "evaluate_demo(prediction, ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "703d1497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match: True \n",
      "F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "prediction = \"his left foot was struck by and run over by a loader\"\n",
    "ground_truths = [\"his right foot was not struck by anything\", \"his left foot was struck by and run over by a loader\"]\n",
    "evaluate_demo(prediction, ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "489f0979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match: False \n",
      "F1: 0.7058823529411764\n"
     ]
    }
   ],
   "source": [
    "prediction = \"his left foot was struck by\"\n",
    "ground_truths = [\"his left foot was struck by and run over by a loader\"]\n",
    "evaluate_demo(prediction, ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb80777a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match: False \n",
      "F1: 0\n"
     ]
    }
   ],
   "source": [
    "prediction = \"\"\n",
    "ground_truths = [\"his left foot was struck by and run over by a loader\"]\n",
    "evaluate_demo(prediction, ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f716744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match: True \n",
      "F1: 1\n"
     ]
    }
   ],
   "source": [
    "prediction = \"\"\n",
    "ground_truths = [\"\"]\n",
    "evaluate_demo(prediction, ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf91f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
