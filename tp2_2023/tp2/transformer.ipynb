{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tâche #3 : Classification d'incidents avec des modèles *Transformers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toujours avec la même tâche et les mêmes fichiers de textes, utiliser la librairie HuggingFace pour accomplir cette tâche. On demande plus spécifiquement d’utiliser le modèle bert-base-uncased et un autre modèle de votre choix. \n",
    "Les consignes associées à cette tâche sont: \n",
    "-\tNom du notebook : transformer.ipynb\n",
    "-\tTokenisation : Celle fournie par les tokeniseurs accompagnant les modèles transformers. \n",
    "-\tPlongements de mots : Ceux du modèle transformer. \n",
    "-\tNormalisation : Lettre en minuscule pour Bert. Aucune contrainte pour le 2e modèle. \n",
    "-\tChoix du 2e transformer: Un modèle encodeur préentraîné pour l’anglais. Le modèle ne doit pas être une autre version de Bert et doit être significativement différent. Utilisez un 2 fichier pour ce modèle si nécessaire (une copie de celui-ci). \n",
    "-\tAnalyse : Comparer les résultats obtenus avec les 2 modèles transformers. Présentez également une comparaison globale des résultats obtenus avec tous les modèles utilisés dans ce travail et ceux du travail précédent (TP #1). \n",
    "\n",
    "\n",
    "Vous pouvez ajouter au *notebook* toutes les cellules dont vous avez besoin pour votre code, vos explications ou la présentation de vos résultats. Vous pouvez également ajouter des sous-sections (par ex. des sous-sections 1.1, 1.2 etc.) si cela améliore la lisibilité.\n",
    "\n",
    "Notes :\n",
    "- Évitez les bouts de code trop longs ou trop complexe. Par exemple, il est difficile de comprendre 4-5 boucles ou conditions imbriquées. Si c'est le cas, définissez des sous-fonctions pour refactoriser et simplifier votre code. \n",
    "- Expliquez sommairement votre démarche.\n",
    "- Expliquez les choix que vous faites au niveau de la programmation et des modèles (si non trivial).\n",
    "- Analyser vos résultats. Indiquez ce que vous observez, si c'est bon ou non, si c'est surprenant, etc. \n",
    "- Une analyse quantitative et qualitative d'erreurs est intéressante et permet de mieux comprendre le comportement d'un modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Création du jeu de données (*dataset*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train data: text_size 2475, target_size 2475'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Dev data: text_size 531, target_size 531'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Test data: text_size 531, target_size 531'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' At approximately 8:50 a.m. on October 29  1997  Employee #1 was painting a  single story house at 2657 7th Ave  Sacramento  CA. He was caulking around the  peak of the roof line on the west side of the house  20 ft above the ground.  He was working off of a 24 ft aluminum extension ladder so that his feet were  approximately 12 to 13 feet above the ground. Employee #1 fell and suffered a  concussion and two dislocated discs in his lower back and was hospitalized.  The ladder was not secured to prevent movement.                                 '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_model = spacy.load(\"en_core_web_md\")\n",
    "embedding_size = spacy_model.meta['vectors']['width']\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "# Assurez-vous que le modèle de langue de spacy est téléchargé\n",
    "# python -m spacy download fr_core_news_md (par exemple pour le français)\n",
    "\n",
    "# Charger le modèle de langue de spacy\n",
    "\n",
    "# Définition des chemins vers les fichiers de données\n",
    "train_data_path = './data/incidents_train.json'\n",
    "dev_data_path = './data/incidents_dev.json'\n",
    "test_data_path = './data/incidents_test.json'\n",
    "\n",
    "def load_incident_dataset(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        incident_list = json.load(fp)\n",
    "        \n",
    "        text = [item[\"text\"] for item in incident_list]\n",
    "        target = np.array([int(item[\"label\"]) for item in incident_list])\n",
    "         \n",
    "    return text, target\n",
    "\n",
    "\n",
    "# Créer les DataFrames pour chaque partition de données\n",
    "train_list, train_target = load_incident_dataset(train_data_path)\n",
    "dev_list, dev_target = load_incident_dataset(dev_data_path)\n",
    "test_list, test_target = load_incident_dataset(test_data_path)\n",
    "\n",
    "# Affichage de l'information de base sur les DataFrames\n",
    "display(f\"Train data: text_size {len(train_list)}, target_size {len(train_target)}\")\n",
    "display(f\"Dev data: text_size {len(dev_list)}, target_size {len(dev_target)}\")\n",
    "display(f\"Test data: text_size {len(test_list)}, target_size {len(test_target)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Vérification des premiers enregistrements dans l'ensemble d'entraînement\n",
    "train_list[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# padding_token = \"<PAD>\"   # mot 0\n",
    "# unk_token = \"<UNK>\"    # mot 1\n",
    "# zero_vec_embedding = np.zeros(embedding_size, dtype=np.float64)\n",
    "\n",
    "# id2word = {}\n",
    "# id2word[0] = padding_token \n",
    "# id2word[1] = unk_token \n",
    "\n",
    "# word2id = {}\n",
    "# word2id[padding_token] = 0\n",
    "# word2id[unk_token] = 1\n",
    "\n",
    "# id2embedding = {}\n",
    "# id2embedding[0] = zero_vec_embedding\n",
    "# id2embedding[1] = zero_vec_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Création de modèle(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': tensor([[  101,  2012,  3155,  1022,  1024,  2753,  1037,  1012,  1049,  1012,\n",
      "          2006,  2255,  2756,  2722,  7904,  1001,  1015,  2001,  4169,  1037,\n",
      "          2309,  2466,  2160,  2012, 20549,  2581,  5504, 13642, 11932,  6187,\n",
      "          1012,  2002,  2001,  6187,  5313,  6834,  2105,  1996,  4672,  1997,\n",
      "          1996,  4412,  2240,  2006,  1996,  2225,  2217,  1997,  1996,  2160,\n",
      "          2322,  3027,  2682,  1996,  2598,  1012,  2002,  2001,  2551,  2125,\n",
      "          1997,  1037,  2484,  3027, 13061,  5331, 10535,  2061,  2008,  2010,\n",
      "          2519,  2020,  3155,  2260,  2000,  2410,  2519,  2682,  1996,  2598,\n",
      "          1012,  7904,  1001,  1015,  3062,  1998,  4265,  1037, 23159,  1998,\n",
      "          2048,  4487, 14540, 24755,  3064, 15303,  1999,  2010,  2896,  2067,\n",
      "          1998,  2001, 24735,  1012,  1996, 10535,  2001,  2025,  7119,  2000,\n",
      "          4652,  2929,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, BertTokenizer, BertModel, AutoTokenizer, AutoModel, DataCollatorWithPadding, BertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_function(text_list, labels):\n",
    "    return tokenizer(text_list, truncation=True, padding=True, return_tensors=\"pt\"), labels\n",
    "\n",
    "tokenized_train_list = [preprocess_function(text, labels) for text, labels in zip(train_list, train_target)]\n",
    "tokenized_validate_list = [preprocess_function(text, labels) for text, labels in zip(dev_list, dev_target)]\n",
    "tokenized_test_list = [preprocess_function(text, labels) for text, labels in zip(test_list, test_target)]\n",
    "\n",
    "\n",
    "print(tokenized_train_list[0])\n",
    "\n",
    "\n",
    "bert_data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# custom_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"bert-base-uncased\", num_labels=9\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entraînement de modèle(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get the model's classification layer weights\n",
    "        classification_layer_weights = model.classifier.weight\n",
    "        \n",
    "        # Assuming you want to use the weights directly, you can pass them to the loss function\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=classification_layer_weights)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/mac/Desktop/Maîtrise IA 2024/Session Automne 2023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb Cellule 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mac/Desktop/Ma%C3%AEtrise%20IA%202024/Session%20Automne%202023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mac/Desktop/Ma%C3%AEtrise%20IA%202024/Session%20Automne%202023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel3/bert\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mac/Desktop/Ma%C3%AEtrise%20IA%202024/Session%20Automne%202023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39m\u001b[39m2e-5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mac/Desktop/Ma%C3%AEtrise%20IA%202024/Session%20Automne%202023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mac/Desktop/Ma%C3%AEtrise%20IA%202024/Session%20Automne%202023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mac/Desktop/Ma%C3%AEtrise%20IA%202024/Session%20Automne%202023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m trainer \u001b[39m=\u001b[39m CustomTrainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mac/Desktop/Ma%C3%AEtrise%20IA%202024/Session%20Automne%202023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     model\u001b[39m=\u001b[39mbert_model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mac/Desktop/Ma%C3%AEtrise%20IA%202024/Session%20Automne%202023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mac/Desktop/Ma%C3%AEtrise%20IA%202024/Session%20Automne%202023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mac/Desktop/Ma%C3%AEtrise%20IA%202024/Session%20Automne%202023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mac/Desktop/Ma%C3%AEtrise%20IA%202024/Session%20Automne%202023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1862\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/trainer.py:2725\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2724\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2725\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2727\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2728\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "\u001b[1;32m/Users/mac/Desktop/Maîtrise IA 2024/Session Automne 2023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb Cellule 16\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mac/Desktop/Ma%C3%AEtrise%20IA%202024/Session%20Automne%202023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_loss\u001b[39m(\u001b[39mself\u001b[39m, model, inputs, return_outputs\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mac/Desktop/Ma%C3%AEtrise%20IA%202024/Session%20Automne%202023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mac/Desktop/Ma%C3%AEtrise%20IA%202024/Session%20Automne%202023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mac/Desktop/Ma%C3%AEtrise%20IA%202024/Session%20Automne%202023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n",
      "\u001b[1;32m/Users/mac/Desktop/Maîtrise IA 2024/Session Automne 2023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb Cellule 16\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mac/Desktop/Ma%C3%AEtrise%20IA%202024/Session%20Automne%202023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_loss\u001b[39m(\u001b[39mself\u001b[39m, model, inputs, return_outputs\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mac/Desktop/Ma%C3%AEtrise%20IA%202024/Session%20Automne%202023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mac/Desktop/Ma%C3%AEtrise%20IA%202024/Session%20Automne%202023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mac/Desktop/Ma%C3%AEtrise%20IA%202024/Session%20Automne%202023/NLP/Pratique/TP1/Travaux-pratiques-NLP/tp2_2023/tp2/transformer.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model3/bert\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=list(zip(tokenized_train_list, train_target)),\n",
    "    eval_dataset=list(zip(tokenized_validate_list, dev_target)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=bert_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Évaluation et analyse de résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
