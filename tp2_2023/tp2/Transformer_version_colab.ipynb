{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ683JsucxtT"
      },
      "source": [
        "# T√¢che #3 : Classification d'incidents avec des mod√®les *Transformers*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMEZk4FQcxtY"
      },
      "source": [
        "Toujours avec la m√™me t√¢che et les m√™mes fichiers de textes, utiliser la librairie HuggingFace pour accomplir cette t√¢che. On demande plus sp√©cifiquement d‚Äôutiliser le mod√®le bert-base-uncased et un autre mod√®le de votre choix.\n",
        "Les consignes associ√©es √† cette t√¢che sont:\n",
        "-\tNom du notebook : transformer.ipynb\n",
        "-\tTokenisation : Celle fournie par les tokeniseurs accompagnant les mod√®les transformers.\n",
        "-\tPlongements de mots : Ceux du mod√®le transformer.\n",
        "-\tNormalisation : Lettre en minuscule pour Bert. Aucune contrainte pour le 2e mod√®le.\n",
        "-\tChoix du 2e transformer: Un mod√®le encodeur pr√©entra√Æn√© pour l‚Äôanglais. Le mod√®le ne doit pas √™tre une autre version de Bert et doit √™tre significativement diff√©rent. Utilisez un 2 fichier pour ce mod√®le si n√©cessaire (une copie de celui-ci).\n",
        "-\tAnalyse : Comparer les r√©sultats obtenus avec les 2 mod√®les transformers. Pr√©sentez √©galement une comparaison globale des r√©sultats obtenus avec tous les mod√®les utilis√©s dans ce travail et ceux du travail pr√©c√©dent (TP #1).\n",
        "\n",
        "\n",
        "Vous pouvez ajouter au *notebook* toutes les cellules dont vous avez besoin pour votre code, vos explications ou la pr√©sentation de vos r√©sultats. Vous pouvez √©galement ajouter des sous-sections (par ex. des sous-sections 1.1, 1.2 etc.) si cela am√©liore la lisibilit√©.\n",
        "\n",
        "Notes :\n",
        "- √âvitez les bouts de code trop longs ou trop complexe. Par exemple, il est difficile de comprendre 4-5 boucles ou conditions imbriqu√©es. Si c'est le cas, d√©finissez des sous-fonctions pour refactoriser et simplifier votre code.\n",
        "- Expliquez sommairement votre d√©marche.\n",
        "- Expliquez les choix que vous faites au niveau de la programmation et des mod√®les (si non trivial).\n",
        "- Analyser vos r√©sultats. Indiquez ce que vous observez, si c'est bon ou non, si c'est surprenant, etc.\n",
        "- Une analyse quantitative et qualitative d'erreurs est int√©ressante et permet de mieux comprendre le comportement d'un mod√®le."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYacBf8jcxtZ"
      },
      "source": [
        "## 1. Cr√©ation du jeu de donn√©es (*dataset*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I2MXPnIOz6U",
        "outputId": "af8b5fd7-2aad-4c6e-9711-ad1d052c1a9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Connexion au drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhnqO8myO9EU",
        "outputId": "4f5986fe-d7ea-46ff-8d3c-1a94c8bb6f23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "PfdvpqyDcxta",
        "outputId": "50de9343-d042-4301-c6c2-e3ac81dd2926"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Train data: text_size 531'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Dev data: text_size 531'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Test data: text_size 2475'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'text': ' At approximately 11:30 a.m. on December 8  2004  Employee #1  a laborer  who  had only been working for a construction company for one month  was laying out  sheets of plywood sheathing on the 2 by 6 feet nominal roof joists of a flat  roof. The employer specializes in nonresidential wood frame construction. In  order to complete the work quickly Employee #1 and a coworker  under the  direction of a supervisor  began installing only whole pieces of plywood  measuring approximately 4 by 8 feet. Because the plywood did not fit the roof  area precisely  holes and gaps were left in various areas. In one particular  spot a gap  measuring approximately two inches  was left at the end of one of  the pieces of plywood which was tacked down with a few nails. As Employee #1  and the coworker were beginning to install smaller pieces of plywood to fill  the gaps  Employee #1 stepped on the portion of the plywood with the two-inch  gap. The gap came loose from its securing nails  causing Employee #1 to fall  15 feet 9 inches to a cement slab below. Employee #1 was hospitalized and  treated for multiple fractures. Employee #1 had only been employed by the  employer for approximately one month.                                           ',\n",
              " 'label': '5'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import spacy\n",
        "import json\n",
        "import datasets\n",
        "\n",
        "spacy_model = spacy.load(\"en_core_web_sm\")\n",
        "embedding_size = spacy_model.meta['vectors']['width']\n",
        "\n",
        "# D√©finition des chemins vers les fichiers de donn√©es dans la m√©moire locale\n",
        "\n",
        "# train_data_path = './data/incidents_train.json'\n",
        "# dev_data_path = './data/incidents_dev.json'\n",
        "# test_data_path = './data/incidents_test.json'\n",
        "\n",
        "# D√©finition des chemins vers les fichiers de donn√©es sur Google Colab\n",
        "\n",
        "train_data_path = '/content/drive/MyDrive/TP2_NLP/incidents_dev.json'\n",
        "dev_data_path = '/content/drive/MyDrive/TP2_NLP/incidents_test.json'\n",
        "test_data_path = '/content/drive/MyDrive/TP2_NLP/incidents_train.json'\n",
        "\n",
        "def load_incident_dataset(filename):\n",
        "    with open(filename, 'r') as fp:\n",
        "        incident_list = json.load(fp)\n",
        "\n",
        "    return incident_list\n",
        "\n",
        "\n",
        "# Cr√©er les DataFrames pour chaque partition de donn√©es\n",
        "train_list  = load_incident_dataset(train_data_path)\n",
        "dev_list  = load_incident_dataset(dev_data_path)\n",
        "test_list = load_incident_dataset(test_data_path)\n",
        "\n",
        "# Affichage de l'information de base sur les DataFrames\n",
        "display(f\"Train data: text_size {len(train_list)}\")\n",
        "display(f\"Dev data: text_size {len(dev_list)}\")\n",
        "display(f\"Test data: text_size {len(test_list)}\")\n",
        "\n",
        "\n",
        "\n",
        "# V√©rification des premiers enregistrements dans l'ensemble d'entra√Ænement\n",
        "train_list[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PudSDolcxtb"
      },
      "source": [
        "## 2. Cr√©ation de mod√®le(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZcDFvZ2N_Sf"
      },
      "source": [
        "Comme 2e mod√®le, nous avons choisi le mod√®le electra parce qu'il donne d'assez bon r√©sultat tout en √©tant l√©ger et assez diff√©rent de Bert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylZXBsXhcxtc"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, BertTokenizer, AutoTokenizer, DataCollatorWithPadding, BertForSequenceClassification\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "electra_tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ce bloc de code effectue des op√©rations de pr√©traitement et de pr√©paration de donn√©es pour le mod√®le "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrsObTvDcxtc"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "def preprocess_function(data, tokenizer):\n",
        "    tokenized = tokenizer(data[\"text\"], padding=True, truncation=True)\n",
        "    tokenized[\"labels\"] = int(data[\"label\"])\n",
        "    return tokenized\n",
        "\n",
        "def prepare_dataset(tokenizer):\n",
        "\n",
        "    tokenized_train_list = [preprocess_function(item, tokenizer) for item in train_list]\n",
        "    tokenized_validate_list = [preprocess_function(item, tokenizer) for item in dev_list]\n",
        "    tokenized_test_list = [preprocess_function(item, tokenizer) for item in test_list]\n",
        "\n",
        "    print(tokenized_train_list[0])\n",
        "\n",
        "\n",
        "    # Assuming tokenized_train_dataset is a list of dictionaries\n",
        "    train_dataset = Dataset.from_dict({\"input_ids\": [item[\"input_ids\"] for item in tokenized_train_list],\n",
        "                                    \"attention_mask\": [item[\"attention_mask\"] for item in tokenized_train_list],\n",
        "                                    \"labels\": [item[\"labels\"] for item in tokenized_train_list]})\n",
        "\n",
        "    dev_dataset = Dataset.from_dict({\"input_ids\": [item[\"input_ids\"] for item in tokenized_validate_list],\n",
        "                                  \"attention_mask\": [item[\"attention_mask\"] for item in tokenized_validate_list],\n",
        "                                  \"labels\": [item[\"labels\"] for item in tokenized_validate_list]})\n",
        "\n",
        "    test_dataset = Dataset.from_dict({\"input_ids\": [item[\"input_ids\"] for item in tokenized_test_list],\n",
        "                                  \"attention_mask\": [item[\"attention_mask\"] for item in tokenized_test_list],\n",
        "                                  \"labels\": [item[\"labels\"] for item in tokenized_test_list]})\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True,  max_length=\"max_length\")\n",
        "\n",
        "    return train_dataset, dev_dataset, test_dataset, data_collator\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBuTLYq7cxtc"
      },
      "source": [
        "## 3. Entra√Ænement de mod√®le(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Choix de la m√©trique de pr√©cision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNL0Qsfhcxtc",
        "outputId": "b06f9a1f-aa2a-49ef-b579-fcd799d44177"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-0040176d210c>:6: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqfukdePN_Sf"
      },
      "source": [
        "### Cr√©ation et entrainement de Bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpYBIFoPcxtd",
        "outputId": "3212ba5f-2274-4e8b-c096-54afef9e2576",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [101, 2012, 3155, 2340, 1024, 2382, 1037, 1012, 1049, 1012, 2006, 2285, 1022, 2432, 7904, 1001, 1015, 1037, 4450, 2121, 2040, 2018, 2069, 2042, 2551, 2005, 1037, 2810, 2194, 2005, 2028, 3204, 2001, 10201, 2041, 8697, 1997, 20228, 26985, 21867, 2075, 2006, 1996, 1016, 2011, 1020, 2519, 15087, 4412, 8183, 5130, 1997, 1037, 4257, 4412, 1012, 1996, 11194, 16997, 1999, 2512, 6072, 5178, 19909, 3536, 4853, 2810, 1012, 1999, 2344, 2000, 3143, 1996, 2147, 2855, 7904, 1001, 1015, 1998, 1037, 11190, 2953, 5484, 2104, 1996, 3257, 1997, 1037, 12366, 2211, 23658, 2069, 2878, 4109, 1997, 20228, 26985, 9854, 3155, 1018, 2011, 1022, 2519, 1012, 2138, 1996, 20228, 26985, 2106, 2025, 4906, 1996, 4412, 2181, 10785, 8198, 1998, 16680, 2020, 2187, 1999, 2536, 2752, 1012, 1999, 2028, 3327, 3962, 1037, 6578, 9854, 3155, 2048, 5282, 2001, 2187, 2012, 1996, 2203, 1997, 2028, 1997, 1996, 4109, 1997, 20228, 26985, 2029, 2001, 26997, 2098, 2091, 2007, 1037, 2261, 10063, 1012, 2004, 7904, 1001, 1015, 1998, 1996, 11190, 2953, 5484, 2020, 2927, 2000, 16500, 3760, 4109, 1997, 20228, 26985, 2000, 6039, 1996, 16680, 7904, 1001, 1015, 3706, 2006, 1996, 4664, 1997, 1996, 20228, 26985, 2007, 1996, 2048, 1011, 4960, 6578, 1012, 1996, 6578, 2234, 6065, 2013, 2049, 12329, 10063, 4786, 7904, 1001, 1015, 2000, 2991, 2321, 2519, 1023, 5282, 2000, 1037, 11297, 17584, 2917, 1012, 7904, 1001, 1015, 2001, 24735, 1998, 5845, 2005, 3674, 28929, 1012, 7904, 1001, 1015, 2018, 2069, 2042, 4846, 2011, 1996, 11194, 2005, 3155, 2028, 3204, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 5}\n"
          ]
        }
      ],
      "source": [
        "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=9)\n",
        "\n",
        "bert_train_dataset, bert_dev_dataset, bert_test_dataset, bert_datacollator = prepare_dataset(bert_tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW2OTuG3QR9V",
        "outputId": "ee4d79fa-1846-4a9c-e612-b6937422137c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "#!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hyperparam√®tres pour l'entrainement du modele BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "id": "wqwRobtCN_Sg",
        "outputId": "3c45ff0c-5994-4eaf-8d64-c6907cf4265b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [340/340 03:38, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.124051</td>\n",
              "      <td>0.696798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.079923</td>\n",
              "      <td>0.706215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.340983</td>\n",
              "      <td>0.700565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.653140</td>\n",
              "      <td>0.711864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.621023</td>\n",
              "      <td>0.726930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.640677</td>\n",
              "      <td>0.711864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.652330</td>\n",
              "      <td>0.715631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.701371</td>\n",
              "      <td>0.713748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.722928</td>\n",
              "      <td>0.713748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.725571</td>\n",
              "      <td>0.713748</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"model_task3/bert\",\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "bert_trainer = Trainer(\n",
        "    model=bert_model,\n",
        "    args=training_args,\n",
        "    train_dataset=bert_train_dataset,\n",
        "    eval_dataset=bert_dev_dataset,\n",
        "    tokenizer=bert_tokenizer,\n",
        "    data_collator=bert_datacollator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "bert_trainer.train()\n",
        "\n",
        "bert_trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "ZyrZS1G-EI7n",
        "outputId": "5e8584fa-3fc8-4662-8cb9-9ca89311ffc8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [34/34 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 1.0799232721328735,\n",
              " 'eval_accuracy': 0.7062146892655368,\n",
              " 'eval_runtime': 3.9463,\n",
              " 'eval_samples_per_second': 134.558,\n",
              " 'eval_steps_per_second': 8.616,\n",
              " 'epoch': 10.0}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert_trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnleYLtqDaDa"
      },
      "source": [
        "### Cr√©ation et entrainement de Electra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "3779844c6158413c96397f652495879e",
            "8276c592b84743d0a1e5326a452cfbb7",
            "947a3622dc574d58b06a20ab1f4fdf0d",
            "cdc5dbe06fe641edb4137a04d269f93d",
            "2687d355de2b45f98a9338e1cabf2259",
            "a1a71cdd262c43eeaf2d4aae88cf53fa",
            "a9f68ba06b904940bc71f12481e986bc",
            "fb83eed54ff847c2b193ea2d67bde6a8",
            "fb7ea8655eb1479486ad6bd335bf63d3",
            "f42289be20a2423c8b8df11bcfc91af0",
            "ee0c80fa922c4435b0ff4954d11f0fc8"
          ]
        },
        "id": "aDElJ9RaDZb7",
        "outputId": "f3554409-cf48-4090-bfd4-6884d75d83af"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3779844c6158413c96397f652495879e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/54.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [101, 2012, 3155, 2340, 1024, 2382, 1037, 1012, 1049, 1012, 2006, 2285, 1022, 2432, 7904, 1001, 1015, 1037, 4450, 2121, 2040, 2018, 2069, 2042, 2551, 2005, 1037, 2810, 2194, 2005, 2028, 3204, 2001, 10201, 2041, 8697, 1997, 20228, 26985, 21867, 2075, 2006, 1996, 1016, 2011, 1020, 2519, 15087, 4412, 8183, 5130, 1997, 1037, 4257, 4412, 1012, 1996, 11194, 16997, 1999, 2512, 6072, 5178, 19909, 3536, 4853, 2810, 1012, 1999, 2344, 2000, 3143, 1996, 2147, 2855, 7904, 1001, 1015, 1998, 1037, 11190, 2953, 5484, 2104, 1996, 3257, 1997, 1037, 12366, 2211, 23658, 2069, 2878, 4109, 1997, 20228, 26985, 9854, 3155, 1018, 2011, 1022, 2519, 1012, 2138, 1996, 20228, 26985, 2106, 2025, 4906, 1996, 4412, 2181, 10785, 8198, 1998, 16680, 2020, 2187, 1999, 2536, 2752, 1012, 1999, 2028, 3327, 3962, 1037, 6578, 9854, 3155, 2048, 5282, 2001, 2187, 2012, 1996, 2203, 1997, 2028, 1997, 1996, 4109, 1997, 20228, 26985, 2029, 2001, 26997, 2098, 2091, 2007, 1037, 2261, 10063, 1012, 2004, 7904, 1001, 1015, 1998, 1996, 11190, 2953, 5484, 2020, 2927, 2000, 16500, 3760, 4109, 1997, 20228, 26985, 2000, 6039, 1996, 16680, 7904, 1001, 1015, 3706, 2006, 1996, 4664, 1997, 1996, 20228, 26985, 2007, 1996, 2048, 1011, 4960, 6578, 1012, 1996, 6578, 2234, 6065, 2013, 2049, 12329, 10063, 4786, 7904, 1001, 1015, 2000, 2991, 2321, 2519, 1023, 5282, 2000, 1037, 11297, 17584, 2917, 1012, 7904, 1001, 1015, 2001, 24735, 1998, 5845, 2005, 3674, 28929, 1012, 7904, 1001, 1015, 2018, 2069, 2042, 4846, 2011, 1996, 11194, 2005, 3155, 2028, 3204, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 5}\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "# Define your model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"google/electra-small-discriminator\", num_labels=9)\n",
        "\n",
        "electra_train_dataset, electra_dev_dataset, electra_test_dataset, electra_datacollator = prepare_dataset(electra_tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hyperparam√®tres pour l'entrainement du modele Electra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VG_iw4TWN_Sh",
        "outputId": "47676d5a-a35e-414e-eee3-ea06cb5158c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='255' max='255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [255/255 01:15, Epoch 15/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.170361</td>\n",
              "      <td>0.653484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.159823</td>\n",
              "      <td>0.661017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.171645</td>\n",
              "      <td>0.653484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.145393</td>\n",
              "      <td>0.661017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.133275</td>\n",
              "      <td>0.674200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.146896</td>\n",
              "      <td>0.661017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.117871</td>\n",
              "      <td>0.689266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.137907</td>\n",
              "      <td>0.664783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.121474</td>\n",
              "      <td>0.679849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.112218</td>\n",
              "      <td>0.685499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.121983</td>\n",
              "      <td>0.683616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.112617</td>\n",
              "      <td>0.685499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.112706</td>\n",
              "      <td>0.687382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.115348</td>\n",
              "      <td>0.687382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.115094</td>\n",
              "      <td>0.687382</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"model_task3/electra\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=15,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "electra_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=electra_train_dataset,\n",
        "    eval_dataset=electra_dev_dataset,\n",
        "    tokenizer=electra_tokenizer,\n",
        "    data_collator=electra_datacollator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Start training (assuming you have train_dataset and dev_dataset defined)\n",
        "electra_trainer.train()\n",
        "\n",
        "electra_trainer.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieA9mNxQcxtd"
      },
      "source": [
        "## 4. √âvaluation et analyse de r√©sultats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bChklg2N_Sh"
      },
      "source": [
        "### Evaluation Bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "F5-gLfxlcxtd",
        "outputId": "a95e5331-a940-452f-f2bd-dede39c05455"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='102' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [34/34 06:07]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.0799232721328735, 'eval_accuracy': 0.7062146892655368, 'eval_runtime': 3.9356, 'eval_samples_per_second': 134.921, 'eval_steps_per_second': 8.639, 'epoch': 10.0}\n"
          ]
        }
      ],
      "source": [
        "#wandb.init(project='evaluation-bert')\n",
        "\n",
        "eval_metrics = bert_trainer.evaluate()\n",
        "\n",
        "train_metrics = bert_trainer.evaluate(bert_train_dataset)\n",
        "\n",
        "print(eval_metrics)\n",
        "\n",
        "#print(train_metrics)\n",
        "\n",
        "#wandb.log({\"train_loss\": train_metrics[\"loss\"], \"eval_loss\": eval_metrics[\"loss\"]})\n",
        "\n",
        "#wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gKyM6ukN_Sh"
      },
      "source": [
        "### Evaluation Electra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "nyBlEa7scxtd",
        "outputId": "876afbf2-71d0-42db-9c75-fd2ed94eda3e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='18' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9/9 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.1122177839279175, 'eval_accuracy': 0.6854990583804144, 'eval_runtime': 1.2671, 'eval_samples_per_second': 419.056, 'eval_steps_per_second': 7.103, 'epoch': 15.0}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "eval_metrics = electra_trainer.evaluate()\n",
        "\n",
        "train_metrics = electra_trainer.evaluate(electra_train_dataset)\n",
        "\n",
        "print(eval_metrics)\n",
        "\n",
        "#print(train_metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVf489wRV5_o"
      },
      "source": [
        "### Comparaison avec les r√©sultats du TP1:\n",
        "\n",
        "Pour analyser les r√©sultats obtenus avec les deux mod√®les Transformers (BERT et ELECTRA) par rapport √† l'ensemble des mod√®les utilis√©s dans ce travail ainsi qu'aux r√©sultats du travail pr√©c√©dent (TP #1), voici une comparaison globale :\n",
        "\n",
        "R√©sultats de BERT :\n",
        "- Pr√©cision sur l'ensemble de test (Accuracy) : 70.62%\n",
        "\n",
        "R√©sultats d'ELECTRA :\n",
        "- Pr√©cision sur l'ensemble de test (Accuracy) : 68.55%\n",
        "\n",
        "Comparaison globale avec les mod√®les pr√©c√©dents (TP #1) :\n",
        "- Naive Bayes (avec Lemmatisation) : Pr√©cision sur l'ensemble de test : 69.30%\n",
        "- Naive Bayes (sans Lemmatisation) : Pr√©cision sur l'ensemble de test : 70.90%\n",
        "- R√©gression Logistique (avec Lemmatisation) : Pr√©cision sur l'ensemble de test : 60.17%\n",
        "- R√©gression Logistique (sans Lemmatisation) : Pr√©cision sur l'ensemble de test : 71.75%\n",
        "\n",
        "Analyse :\n",
        "\n",
        "1. **BERT vs. ELECTRA** :\n",
        "   - BERT a obtenu une pr√©cision de 70.62% sur l'ensemble de test, tandis qu'ELECTRA a obtenu une pr√©cision l√©g√®rement inf√©rieure de 68.55%. BERT semble donc surpasser ELECTRA en termes de pr√©cision.\n",
        "\n",
        "2. **Mod√®les Transformers vs. Mod√®les Classiques (TP #1)** :\n",
        "   - Les mod√®les Transformers (BERT et ELECTRA) surpassent clairement les mod√®les classiques (Naive Bayes et R√©gression Logistique) en termes de pr√©cision. Les mod√®les Transformers atteignent des pr√©cisions sup√©rieures √† 68%, tandis que les mod√®les classiques atteignent au mieux une pr√©cision de 71.75% (R√©gression Logistique sans lemmatisation).\n",
        "   \n",
        "3. **Impact de la Lemmatisation** :\n",
        "   - Les mod√®les classiques (Naive Bayes et R√©gression Logistique) semblent √™tre plus sensibles √† la lemmatisation. Dans le TP #1, la R√©gression Logistique avec lemmatisation a montr√© une pr√©cision de seulement 60.17%, tandis que sans lemmatisation, elle a atteint 71.75%. Pour Naive Bayes, l'effet de la lemmatisation √©tait moins prononc√©, avec une l√©g√®re am√©lioration de la pr√©cision lorsqu'elle √©tait omise.\n",
        "\n",
        "4. **Temps d'ex√©cution** :\n",
        "   - Il est important de noter que les mod√®les Transformers sont g√©n√©ralement plus lents √† s'entra√Æner et √† pr√©dire que les mod√®les classiques. Les mod√®les classiques √©taient significativement plus rapides, avec des temps d'ex√©cution beaucoup plus courts.\n",
        "\n",
        "En conclusion, les mod√®les Transformers, en particulier BERT, surpassent legerement les mod√®les classiques en termes de pr√©cision pour cette t√¢che de classification de texte. La raison ne vient pas du manque de puissance des transformers mais surtout de la quantit√© limit√©e de donn√©es disponible pour l'entrainement. Avec un plus gros jeu de donn√©, les r√©sultats seraient clairement meilleurs pour les transformers.Il faut tenir compte du temps d'ex√©cution plus long associ√© aux mod√®les Transformers."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2687d355de2b45f98a9338e1cabf2259": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3779844c6158413c96397f652495879e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8276c592b84743d0a1e5326a452cfbb7",
              "IPY_MODEL_947a3622dc574d58b06a20ab1f4fdf0d",
              "IPY_MODEL_cdc5dbe06fe641edb4137a04d269f93d"
            ],
            "layout": "IPY_MODEL_2687d355de2b45f98a9338e1cabf2259"
          }
        },
        "8276c592b84743d0a1e5326a452cfbb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1a71cdd262c43eeaf2d4aae88cf53fa",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a9f68ba06b904940bc71f12481e986bc",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "947a3622dc574d58b06a20ab1f4fdf0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb83eed54ff847c2b193ea2d67bde6a8",
            "max": 54245363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb7ea8655eb1479486ad6bd335bf63d3",
            "value": 54245363
          }
        },
        "a1a71cdd262c43eeaf2d4aae88cf53fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9f68ba06b904940bc71f12481e986bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdc5dbe06fe641edb4137a04d269f93d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f42289be20a2423c8b8df11bcfc91af0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ee0c80fa922c4435b0ff4954d11f0fc8",
            "value": " 54.2M/54.2M [00:00&lt;00:00, 115MB/s]"
          }
        },
        "ee0c80fa922c4435b0ff4954d11f0fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f42289be20a2423c8b8df11bcfc91af0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb7ea8655eb1479486ad6bd335bf63d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb83eed54ff847c2b193ea2d67bde6a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
