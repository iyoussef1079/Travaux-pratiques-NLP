{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tâche #2 : Classification d'incidents avec un réseau  récurrent et des *embeddings* Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette tâche est similaire à la précédente et vous réutilisez les mêmes fichiers d’entraînement, de validation et de test. Cependant, vous devez utiliser des réseaux récurrents pour classifier les textes. Plus particulièrement, vous devez entraîner un réseau de neurones LSTM pour encoder les textes et une couche linéaire pour faire la classification des textes. \n",
    "\n",
    "Les consignes pour cette tâche sont: \n",
    "- \tNom du notebook : rnn.ipynb\n",
    "- \tTokenisation : Utilisation de Spacy. \n",
    "- \tPlongements de mots : Ceux de Spacy. \n",
    "- \tNormalisation : Aucune normalisation. \n",
    "- \tStructure du réseau : Un réseau LSTM avec 1 seule couche pour l’encodage de textes. Je vous laisse déterminer la taille de cette couche (à expliquer). \n",
    "- \tAnalyse : Comparer les résultats obtenus avec un réseau unidirectionnel et un réseau bidirectionnel. Si vous éprouvez des difficultés à entraîner les 2 réseaux dans un même notebook, faites une copie et nommez le 2e fichier rnn-bidirectionnel.ipynb.\n",
    "- \tExpliquez comment les modèles sont utilisés pour faire la classification d’un texte. \n",
    "- \tPrésentez clairement vos résultats et faites-en l’analyse. \n",
    "\n",
    "\n",
    "Vous pouvez ajouter au *notebook* toutes les cellules dont vous avez besoin pour votre code, vos explications ou la présentation de vos résultats. Vous pouvez également ajouter des sous-sections (par ex. des sous-sections 1.1, 1.2 etc.) si cela améliore la lisibilité.\n",
    "\n",
    "Notes :\n",
    "- Évitez les bouts de code trop longs ou trop complexes. Par exemple, il est difficile de comprendre 4-5 boucles ou conditions imbriquées. Si c'est le cas, définissez des sous-fonctions pour refactoriser et simplifier votre code. \n",
    "- Expliquez sommairement votre démarche.\n",
    "- Expliquez les choix que vous faites au niveau de la programmation et des modèles (si trivial).\n",
    "- Analyser vos résultats. Indiquez ce que vous observez, si c'est bon ou non, si c'est surprenant, etc. \n",
    "- Une analyse quantitative et qualitative d'erreurs est intéressante et permet de mieux comprendre le comportement d'un modèle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Création du jeu de données (*dataset*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train data: text_size 2475, target_size 2475'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Dev data: text_size 531, target_size 531'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Test data: text_size 531, target_size 531'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' At approximately 8:50 a.m. on October 29  1997  Employee #1 was painting a  single story house at 2657 7th Ave  Sacramento  CA. He was caulking around the  peak of the roof line on the west side of the house  20 ft above the ground.  He was working off of a 24 ft aluminum extension ladder so that his feet were  approximately 12 to 13 feet above the ground. Employee #1 fell and suffered a  concussion and two dislocated discs in his lower back and was hospitalized.  The ladder was not secured to prevent movement.                                 '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_model = spacy.load(\"en_core_web_md\")\n",
    "embedding_size = spacy_model.meta['vectors']['width']\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "# Assurez-vous que le modèle de langue de spacy est téléchargé\n",
    "# python -m spacy download fr_core_news_md (par exemple pour le français)\n",
    "\n",
    "# Charger le modèle de langue de spacy\n",
    "\n",
    "# Définition des chemins vers les fichiers de données\n",
    "train_data_path = './data/incidents_train.json'\n",
    "dev_data_path = './data/incidents_dev.json'\n",
    "test_data_path = './data/incidents_test.json'\n",
    "\n",
    "def load_incident_dataset(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        incident_list = json.load(fp)\n",
    "        \n",
    "        text = [item[\"text\"] for item in incident_list]\n",
    "        target = np.array([int(item[\"label\"]) for item in incident_list])\n",
    "         \n",
    "    return text, target\n",
    "\n",
    "\n",
    "# Créer les DataFrames pour chaque partition de données\n",
    "train_list, train_target = load_incident_dataset(train_data_path)\n",
    "dev_list, dev_target = load_incident_dataset(dev_data_path)\n",
    "test_list, test_target = load_incident_dataset(test_data_path)\n",
    "\n",
    "# Affichage de l'information de base sur les DataFrames\n",
    "display(f\"Train data: text_size {len(train_list)}, target_size {len(train_target)}\")\n",
    "display(f\"Dev data: text_size {len(dev_list)}, target_size {len(dev_target)}\")\n",
    "display(f\"Test data: text_size {len(test_list)}, target_size {len(test_target)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Vérification des premiers enregistrements dans l'ensemble d'entraînement\n",
    "train_list[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "padding_token = \"<PAD>\"   # mot 0\n",
    "unk_token = \"<UNK>\"    # mot 1\n",
    "zero_vec_embedding = np.zeros(embedding_size, dtype=np.float64)\n",
    "\n",
    "id2word = {}\n",
    "id2word[0] = padding_token \n",
    "id2word[1] = unk_token \n",
    "\n",
    "word2id = {}\n",
    "word2id[padding_token] = 0\n",
    "word2id[unk_token] = 1\n",
    "\n",
    "id2embedding = {}\n",
    "id2embedding[0] = zero_vec_embedding\n",
    "id2embedding[1] = zero_vec_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gestion de plongements de mots (*embeddings*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import FloatTensor\n",
    "\n",
    "def get_spacy_embeddings(text, spacy_analyzer=spacy_model):\n",
    "    doc = spacy_analyzer(text)\n",
    "    embeddings = [token.vector for token in doc]\n",
    "    return FloatTensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = 2\n",
    "vocab = word2id.keys()\n",
    "for question in train_list:\n",
    "    for word in spacy_model(question):\n",
    "        if word.text not in vocab:\n",
    "            word2id[word.text] = word_index\n",
    "            id2word[word_index] = word.text\n",
    "            id2embedding[word_index] = word.vector\n",
    "            word_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'approximately'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import FloatTensor, LongTensor, unsqueeze\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "class SpacyDataset(Dataset):\n",
    "    def __init__(self, dataset: List[str] , target: np.array, wordId: dict, model=spacy_model):\n",
    "        self.dataset = dataset\n",
    "        self.doc_embeddings = [None for _ in range(len(dataset))]\n",
    "        self.targets = target\n",
    "        self.tokenizer = model\n",
    "        self.word2id = wordId\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.doc_embeddings[index] is None:\n",
    "            self.doc_embeddings[index] = self.tokenize(self.dataset[index])\n",
    "        return LongTensor(self.doc_embeddings[index]), LongTensor([self.targets[index]]).squeeze(0)\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        tokens = [word.text for word in self.tokenizer(sentence)]\n",
    "        return [self.word2id.get(token, 1) for token in tokens]  # get(token, 1) retourne 1 par défaut si mot inconnu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "def pad_batch(batch):\n",
    "    x = [x for x,y in batch]\n",
    "    x_true_length = [len(x) for x,y in batch]\n",
    "    y = torch.stack([y for x,y in batch], dim=0)\n",
    "    return ((pad_sequence(x, batch_first=True), x_true_length), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On finalise la construction des 3 jeux de données et leurs dataloaders\n",
    "train_dataset = SpacyDataset(train_list, train_target, word2id)\n",
    "valid_dataset = SpacyDataset(dev_list, dev_target, word2id)\n",
    "test_dataset = SpacyDataset(test_list, test_target, word2id)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Création de modèle(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de la couche d'embeddings: torch.Size([11642, 300])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(id2embedding)\n",
    "embedding_layer = np.zeros((vocab_size, embedding_size), dtype=np.float32)\n",
    "for token_id, embedding in id2embedding.items():\n",
    "    embedding_layer[token_id,:] = embedding\n",
    "embedding_layer = torch.from_numpy(embedding_layer)\n",
    "\n",
    "print(\"Taille de la couche d'embeddings:\", embedding_layer.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# class LSTMClassifier(nn.Module):\n",
    "#     def __init__(self, embeddings, hidden_state_size, nb_classes) :\n",
    "#         super(LSTMClassifier, self).__init__()\n",
    "#         self.embedding_layer = nn.Embedding.from_pretrained(embeddings)\n",
    "#         self.embedding_size = embeddings.size()[1]\n",
    "#         self.rnn = nn.LSTM(self.embedding_size, hidden_state_size, 1, batch_first=True)\n",
    "#         self.classification_layer = nn.Linear(hidden_state_size, nb_classes)\n",
    "\n",
    "#     def forward(self, x, x_lengths):\n",
    "#         x = self.embedding_layer(x)\n",
    "#         packed_batch = pack_padded_sequence(x, x_lengths, batch_first=True, enforce_sorted=False)\n",
    "#         x, last_hidden_state = self.rnn(packed_batch)  # On utilise le hidden state de la dernière cellule\n",
    "#         # x = last_hidden_state.squeeze()  # Le LSTM a une seule couche, on retire cette dimension\n",
    "#         x = self.classification_layer(x)\n",
    "#         x = nn.functional.softmax(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_state_size, nb_classes, bidirectional=False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(embeddings)\n",
    "        self.rnn = nn.LSTM(input_size=embeddings.size()[1], hidden_size=hidden_state_size, batch_first=True, bidirectional=bidirectional)\n",
    "        multiplier = 2 if bidirectional else 1\n",
    "        self.classification_layer = nn.Linear(hidden_state_size * multiplier, nb_classes)\n",
    "\n",
    "    def forward(self, x, x_lenght):\n",
    "        x = self.embedding_layer(x)\n",
    "        packed_batch = pack_padded_sequence(x, x_lenght, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden_state, cell_state) = self.rnn(packed_batch)\n",
    "\n",
    "        if self.rnn.bidirectional:\n",
    "            # Concatenating the hidden states of the last time step from both directions\n",
    "            x = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            x = hidden_state[-1,:,:]  # Taking the last time step of the hidden state\n",
    "\n",
    "        x = self.classification_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poutyne import set_seeds\n",
    "\n",
    "set_seeds(42)\n",
    "hidden_size = 100  # choisi arbitrairement\n",
    "nb_classes = 9\n",
    "model = LSTMClassifier(embedding_layer, hidden_size, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poutyne import set_seeds\n",
    "\n",
    "set_seeds(42)\n",
    "hidden_size = 100  # choisi arbitrairement\n",
    "nb_classes = 9\n",
    "bidirectional_model = LSTMClassifier(embedding_layer, hidden_size, nb_classes, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fonctions utilitaires\n",
    "\n",
    "Vous pouvez mettre ici toutes les fonctions qui seront utiles pour les sections suivantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement de modèle(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poutyne.framework import Experiment\n",
    "from poutyne import set_seeds\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "\n",
    "set_seeds(42)\n",
    "hidden_size = 100\n",
    "optimizer = \"ADAM\"\n",
    "\n",
    "directory_name = 'model2/_mlp_optimizer{}'.format(optimizer)\n",
    "\n",
    "experiment = Experiment(directory_name,\n",
    "                        model,\n",
    "                        optimizer = optimizer,\n",
    "                        task=\"classification\",\n",
    "                        loss_function=\"cross_entropy\",\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from model2/_mlp_optimizerADAM\\checkpoint.ckpt and starting at epoch 51.\n",
      "Loading optimizer state from model2/_mlp_optimizerADAM\\checkpoint.optim and starting at epoch 51.\n",
      "Loading random states from model2/_mlp_optimizerADAM\\checkpoint.randomstate and starting at epoch 51.\n",
      "Restoring data from model2/_mlp_optimizerADAM\\checkpoint_epoch_9.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\anaconda3\\Lib\\site-packages\\poutyne\\framework\\model_bundle.py:751: UserWarning: tensorboard does not seem to be installed. To remove this warning, set the 'disable_tensorboard' flag to True or install tensorboard.\n",
      "  return self._train(self.model.fit_generator, train_generator, valid_generator, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "logging = experiment.train(train_dataloader, valid_dataloader, epochs=50, disable_tensorboard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found best checkpoint at epoch: 9\n",
      "lr: 0.001, loss: 0.33516, acc: 89.3333, fscore_macro: 0.820262, val_loss: 1.14128, val_acc: 67.9849, val_fscore_macro: 0.476044\n",
      "Loading checkpoint model2/_mlp_optimizerADAM\\checkpoint_epoch_9.ckpt\n",
      "Running test\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LSTMClassifier.forward() missing 1 required positional argument: 'x_lenght'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Utilisateur\\Desktop\\Ulaval\\A23\\NLP\\Travaux-pratiques-NLP\\tp2_2023\\tp2\\rnn.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Utilisateur/Desktop/Ulaval/A23/NLP/Travaux-pratiques-NLP/tp2_2023/tp2/rnn.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m experiment\u001b[39m.\u001b[39mtest(test_dataset)\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\anaconda3\\Lib\\site-packages\\poutyne\\framework\\model_bundle.py:1051\u001b[0m, in \u001b[0;36mModelBundle.test\u001b[1;34m(self, test_generator, **kwargs)\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest\u001b[39m(\u001b[39mself\u001b[39m, test_generator, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1019\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1020\u001b[0m \u001b[39m    Computes and returns the loss and the metrics of the model on a given test examples\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[39m    generator.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[39m        dict sorting of all the test metrics values by their names.\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_test(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mevaluate_generator, test_generator, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\anaconda3\\Lib\\site-packages\\poutyne\\framework\\model_bundle.py:1147\u001b[0m, in \u001b[0;36mModelBundle._test\u001b[1;34m(self, evaluate_func, checkpoint, seed, name, verbose, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[0;32m   1146\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRunning \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1147\u001b[0m ret \u001b[39m=\u001b[39m evaluate_func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs, verbose\u001b[39m=\u001b[39mverbose)\n\u001b[0;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogging:\n\u001b[0;32m   1150\u001b[0m     test_metrics_dict \u001b[39m=\u001b[39m ret[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ret, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\anaconda3\\Lib\\site-packages\\poutyne\\framework\\model.py:1378\u001b[0m, in \u001b[0;36mModel.evaluate_generator\u001b[1;34m(self, generator, steps, return_pred, return_ground_truth, return_dict_format, concatenate_returns, convert_to_numpy, verbose, progress_options, callbacks)\u001b[0m\n\u001b[0;32m   1373\u001b[0m step_iterator \u001b[39m=\u001b[39m StepIterator(\n\u001b[0;32m   1374\u001b[0m     generator, steps, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_metrics_names, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch_metrics_names, callback_list, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1375\u001b[0m )\n\u001b[0;32m   1377\u001b[0m test_begin_time \u001b[39m=\u001b[39m timeit\u001b[39m.\u001b[39mdefault_timer()\n\u001b[1;32m-> 1378\u001b[0m pred_y, true_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate(\n\u001b[0;32m   1379\u001b[0m     step_iterator,\n\u001b[0;32m   1380\u001b[0m     return_pred\u001b[39m=\u001b[39mreturn_pred,\n\u001b[0;32m   1381\u001b[0m     return_ground_truth\u001b[39m=\u001b[39mreturn_ground_truth,\n\u001b[0;32m   1382\u001b[0m     convert_to_numpy\u001b[39m=\u001b[39mconvert_to_numpy,\n\u001b[0;32m   1383\u001b[0m )\n\u001b[0;32m   1385\u001b[0m step_iterator\u001b[39m.\u001b[39mloss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_loss()\n\u001b[0;32m   1386\u001b[0m step_iterator\u001b[39m.\u001b[39mbatch_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_batch_metrics()\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\anaconda3\\Lib\\site-packages\\poutyne\\framework\\model.py:1460\u001b[0m, in \u001b[0;36mModel._validate\u001b[1;34m(self, step_iterator, return_pred, return_ground_truth, convert_to_numpy)\u001b[0m\n\u001b[0;32m   1458\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_training_mode(\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m   1459\u001b[0m     \u001b[39mfor\u001b[39;00m step, (x, y) \u001b[39min\u001b[39;00m step_iterator:\n\u001b[1;32m-> 1460\u001b[0m         step\u001b[39m.\u001b[39mloss, step\u001b[39m.\u001b[39mbatch_metrics, pred_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_loss_and_metrics(\n\u001b[0;32m   1461\u001b[0m             x, y, return_pred\u001b[39m=\u001b[39mreturn_pred, convert_to_numpy\u001b[39m=\u001b[39mconvert_to_numpy\n\u001b[0;32m   1462\u001b[0m         )\n\u001b[0;32m   1463\u001b[0m         \u001b[39mif\u001b[39;00m return_pred:\n\u001b[0;32m   1464\u001b[0m             pred_list\u001b[39m.\u001b[39mappend(pred_y)\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\anaconda3\\Lib\\site-packages\\poutyne\\framework\\model.py:1477\u001b[0m, in \u001b[0;36mModel._compute_loss_and_metrics\u001b[1;34m(self, x, y, return_loss_tensor, return_pred, convert_to_numpy)\u001b[0m\n\u001b[0;32m   1475\u001b[0m     pred_y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mparallel\u001b[39m.\u001b[39mdata_parallel(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork, x, [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mother_device)\n\u001b[0;32m   1476\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1477\u001b[0m     pred_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork(\u001b[39m*\u001b[39mx)\n\u001b[0;32m   1478\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_function(pred_y, y)\n\u001b[0;32m   1479\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_loss_tensor:\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: LSTMClassifier.forward() missing 1 required positional argument: 'x_lenght'"
     ]
    }
   ],
   "source": [
    "experiment.test(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poutyne.framework import Experiment\n",
    "from poutyne import set_seeds\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "\n",
    "set_seeds(42)\n",
    "hidden_size = 100\n",
    "optimizer = \"ADAM\"\n",
    "\n",
    "directory_name = 'model2/_mlp_optimizer{}/bidirectional'.format(optimizer)\n",
    "\n",
    "bidirectional_experiment = Experiment(directory_name,\n",
    "                        bidirectional_model,\n",
    "                        optimizer = optimizer,\n",
    "                        task=\"classification\",\n",
    "                        loss_function=\"cross_entropy\",\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mEpoch: \u001b[36m 1/50 \u001b[35mStep: \u001b[36m 34/155 \u001b[35m 21.94% |\u001b[35m████▍               \u001b[35m|\u001b[35mETA: \u001b[32m5m11.12s \u001b[35mloss:\u001b[94m 1.815479\u001b[35m acc:\u001b[94m 37.500000"
     ]
    }
   ],
   "source": [
    "logging = bidirectional_experiment.train(train_dataloader, valid_dataloader, epochs=50, disable_tensorboard=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Évaluation et analyse de résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
