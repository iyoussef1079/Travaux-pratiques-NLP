{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65179fa",
   "metadata": {},
   "source": [
    "# *Notebook* à utiliser pour faire le travail pratique # 3 sur l'analyse d'incidents.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du jeu de donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2233dd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "train_data_path = './data/dev_examples.json'\n",
    "new_exemples_path = './data/new_examples.json'\n",
    "test_data_path = './data/test_examples.json'\n",
    "\n",
    "def load_incident_dataset(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        incident_list = json.load(fp)\n",
    "\n",
    "    return incident_list\n",
    "\n",
    "# Load datasets\n",
    "train_data = load_incident_dataset(train_data_path)\n",
    "new_examples = load_incident_dataset(new_exemples_path)\n",
    "test_data_path = load_incident_dataset(test_data_path)\n",
    "\n",
    "# Merge datasets\n",
    "merged_data = train_data + new_examples\n",
    "\n",
    "# merged_data = merged_data[:20]\n",
    "\n",
    "len(merged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer from transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode texts from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34d8bd93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_texts(tokenizer, texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "texts = [incident['text'] for incident in merged_data]\n",
    "encoded_texts = encode_texts(tokenizer, texts)\n",
    "encoded_texts.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceb24f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-EVENT': 0, 'I-EVENT': 1, 'B-ACTIVITY': 2, 'I-ACTIVITY': 3, 'B-WHO': 4, 'I-WHO': 5, 'B-WHERE': 6, 'I-WHERE': 7, 'B-WHEN': 8, 'I-WHEN': 9, 'B-CAUSE': 10, 'I-CAUSE': 11, 'B-EQUIPMENT': 12, 'I-EQUIPMENT': 13, 'B-INJURY': 14, 'I-INJURY': 15, 'B-INJURED': 16, 'I-INJURED': 17, 'B-BODY-PARTS': 18, 'I-BODY-PARTS': 19, 'B-DEATH': 20, 'I-DEATH': 21, 'O': 22}\n"
     ]
    }
   ],
   "source": [
    "slot_labels = [\"B-EVENT\", \"I-EVENT\", \"B-ACTIVITY\", \"I-ACTIVITY\", \"B-WHO\", \"I-WHO\", \"B-WHERE\", \"I-WHERE\", \"B-WHEN\", \"I-WHEN\", \"B-CAUSE\", \"I-CAUSE\", \"B-EQUIPMENT\", \"I-EQUIPMENT\", \"B-INJURY\", \"I-INJURY\", \"B-INJURED\", \"I-INJURED\", \"B-BODY-PARTS\", \"I-BODY-PARTS\", \"B-DEATH\", \"I-DEATH\", \"O\"]\n",
    "slot_label_to_id = {label: i for i, label in enumerate(slot_labels)}\n",
    "\n",
    "print(slot_label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a0b0d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I-CAUSE']\n"
     ]
    }
   ],
   "source": [
    "def get_slot_from_word(word, data):\n",
    "    found_in = []\n",
    "    for argument, values in data['arguments'].items():\n",
    "        # Check if the word is in any of the values for this argument\n",
    "        for value in values:\n",
    "            if word in value:\n",
    "                # find index of word in value\n",
    "                word_index = value.index(word)\n",
    "                found_in.append(\"B-\" + argument) if word_index == 0 else found_in.append('I-' + argument)\n",
    "\n",
    "    return found_in\n",
    "\n",
    "print(get_slot_from_word('driver', train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def align_tokens_with_all_slots_bert(data, slot_label_to_id, tokenizer):\n",
    "    number_slots = len(slot_label_to_id)\n",
    "    other_array = np.zeros(number_slots)\n",
    "    other_array[-1] = 1 # O slot\n",
    "\n",
    "    aligned_token_slots = []\n",
    "    words = data[\"text\"].split()\n",
    "\n",
    "    aligned_token_slots.append(other_array) # [CLS] token\n",
    "\n",
    "    for word in words:\n",
    "        tokens = tokenizer.tokenize(word)\n",
    "        expected_slots = get_slot_from_word(word, data)  # This can now be a list of slots\n",
    "\n",
    "        for bert_token in tokens:\n",
    "            # Here, each token is represented by a list, with the first element being the token\n",
    "            # and the subsequent elements being flags for each slot\n",
    "            token_with_slots = []\n",
    "\n",
    "            # Adding flags for each slot\n",
    "            for slot_label in slot_label_to_id.keys():\n",
    "                slot_flag = 1 if slot_label in expected_slots else 0\n",
    "                token_with_slots.append(slot_flag)\n",
    "\n",
    "            # if token_with_slots does not contain any slot, then it is an O token\n",
    "            if sum(token_with_slots) == 0:\n",
    "                token_with_slots[-1] = 1 # -1 is the index of the O slot (last slot)\n",
    "\n",
    "            aligned_token_slots.append(token_with_slots)\n",
    "\n",
    "    aligned_token_slots.append(other_array) # [SEP] token\n",
    "\n",
    "    return aligned_token_slots\n",
    "\n",
    "\n",
    "exple = {\n",
    "        \"text\": \"John had an accident at the construction site while walking.\",\n",
    "        \"arguments\": {\n",
    "            \"EVENT\": [\"accident\"],\n",
    "            \"ACTIVITY\": [\"walking\"],\n",
    "            \"WHO\": [\"John\"],\n",
    "            \"WHERE\": [\"construction site\"]\n",
    "        }\n",
    "    }\n",
    "encoded_slots_matrix = align_tokens_with_all_slots_bert(exple, slot_label_to_id, tokenizer)\n",
    "print(encoded_slots_matrix[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "torch.Size([512, 23])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class SlotDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, slot_label_to_id, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.slot_label_to_id = slot_label_to_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx][\"text\"]\n",
    "        arguments = self.texts[idx][\"arguments\"]\n",
    "\n",
    "        # Tokenize text and align labels\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        labels = align_tokens_with_all_slots_bert(self.texts[idx], self.slot_label_to_id, self.tokenizer)\n",
    "\n",
    "        # Adjust the labels to match the length of the tokenized input\n",
    "        # Truncate or pad the labels\n",
    "        padded_labels = []\n",
    "        for label in labels:\n",
    "            if len(padded_labels) < self.max_len:\n",
    "                padded_labels.append(label)\n",
    "            else:\n",
    "                break\n",
    "        while len(padded_labels) < self.max_len:\n",
    "            padded_labels.append([0] * len(self.slot_label_to_id))  # Padding\n",
    "\n",
    "        padded_labels = np.array(padded_labels)\n",
    "        padded_labels = torch.tensor(padded_labels, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': padded_labels\n",
    "        }\n",
    "\n",
    "# Your dataset\n",
    "# texts = [exple]\n",
    "\n",
    "# Create dataset\n",
    "dataset = SlotDataset(merged_data, tokenizer, slot_label_to_id)\n",
    "\n",
    "print(dataset[0][\"input_ids\"].shape)\n",
    "print(dataset[0][\"labels\"].shape)\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46fa5340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, BertTokenizer, AdamW\n",
    "import torch\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "num_labels = len(slot_label_to_id)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef3fdd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 5\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a19c165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 0.2874, Accuracy: 0.9387\n",
      "Epoch 2/5 - Loss: 0.1527, Accuracy: 0.9763\n",
      "Epoch 3/5 - Loss: 0.1316, Accuracy: 0.9763\n",
      "Epoch 4/5 - Loss: 0.1195, Accuracy: 0.9763\n",
      "Epoch 5/5 - Loss: 0.1103, Accuracy: 0.9763\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "import torch\n",
    "\n",
    "def calculate_accuracy(logits, labels):\n",
    "    # Applying sigmoid to logits and rounding to get predictions\n",
    "    preds = torch.sigmoid(logits) > 0.5\n",
    "    correct_preds = (preds == labels).float()\n",
    "    accuracy = correct_preds.sum() / correct_preds.numel()\n",
    "    return accuracy.item()\n",
    "\n",
    "model.train()\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Convert labels to float\n",
    "        labels = batch['labels'].float()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits.view(-1, num_labels), labels.view(-1, num_labels))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = calculate_accuracy(logits, labels)\n",
    "        total_accuracy += accuracy\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Compute average loss and accuracy\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_accuracy = total_accuracy / len(data_loader)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_slots(text, model, tokenizer, slot_label_to_id, threshold=0.5):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    # Run model to get logits\n",
    "    model.eval()  # Put model in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Apply sigmoid and threshold\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs > threshold).int()\n",
    "\n",
    "    # Convert predictions to label names\n",
    "    id_to_label = {v: k for k, v in slot_label_to_id.items()}\n",
    "    predicted_labels = [[id_to_label.get(idx) for idx, val in enumerate(row) if val == 1] for row in preds.squeeze().tolist()]\n",
    "\n",
    "    # Tokenized text for reference\n",
    "    tokenized_text = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "\n",
    "    return list(zip(tokenized_text, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On August 27  2013  Employees #1 and #2  of Templar Inc.  a construction  company specializing in fiber optic installation and services  were working  along a highway. The highway speed limit was posted at 55 miles per hour.  Employee #1 was marking the location of an underground line that ran below the  turn lane. Employee #2 was next to Employee #1 and performing the duties of a  flagger. A privately owned vehicle was travelling in the travel/through lane.  The vehicle veered to the right  entered the turn lane  and struck both  workers. Emergency medical services were called. Employee #1 was declared dead  at the scene. Employee #2 refused emergency medical treatment for the bruises  he received when struck.\n",
      "[('[CLS]', []), ('on', []), ('august', []), ('27', []), ('2013', []), ('employees', []), ('#', []), ('1', []), ('and', []), ('#', []), ('2', []), ('of', []), ('templar', []), ('inc', []), ('.', []), ('a', []), ('construction', []), ('company', []), ('specializing', []), ('in', []), ('fiber', []), ('optic', []), ('installation', []), ('and', []), ('services', []), ('were', []), ('working', []), ('along', []), ('a', []), ('highway', []), ('.', ['O']), ('the', []), ('highway', []), ('speed', []), ('limit', []), ('was', []), ('posted', []), ('at', []), ('55', []), ('miles', []), ('per', []), ('hour', []), ('.', ['O']), ('employee', []), ('#', []), ('1', []), ('was', []), ('marking', []), ('the', []), ('location', []), ('of', []), ('an', []), ('underground', []), ('line', []), ('that', []), ('ran', []), ('below', []), ('the', []), ('turn', []), ('lane', []), ('.', []), ('employee', []), ('#', []), ('2', []), ('was', []), ('next', []), ('to', []), ('employee', []), ('#', []), ('1', []), ('and', []), ('performing', []), ('the', []), ('duties', []), ('of', []), ('a', []), ('flag', []), ('##ger', []), ('.', []), ('a', []), ('privately', []), ('owned', []), ('vehicle', []), ('was', []), ('travelling', []), ('in', []), ('the', []), ('travel', []), ('/', []), ('through', []), ('lane', []), ('.', []), ('the', []), ('vehicle', []), ('ve', []), ('##ered', []), ('to', []), ('the', []), ('right', []), ('entered', []), ('the', []), ('turn', []), ('lane', []), ('and', []), ('struck', []), ('both', []), ('workers', []), ('.', []), ('emergency', []), ('medical', []), ('services', []), ('were', []), ('called', []), ('.', []), ('employee', []), ('#', []), ('1', []), ('was', []), ('declared', []), ('dead', []), ('at', []), ('the', []), ('scene', []), ('.', []), ('employee', []), ('#', []), ('2', []), ('refused', []), ('emergency', []), ('medical', []), ('treatment', []), ('for', []), ('the', []), ('bruises', []), ('he', []), ('received', []), ('when', []), ('struck', []), ('.', []), ('[SEP]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', [])]\n"
     ]
    }
   ],
   "source": [
    "def construct_response(predicted_slots):\n",
    "    # Aggregate slot values\n",
    "    slot_values = {}\n",
    "    for token, slots in predicted_slots:\n",
    "        for slot in slots:\n",
    "            if slot not in slot_values:\n",
    "                slot_values[slot] = []\n",
    "            slot_values[slot].append(token)\n",
    "\n",
    "    # Format the response\n",
    "    response = \"Identified Slots:\\n\"\n",
    "    for slot, values in slot_values.items():\n",
    "        # Join the tokens for each slot and add to the response\n",
    "        token_str = \" \".join(values).replace(' ##', '')  # Handling subword tokens\n",
    "        response += f\"- {slot}: {token_str}\\n\"\n",
    "\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "text = test_data_path[0][\"text\"]\n",
    "print(text)\n",
    "\n",
    "predicted_slots = infer_slots(text, model, tokenizer, slot_label_to_id)\n",
    "response = construct_response(predicted_slots)\n",
    "print(predicted_slots)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
