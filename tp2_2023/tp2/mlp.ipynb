{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tâche #1 : Classification d'incidents avec un réseau *feedforward* et des *embeddings* Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On reprend la classification des descriptions d’accidents du premier travail. Le corpus de textes contient 3 partitions : \n",
    "-\tun fichier d’entraînement -  data/incidents_train.json\n",
    "-\tun fichier de validation -  data/incidents_dev.json\n",
    "-\tun fichier de test - data/incidents_test.json\n",
    "\n",
    "Entraînez un modèle de réseau de neurones de type feedforward multicouche (MLP) avec plongements de mots pour déterminer le type d’un incident à partir de sa description. \n",
    "\n",
    "Voici les consignes pour cette tâche : \n",
    "\n",
    "-\tNom du notebook : mlp.ipynb\n",
    "-\tTokenisation : Utilisation de Spacy. \n",
    "-\tPlongements de mots : Ceux de Spacy. \n",
    "-\tNormalisation : Aucune normalisation. \n",
    "-\tAgrégation des plongements de mots : Comparer les approches max, average et min pooling. \n",
    "-\tStructure du réseau : 1 seule couche cachée dont vous choisirez la taille (à expliquer). \n",
    "-\tPrésentez clairement vos résultats et faites-en l’analyse. En cas de doute, inspirez-vous de ce qui a été fait dans le travail pratique #1. \n",
    "\n",
    "Vous pouvez ajouter au *notebook* toutes les cellules dont vous avez besoin pour votre code, vos explications ou la présentation de vos résultats. Vous pouvez également ajouter des sous-sections (par ex. des sous-sections 1.1, 1.2 etc.) si cela améliore la lisibilité.\n",
    "\n",
    "Notes :\n",
    "- Évitez les bouts de code trop longs ou trop complexes. Par exemple, il est difficile de comprendre 4-5 boucles ou conditions imbriquées. Si c'est le cas, définissez des sous-fonctions pour refactoriser et simplifier votre code. \n",
    "- Expliquez sommairement votre démarche.\n",
    "- Expliquez les choix que vous faites au niveau de la programmation et des modèles (si non trivial).\n",
    "- Analyser vos résultats. Indiquez ce que vous observez, si c'est bon ou non, si c'est surprenant, etc. \n",
    "- Une analyse quantitative et qualitative d'erreurs est intéressante et permet de mieux comprendre le comportement d'un modèle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Création du jeu de données (*dataset*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_model = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train data: text_size 2475, target_size 2475'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Dev data: text_size 531, target_size 531'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Test data: text_size 531, target_size 531'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' At approximately 8:50 a.m. on October 29  1997  Employee #1 was painting a  single story house at 2657 7th Ave  Sacramento  CA. He was caulking around the  peak of the roof line on the west side of the house  20 ft above the ground.  He was working off of a 24 ft aluminum extension ladder so that his feet were  approximately 12 to 13 feet above the ground. Employee #1 fell and suffered a  concussion and two dislocated discs in his lower back and was hospitalized.  The ladder was not secured to prevent movement.                                 '"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "# Assurez-vous que le modèle de langue de spacy est téléchargé\n",
    "# python -m spacy download fr_core_news_md (par exemple pour le français)\n",
    "\n",
    "# Charger le modèle de langue de spacy\n",
    "\n",
    "# Définition des chemins vers les fichiers de données\n",
    "train_data_path = './data/incidents_train.json'\n",
    "dev_data_path = './data/incidents_dev.json'\n",
    "test_data_path = './data/incidents_test.json'\n",
    "\n",
    "def load_incident_dataset(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        incident_list = json.load(fp)\n",
    "        \n",
    "        text = [item[\"text\"] for item in incident_list]\n",
    "        target = np.array([int(item[\"label\"]) for item in incident_list])\n",
    "         \n",
    "    return text, target\n",
    "\n",
    "\n",
    "# Créer les DataFrames pour chaque partition de données\n",
    "train_list, train_target = load_incident_dataset(train_data_path)\n",
    "dev_list, dev_target = load_incident_dataset(dev_data_path)\n",
    "test_list, test_target = load_incident_dataset(test_data_path)\n",
    "\n",
    "# Affichage de l'information de base sur les DataFrames\n",
    "display(f\"Train data: text_size {len(train_list)}, target_size {len(train_target)}\")\n",
    "display(f\"Dev data: text_size {len(dev_list)}, target_size {len(dev_target)}\")\n",
    "display(f\"Test data: text_size {len(test_list)}, target_size {len(test_target)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Vérification des premiers enregistrements dans l'ensemble d'entraînement\n",
    "train_list[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gestion de plongements de mots (*embeddings*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_embeddings = {}\n",
    "\n",
    "# for train in train_list:\n",
    "#     doc = spacy_model(train['text'])\n",
    "#     for token in doc:\n",
    "#         word_embeddings[token.text] = token.vector\n",
    "\n",
    "# nb_dim = 50\n",
    "# word = \"painting\"\n",
    "# print(\"\\nLes {} premières valeurs du vecteur de plongement du mot \\\"{}\\\": \\n{}...\".format(nb_dim, word, word_embeddings[word][:nb_dim]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for train in train_list:\n",
    "    docs.append(spacy_model(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def average_embedding(sentence, nlp_model=spacy_model):\n",
    "    tokenised_sentence = nlp_model(sentence)  # tokenized\n",
    "    nb_column = len(tokenised_sentence)\n",
    "    nb_rows =  nlp_model.vocab.vectors_length\n",
    "    sentence_embedding_matrix = np.zeros((nb_rows, nb_column))                                  \n",
    "    for index, token in enumerate(tokenised_sentence):\n",
    "        sentence_embedding_matrix[:, index] = token.vector\n",
    "    return np.average(sentence_embedding_matrix, axis=1)\n",
    "\n",
    "def maxpool_embedding(sentence, nlp_model=spacy_model): \n",
    "    tokenised_sentence = nlp_model(sentence)\n",
    "    nb_column = len(tokenised_sentence)\n",
    "    nb_rows =  nlp_model.vocab.vectors_length \n",
    "    sentence_embedding_matrix = np.zeros((nb_rows, nb_column))                                    \n",
    "    for index, token in enumerate(tokenised_sentence):\n",
    "        sentence_embedding_matrix[:, index] = token.vector\n",
    "    return np.max(sentence_embedding_matrix, axis=1)\n",
    "\n",
    "def minpool_embedding(sentence, nlp_model=spacy_model):\n",
    "    tokenised_sentence = nlp_model(sentence)\n",
    "    nb_column = len(tokenised_sentence)\n",
    "    nb_rows =  nlp_model.vocab.vectors_length \n",
    "    sentence_embedding_matrix = np.zeros((nb_rows, nb_column))                                    \n",
    "    for index, token in enumerate(tokenised_sentence):\n",
    "        sentence_embedding_matrix[:, index] = token.vector\n",
    "    return np.min(sentence_embedding_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.96873228e+00, -5.10729270e-01, -2.35349378e+00,  4.97672888e-01,\n",
       "        4.88353263e+00, -5.49114923e-01,  5.77439719e-01,  4.37476939e+00,\n",
       "        9.82608505e-01,  2.76181211e-01,  3.32291929e+00,  1.83452836e+00,\n",
       "       -2.19539340e+00,  7.13141577e-01,  4.67925282e-01,  1.69569208e+00,\n",
       "       -4.15554943e-01,  5.46220097e-01, -8.17133067e-01, -1.81754243e+00,\n",
       "        1.45045962e+00, -1.64859988e-01,  7.35189274e-01,  1.39185669e+00,\n",
       "       -2.16275255e-01, -1.53836771e+00, -2.90574922e+00, -1.10088644e+00,\n",
       "       -2.45176188e-01,  1.23890522e+00,  7.16629948e-01,  2.70870438e-01,\n",
       "       -4.62952329e-01, -2.58849184e+00, -2.63352788e+00, -1.38033428e+00,\n",
       "       -6.32679274e-01,  1.24110528e+00,  4.78200293e-01,  2.11488465e-02,\n",
       "        1.16837779e+00, -2.70511874e-01, -1.43659251e-01,  9.54270908e-01,\n",
       "       -1.01153907e+00,  1.14068031e+00,  1.59027040e-01, -2.24387173e-01,\n",
       "       -2.60219337e-03,  1.40369611e+00, -4.18182354e-01,  1.69740604e+00,\n",
       "       -1.48766517e+00, -2.45625057e+00,  1.64713725e-01,  1.28808839e+00,\n",
       "        8.62606266e-02,  4.15198956e-01,  4.98442410e-01, -5.37761597e-01,\n",
       "       -1.20520101e+00, -1.13710915e+00, -6.59722981e-01, -4.61622877e-01,\n",
       "        9.79431047e-01,  7.08585172e-01, -1.01729620e+00, -1.13611561e+00,\n",
       "        7.19882042e-01,  1.85753255e+00, -5.66562619e-01, -1.42907024e+00,\n",
       "       -1.65274502e+00, -1.15747813e-01,  9.91672008e-02,  1.03055562e+00,\n",
       "       -1.64221492e+00,  2.05763068e+00, -2.56929823e+00,  7.85430761e-02,\n",
       "       -2.75466940e+00, -5.84539363e-01, -1.63984744e-01, -3.80676353e-01,\n",
       "        1.95624249e+00,  1.70345415e-01, -1.26160681e+00, -1.61548122e+00,\n",
       "       -8.32527352e-02, -5.94322507e-01, -1.27867962e+00,  5.98755919e-01,\n",
       "        1.62920096e+00, -1.44040656e+00,  1.08236223e+00, -2.66479838e-01,\n",
       "        1.63859324e+00, -1.74873323e-01,  1.54814026e+00,  1.17597607e+00,\n",
       "        1.35302079e+00,  7.78074987e-01,  3.28567882e+00,  1.84762997e+00,\n",
       "        3.32296035e-01,  1.82446759e+00, -3.53985316e-01, -2.17733662e+00,\n",
       "       -1.34298582e+00, -3.08889922e+00, -1.42204216e-01,  5.42597832e-01,\n",
       "       -9.57087020e-01,  5.14289913e-01,  6.15424550e-01, -1.67098265e-01,\n",
       "       -8.12221410e-01, -2.05723570e-01,  4.63872684e-01, -1.08724221e+00,\n",
       "        3.09349468e-01, -2.82193602e+00,  2.87569028e-01,  6.95764417e-01,\n",
       "       -1.17024027e+00, -2.86958866e+00,  1.30798503e+00, -1.76611095e+00,\n",
       "        2.10508720e+00, -6.62218154e-01, -2.80313200e+00, -9.01863532e-02,\n",
       "        4.11104895e+00,  3.32299838e-01,  8.18393217e-01,  2.29702159e-01,\n",
       "       -9.21297337e-01,  2.66674112e-01,  1.53976315e+00, -1.20156723e+00,\n",
       "       -7.02824641e-01, -2.17315579e-02,  2.38082683e-01,  8.87372972e-01,\n",
       "        2.31748150e+00,  1.73541509e+00, -3.68527398e+00, -4.03775594e-01,\n",
       "        1.74377023e+00,  1.18632791e+00,  5.63058614e-01,  8.99596926e-01,\n",
       "        5.27105703e-01,  8.64436802e-01,  5.26340472e-03, -2.17649929e-01,\n",
       "        1.65743548e+00, -6.08410644e-01, -1.11949530e+00, -2.15546253e+00,\n",
       "       -1.34228401e+00, -2.18393996e+00,  1.87448469e+00,  1.21739417e+00,\n",
       "       -7.07873372e-01,  1.02100749e-01, -2.20846534e+00, -4.87494622e-01,\n",
       "        7.05138816e-01,  2.08219933e+00,  1.23224026e-01,  9.16752811e-01,\n",
       "        8.52226765e-01, -6.63847013e-01,  1.00772443e+00,  1.49246591e+00,\n",
       "       -6.47708071e-01, -1.90595963e+00, -2.46991214e+00, -2.19778659e+00,\n",
       "       -8.70971470e-01, -5.56545272e-02, -2.96860380e-01,  6.60292969e-01,\n",
       "       -1.10994549e+00,  1.72842832e-01, -7.74296783e-01,  5.30756227e-02,\n",
       "       -4.40770815e-01,  8.99063989e-01,  7.21476239e-01, -1.20376129e+00,\n",
       "        5.67012176e-01, -2.54384665e+00,  6.56247949e-01,  3.75476704e-01,\n",
       "       -1.60708201e+00, -1.09388633e+00, -9.74294109e-01,  4.37457085e-01,\n",
       "        9.66083330e-02, -2.26582406e+00,  4.12299641e-01, -2.51909335e+00,\n",
       "        3.52094564e+00,  2.73591996e-01, -1.86047153e+00,  1.39799713e+00,\n",
       "       -7.08549824e-01, -4.46750174e-02, -2.03775233e-01,  1.49421349e+00,\n",
       "       -1.35233676e+00,  1.60254189e+00,  1.09645992e+00,  7.91542614e-01,\n",
       "        1.68371097e+00, -1.30475499e+00, -1.44171668e+00, -1.45109134e+00,\n",
       "       -1.36317211e+00,  1.06909485e+00, -1.07901798e+00, -2.74204980e-01,\n",
       "       -1.91685147e+00, -1.88006396e+00,  1.66656842e-01,  2.78838851e+00,\n",
       "        1.10033130e+00,  1.01545347e-01,  2.59105533e+00, -2.50196448e+00,\n",
       "        1.76116530e-01,  2.32385743e+00,  1.92074370e+00,  1.91828095e+00,\n",
       "       -1.10651456e+00,  6.49842954e-01, -1.35461708e+00, -1.10663825e-01,\n",
       "        1.85675622e-01, -1.88465076e-01,  2.73124013e-01, -8.24189863e-02,\n",
       "       -6.73801658e-01,  2.94731682e-01, -1.83029973e+00,  1.50096592e+00,\n",
       "       -7.29662726e-03,  1.89332610e+00,  1.24892995e+00, -1.85086565e+00,\n",
       "       -3.89937046e+00,  4.03489439e-01, -7.45697393e-01, -3.57439990e-01,\n",
       "        1.60210832e+00,  1.03600199e-01,  1.42626886e+00, -7.72361350e-02,\n",
       "        1.24209665e+00,  4.33458589e+00,  3.11159260e+00,  1.16459602e+00,\n",
       "        1.24675497e+00, -2.16846758e+00,  2.87291458e-01,  1.50374453e+00,\n",
       "       -1.65664237e+00,  6.87693177e-01,  8.12787855e-01, -1.50196641e-02,\n",
       "       -3.78258125e-01, -2.21603893e+00,  4.29642058e-01,  5.79907377e-01,\n",
       "        6.13420526e-01, -6.29024197e-01, -1.58631465e-01,  7.08892060e-01,\n",
       "        2.77624264e+00,  1.02410194e+00,  1.71450248e+00,  1.58539266e+00,\n",
       "        2.15115849e+00, -1.24033993e+00, -1.63436324e+00, -1.55968347e-02,\n",
       "       -1.51300579e+00, -3.70410255e-01, -6.97573604e-01,  8.93375907e-01,\n",
       "       -9.00048171e-01,  1.25225933e+00, -9.34773770e-01,  1.24417414e+00,\n",
       "        1.14301540e+00, -1.98515399e+00, -1.44004622e+00,  1.03731580e+00])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(average_embedding(train_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Création de modèle(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_layer_size, output_size) :\n",
    "        super().__init__()\n",
    "        self.intput_layer = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.output_layer = nn.Linear(hidden_layer_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.intput_layer(x)\n",
    "        x = nn.functional.relu_(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import FloatTensor, LongTensor\n",
    "from typing import List\n",
    "\n",
    "class SpacyDataset(Dataset):\n",
    "    def __init__(self, dataset: List[str] , target: np.array, sentence_aggregation_function):\n",
    "        self.dataset = dataset\n",
    "        self.doc_embeddings = [None for _ in range(len(dataset))]\n",
    "        self.sentence_aggregation_function = sentence_aggregation_function \n",
    "        self.target = target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.doc_embeddings[index] is None:\n",
    "            self.doc_embeddings[index] = self.sentence_aggregation_function(self.dataset[index])  \n",
    "        return FloatTensor(self.doc_embeddings[index]), LongTensor([self.target[index]]).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un dictionnaire pour choisir le type d'agrégation\n",
    "aggregation = {\n",
    "    \"average\" : average_embedding,\n",
    "    \"maxpool\" : maxpool_embedding,\n",
    "    \"minpool\" : minpool_embedding\n",
    "}\n",
    "\n",
    "# On détermine ici comment la classe SpacyDataset construit la représentation d'un texte\n",
    "# par l'agrégation des représentations de mots. \n",
    "# Choix possibles: \"average\", \"maxpool\", \"minpool\"\n",
    "aggregation_function = aggregation[\"average\"]  \n",
    "\n",
    "# On finalise la construction des 3 jeux de données et leurs dataloaders\n",
    "train_dataset = SpacyDataset(train_list, train_target, aggregation_function)\n",
    "valid_dataset = SpacyDataset(dev_list, dev_target, aggregation_function)\n",
    "test_dataset = SpacyDataset(test_list, test_target, aggregation_function)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fonctions utilitaires\n",
    "\n",
    "Vous pouvez mettre ici toutes les fonctions qui seront utiles pour les sections suivantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement de modèle(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des plongements de Spacy: 300\n",
      "Nombre de classes: 9\n"
     ]
    }
   ],
   "source": [
    "embedding_size = spacy_model.meta['vectors']['width'] # La dimension des vecteurs d'embeddings de Spacy\n",
    "nb_classes = len([i for i in range(9)])\n",
    "\n",
    "print(\"Taille des plongements de Spacy:\", embedding_size)\n",
    "print(\"Nombre de classes:\", nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLayerPerceptron(\n",
       "  (intput_layer): Linear(in_features=300, out_features=500, bias=True)\n",
       "  (output_layer): Linear(in_features=500, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from poutyne.framework import Experiment\n",
    "from poutyne import set_seeds\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "\n",
    "set_seeds(42)\n",
    "hidden_size = 100\n",
    "\n",
    "directory_name = 'model/{}_mlp'.format(aggregation_function.__name__)  \n",
    "\n",
    "model = MultiLayerPerceptron(embedding_size, hidden_size, nb_classes)\n",
    "experiment = Experiment(directory_name, \n",
    "                        model, \n",
    "                        optimizer = \"ADAM\", \n",
    "                        task=\"classification\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/200 Train steps: 155 Val steps: 34 0.40s loss: 1.503999 acc: 47.070707 fscore_macro: 0.185021 val_loss: 1.272199 val_acc: 53.107345 val_fscore_macro: 0.255907\n",
      "Epoch 1: val_acc improved from -inf to 53.10734, saving file to model/average_embedding_mlp/checkpoint_epoch_1.ckpt\n",
      "Epoch:   2/200 Train steps: 155 Val steps: 34 0.55s loss: 1.192977 acc: 58.181818 fscore_macro: 0.318504 val_loss: 1.104100 val_acc: 61.770245 val_fscore_macro: 0.393248\n",
      "Epoch 2: val_acc improved from 53.10734 to 61.77024, saving file to model/average_embedding_mlp/checkpoint_epoch_2.ckpt\n",
      "Epoch:   3/200 Train steps: 155 Val steps: 34 0.52s loss: 1.097715 acc: 61.252525 fscore_macro: 0.380844 val_loss: 1.057427 val_acc: 62.523540 val_fscore_macro: 0.328856\n",
      "Epoch 3: val_acc improved from 61.77024 to 62.52354, saving file to model/average_embedding_mlp/checkpoint_epoch_3.ckpt\n",
      "Epoch:   4/200 Train steps: 155 Val steps: 34 0.44s loss: 1.016884 acc: 64.282828 fscore_macro: 0.436080 val_loss: 1.015536 val_acc: 65.913371 val_fscore_macro: 0.428083\n",
      "Epoch 4: val_acc improved from 62.52354 to 65.91337, saving file to model/average_embedding_mlp/checkpoint_epoch_4.ckpt\n",
      "Epoch:   5/200 Train steps: 155 Val steps: 34 0.46s loss: 0.948216 acc: 66.505051 fscore_macro: 0.472521 val_loss: 1.052393 val_acc: 63.088512 val_fscore_macro: 0.438624\n",
      "Epoch:   6/200 Train steps: 155 Val steps: 34 0.48s loss: 0.925442 acc: 66.747475 fscore_macro: 0.518067 val_loss: 1.046701 val_acc: 64.406780 val_fscore_macro: 0.381716\n",
      "Epoch:   7/200 Train steps: 155 Val steps: 34 0.49s loss: 0.875512 acc: 68.000000 fscore_macro: 0.544602 val_loss: 0.944942 val_acc: 68.173258 val_fscore_macro: 0.468704\n",
      "Epoch 7: val_acc improved from 65.91337 to 68.17326, saving file to model/average_embedding_mlp/checkpoint_epoch_7.ckpt\n",
      "Epoch:   8/200 Train steps: 155 Val steps: 34 0.49s loss: 0.851676 acc: 69.333333 fscore_macro: 0.545687 val_loss: 1.009339 val_acc: 67.984934 val_fscore_macro: 0.439167\n",
      "Epoch:   9/200 Train steps: 155 Val steps: 34 0.55s loss: 0.827947 acc: 69.656566 fscore_macro: 0.562058 val_loss: 0.940310 val_acc: 67.231638 val_fscore_macro: 0.440309\n",
      "Epoch:  10/200 Train steps: 155 Val steps: 34 0.44s loss: 0.815863 acc: 69.575758 fscore_macro: 0.600565 val_loss: 0.917579 val_acc: 68.738230 val_fscore_macro: 0.509867\n",
      "Epoch 10: val_acc improved from 68.17326 to 68.73823, saving file to model/average_embedding_mlp/checkpoint_epoch_10.ckpt\n",
      "Epoch:  11/200 Train steps: 155 Val steps: 34 0.42s loss: 0.771770 acc: 71.474747 fscore_macro: 0.635427 val_loss: 0.922390 val_acc: 68.173258 val_fscore_macro: 0.480574\n",
      "Epoch:  12/200 Train steps: 155 Val steps: 34 0.47s loss: 0.803422 acc: 71.030303 fscore_macro: 0.622317 val_loss: 1.043725 val_acc: 64.783427 val_fscore_macro: 0.513540\n",
      "Epoch:  13/200 Train steps: 155 Val steps: 34 0.45s loss: 0.764602 acc: 72.202020 fscore_macro: 0.631527 val_loss: 0.913801 val_acc: 69.679849 val_fscore_macro: 0.578078\n",
      "Epoch 13: val_acc improved from 68.73823 to 69.67985, saving file to model/average_embedding_mlp/checkpoint_epoch_13.ckpt\n",
      "Epoch:  14/200 Train steps: 155 Val steps: 34 0.42s loss: 0.723184 acc: 73.494949 fscore_macro: 0.655238 val_loss: 0.928075 val_acc: 68.361582 val_fscore_macro: 0.563528\n",
      "Epoch:  15/200 Train steps: 155 Val steps: 34 0.42s loss: 0.728777 acc: 72.767677 fscore_macro: 0.682959 val_loss: 0.932400 val_acc: 69.491525 val_fscore_macro: 0.512917\n",
      "Epoch:  16/200 Train steps: 155 Val steps: 34 0.40s loss: 0.692534 acc: 74.787879 fscore_macro: 0.683960 val_loss: 0.902976 val_acc: 70.809793 val_fscore_macro: 0.547398\n",
      "Epoch 16: val_acc improved from 69.67985 to 70.80979, saving file to model/average_embedding_mlp/checkpoint_epoch_16.ckpt\n",
      "Epoch:  17/200 Train steps: 155 Val steps: 34 0.51s loss: 0.681867 acc: 75.151515 fscore_macro: 0.727621 val_loss: 0.940148 val_acc: 67.419962 val_fscore_macro: 0.504015\n",
      "Epoch:  18/200 Train steps: 155 Val steps: 34 0.48s loss: 0.676910 acc: 75.313131 fscore_macro: 0.704930 val_loss: 0.947502 val_acc: 68.361582 val_fscore_macro: 0.500396\n",
      "Epoch:  19/200 Train steps: 155 Val steps: 34 0.41s loss: 0.650679 acc: 76.848485 fscore_macro: 0.726087 val_loss: 0.962256 val_acc: 69.679849 val_fscore_macro: 0.522351\n",
      "Epoch:  20/200 Train steps: 155 Val steps: 34 0.40s loss: 0.642365 acc: 76.727273 fscore_macro: 0.734264 val_loss: 0.937397 val_acc: 69.114878 val_fscore_macro: 0.573029\n",
      "Epoch:  21/200 Train steps: 155 Val steps: 34 0.39s loss: 0.629077 acc: 77.333333 fscore_macro: 0.748242 val_loss: 1.099701 val_acc: 64.406780 val_fscore_macro: 0.475644\n",
      "Epoch:  22/200 Train steps: 155 Val steps: 34 0.41s loss: 0.614396 acc: 78.101010 fscore_macro: 0.776905 val_loss: 1.074609 val_acc: 66.290019 val_fscore_macro: 0.518579\n",
      "Epoch:  23/200 Train steps: 155 Val steps: 34 0.40s loss: 0.600238 acc: 77.696970 fscore_macro: 0.776195 val_loss: 0.953791 val_acc: 68.738230 val_fscore_macro: 0.531858\n",
      "Epoch:  24/200 Train steps: 155 Val steps: 34 0.43s loss: 0.591650 acc: 78.101010 fscore_macro: 0.767918 val_loss: 1.046442 val_acc: 65.160075 val_fscore_macro: 0.467397\n",
      "Epoch:  25/200 Train steps: 155 Val steps: 34 0.45s loss: 0.583106 acc: 79.434343 fscore_macro: 0.795151 val_loss: 1.095447 val_acc: 63.653484 val_fscore_macro: 0.492054\n",
      "Epoch:  26/200 Train steps: 155 Val steps: 34 0.49s loss: 0.568322 acc: 79.272727 fscore_macro: 0.797194 val_loss: 0.989445 val_acc: 68.549906 val_fscore_macro: 0.505317\n",
      "Epoch:  27/200 Train steps: 155 Val steps: 34 0.47s loss: 0.539065 acc: 80.080808 fscore_macro: 0.806219 val_loss: 1.114468 val_acc: 64.030132 val_fscore_macro: 0.482842\n",
      "Epoch:  28/200 Train steps: 155 Val steps: 34 0.43s loss: 0.535737 acc: 80.969697 fscore_macro: 0.805303 val_loss: 1.109893 val_acc: 63.653484 val_fscore_macro: 0.488419\n",
      "Epoch:  29/200 Train steps: 155 Val steps: 34 0.42s loss: 0.545477 acc: 80.525253 fscore_macro: 0.809320 val_loss: 1.087939 val_acc: 65.913371 val_fscore_macro: 0.528387\n",
      "Epoch:  30/200 Train steps: 155 Val steps: 34 0.41s loss: 0.517316 acc: 81.414141 fscore_macro: 0.818860 val_loss: 1.129301 val_acc: 65.536723 val_fscore_macro: 0.513921\n",
      "Epoch:  31/200 Train steps: 155 Val steps: 34 0.41s loss: 0.506722 acc: 80.767677 fscore_macro: 0.822543 val_loss: 1.170061 val_acc: 64.783427 val_fscore_macro: 0.517026\n",
      "Epoch:  32/200 Train steps: 155 Val steps: 34 0.44s loss: 0.482389 acc: 81.454545 fscore_macro: 0.830420 val_loss: 1.056118 val_acc: 67.984934 val_fscore_macro: 0.556778\n",
      "Epoch:  33/200 Train steps: 155 Val steps: 34 0.44s loss: 0.484926 acc: 82.626263 fscore_macro: 0.846767 val_loss: 1.146985 val_acc: 65.913371 val_fscore_macro: 0.524069\n",
      "Epoch:  34/200 Train steps: 155 Val steps: 34 0.41s loss: 0.466112 acc: 83.030303 fscore_macro: 0.844018 val_loss: 1.232709 val_acc: 60.640301 val_fscore_macro: 0.547236\n",
      "Epoch:  35/200 Train steps: 155 Val steps: 34 0.42s loss: 0.469844 acc: 83.878788 fscore_macro: 0.849488 val_loss: 1.028586 val_acc: 70.998117 val_fscore_macro: 0.545839\n",
      "Epoch 35: val_acc improved from 70.80979 to 70.99812, saving file to model/average_embedding_mlp/checkpoint_epoch_35.ckpt\n",
      "Epoch:  36/200 Train steps: 155 Val steps: 34 0.40s loss: 0.434727 acc: 83.919192 fscore_macro: 0.856217 val_loss: 1.101407 val_acc: 67.984934 val_fscore_macro: 0.486943\n",
      "Epoch:  37/200 Train steps: 155 Val steps: 34 0.43s loss: 0.439153 acc: 84.121212 fscore_macro: 0.855556 val_loss: 1.039035 val_acc: 68.361582 val_fscore_macro: 0.573848\n",
      "Epoch:  38/200 Train steps: 155 Val steps: 34 0.42s loss: 0.420346 acc: 84.525253 fscore_macro: 0.868796 val_loss: 1.130322 val_acc: 65.348399 val_fscore_macro: 0.480082\n",
      "Epoch:  39/200 Train steps: 155 Val steps: 34 0.40s loss: 0.407840 acc: 85.252525 fscore_macro: 0.877378 val_loss: 1.138550 val_acc: 67.231638 val_fscore_macro: 0.492996\n",
      "Epoch:  40/200 Train steps: 155 Val steps: 34 0.39s loss: 0.421575 acc: 84.202020 fscore_macro: 0.864380 val_loss: 1.073503 val_acc: 67.796610 val_fscore_macro: 0.577900\n",
      "Epoch:  41/200 Train steps: 155 Val steps: 34 0.40s loss: 0.389337 acc: 85.535354 fscore_macro: 0.884087 val_loss: 1.240907 val_acc: 63.653484 val_fscore_macro: 0.584276\n",
      "Epoch:  42/200 Train steps: 155 Val steps: 34 0.43s loss: 0.384111 acc: 85.898990 fscore_macro: 0.881960 val_loss: 1.117910 val_acc: 67.984934 val_fscore_macro: 0.531487\n",
      "Epoch:  43/200 Train steps: 155 Val steps: 34 0.40s loss: 0.422556 acc: 84.202020 fscore_macro: 0.861000 val_loss: 1.236063 val_acc: 62.523541 val_fscore_macro: 0.515070\n",
      "Epoch:  44/200 Train steps: 155 Val steps: 34 0.38s loss: 0.386576 acc: 86.262626 fscore_macro: 0.878557 val_loss: 1.199108 val_acc: 66.290019 val_fscore_macro: 0.569418\n",
      "Epoch:  45/200 Train steps: 155 Val steps: 34 0.47s loss: 0.362871 acc: 86.989899 fscore_macro: 0.884118 val_loss: 1.148853 val_acc: 68.549906 val_fscore_macro: 0.558369\n",
      "Epoch:  46/200 Train steps: 155 Val steps: 34 0.46s loss: 0.341750 acc: 87.838384 fscore_macro: 0.906895 val_loss: 1.182081 val_acc: 65.913371 val_fscore_macro: 0.532653\n",
      "Epoch:  47/200 Train steps: 155 Val steps: 34 0.45s loss: 0.335718 acc: 87.797980 fscore_macro: 0.907081 val_loss: 1.148799 val_acc: 69.114878 val_fscore_macro: 0.570226\n",
      "Epoch:  48/200 Train steps: 155 Val steps: 34 0.41s loss: 0.323654 acc: 88.000000 fscore_macro: 0.906528 val_loss: 1.198457 val_acc: 66.478343 val_fscore_macro: 0.529237\n",
      "Epoch:  49/200 Train steps: 155 Val steps: 34 0.43s loss: 0.350290 acc: 87.151515 fscore_macro: 0.894612 val_loss: 1.314559 val_acc: 65.913371 val_fscore_macro: 0.485158\n",
      "Epoch:  50/200 Train steps: 155 Val steps: 34 0.40s loss: 0.333437 acc: 87.595960 fscore_macro: 0.904567 val_loss: 1.172123 val_acc: 68.549906 val_fscore_macro: 0.551573\n",
      "Epoch:  51/200 Train steps: 155 Val steps: 34 0.39s loss: 0.286155 acc: 89.292929 fscore_macro: 0.911954 val_loss: 1.301799 val_acc: 66.290019 val_fscore_macro: 0.495135\n",
      "Epoch:  52/200 Train steps: 155 Val steps: 34 0.60s loss: 0.295486 acc: 88.969697 fscore_macro: 0.918107 val_loss: 1.211464 val_acc: 68.173258 val_fscore_macro: 0.557572\n",
      "Epoch:  53/200 Train steps: 155 Val steps: 34 0.47s loss: 0.300507 acc: 89.050505 fscore_macro: 0.918426 val_loss: 1.171074 val_acc: 68.361582 val_fscore_macro: 0.617073\n",
      "Epoch:  54/200 Train steps: 155 Val steps: 34 0.47s loss: 0.262145 acc: 91.232323 fscore_macro: 0.934047 val_loss: 1.326545 val_acc: 66.290019 val_fscore_macro: 0.493405\n",
      "Epoch:  55/200 Train steps: 155 Val steps: 34 0.38s loss: 0.281577 acc: 89.939394 fscore_macro: 0.905142 val_loss: 1.362236 val_acc: 66.478343 val_fscore_macro: 0.543302\n",
      "Epoch:  56/200 Train steps: 155 Val steps: 34 0.39s loss: 0.286523 acc: 89.414141 fscore_macro: 0.909825 val_loss: 1.271036 val_acc: 68.738230 val_fscore_macro: 0.577966\n",
      "Epoch:  57/200 Train steps: 155 Val steps: 34 0.38s loss: 0.262582 acc: 90.626263 fscore_macro: 0.915811 val_loss: 1.305384 val_acc: 67.419962 val_fscore_macro: 0.511191\n",
      "Epoch:  58/200 Train steps: 155 Val steps: 34 0.34s loss: 0.268488 acc: 89.656566 fscore_macro: 0.914586 val_loss: 1.306358 val_acc: 68.738230 val_fscore_macro: 0.507236\n",
      "Epoch:  59/200 Train steps: 155 Val steps: 34 0.38s loss: 0.254193 acc: 91.070707 fscore_macro: 0.935793 val_loss: 1.282318 val_acc: 68.926554 val_fscore_macro: 0.589162\n",
      "Epoch:  60/200 Train steps: 155 Val steps: 34 0.38s loss: 0.253505 acc: 90.747475 fscore_macro: 0.929337 val_loss: 1.278490 val_acc: 69.114878 val_fscore_macro: 0.572625\n",
      "Epoch:  61/200 Train steps: 155 Val steps: 34 0.36s loss: 0.226397 acc: 92.121212 fscore_macro: 0.937726 val_loss: 1.397029 val_acc: 67.231638 val_fscore_macro: 0.561620\n",
      "Epoch:  62/200 Train steps: 155 Val steps: 34 0.37s loss: 0.277065 acc: 89.454545 fscore_macro: 0.914382 val_loss: 1.274150 val_acc: 68.738230 val_fscore_macro: 0.555552\n",
      "Epoch:  63/200 Train steps: 155 Val steps: 34 0.33s loss: 0.202086 acc: 92.929293 fscore_macro: 0.947771 val_loss: 1.422317 val_acc: 65.725047 val_fscore_macro: 0.545662\n",
      "Epoch:  64/200 Train steps: 155 Val steps: 34 0.33s loss: 0.194256 acc: 93.575758 fscore_macro: 0.952216 val_loss: 1.393621 val_acc: 67.796610 val_fscore_macro: 0.502676\n",
      "Epoch:  65/200 Train steps: 155 Val steps: 34 0.36s loss: 0.193594 acc: 93.696970 fscore_macro: 0.948541 val_loss: 1.395508 val_acc: 67.984934 val_fscore_macro: 0.579962\n",
      "Epoch:  66/200 Train steps: 155 Val steps: 34 0.35s loss: 0.186258 acc: 93.818182 fscore_macro: 0.951832 val_loss: 1.610032 val_acc: 64.783428 val_fscore_macro: 0.496801\n",
      "Epoch:  67/200 Train steps: 155 Val steps: 34 0.36s loss: 0.190977 acc: 93.575758 fscore_macro: 0.953208 val_loss: 1.445762 val_acc: 67.419962 val_fscore_macro: 0.579732\n",
      "Epoch:  68/200 Train steps: 155 Val steps: 34 0.32s loss: 0.178133 acc: 93.939394 fscore_macro: 0.956741 val_loss: 1.396960 val_acc: 70.621469 val_fscore_macro: 0.602363\n",
      "Epoch:  69/200 Train steps: 155 Val steps: 34 0.39s loss: 0.177662 acc: 94.101010 fscore_macro: 0.956185 val_loss: 1.429695 val_acc: 67.796610 val_fscore_macro: 0.542527\n",
      "Epoch:  70/200 Train steps: 155 Val steps: 34 0.34s loss: 0.210419 acc: 92.565657 fscore_macro: 0.934824 val_loss: 1.608604 val_acc: 68.361582 val_fscore_macro: 0.492264\n",
      "Epoch:  71/200 Train steps: 155 Val steps: 34 0.34s loss: 0.181311 acc: 93.898990 fscore_macro: 0.954954 val_loss: 1.490252 val_acc: 67.043315 val_fscore_macro: 0.579746\n",
      "Epoch:  72/200 Train steps: 155 Val steps: 34 0.33s loss: 0.160679 acc: 94.828283 fscore_macro: 0.961430 val_loss: 1.481892 val_acc: 69.491525 val_fscore_macro: 0.582579\n",
      "Epoch:  73/200 Train steps: 155 Val steps: 34 0.34s loss: 0.132073 acc: 96.363636 fscore_macro: 0.974614 val_loss: 1.662364 val_acc: 68.549906 val_fscore_macro: 0.570878\n",
      "Epoch:  74/200 Train steps: 155 Val steps: 34 0.33s loss: 0.133698 acc: 96.080808 fscore_macro: 0.970735 val_loss: 1.572630 val_acc: 67.608286 val_fscore_macro: 0.525678\n",
      "Epoch:  75/200 Train steps: 155 Val steps: 34 0.33s loss: 0.155688 acc: 94.989899 fscore_macro: 0.963461 val_loss: 1.518818 val_acc: 67.419962 val_fscore_macro: 0.567781\n",
      "Epoch:  76/200 Train steps: 155 Val steps: 34 0.36s loss: 0.132772 acc: 96.080808 fscore_macro: 0.964765 val_loss: 1.656146 val_acc: 64.218456 val_fscore_macro: 0.474347\n",
      "Epoch:  77/200 Train steps: 155 Val steps: 34 0.36s loss: 0.121802 acc: 96.646465 fscore_macro: 0.976894 val_loss: 1.610201 val_acc: 66.290019 val_fscore_macro: 0.548259\n",
      "Epoch:  78/200 Train steps: 155 Val steps: 34 0.39s loss: 0.152767 acc: 94.949495 fscore_macro: 0.960454 val_loss: 1.562562 val_acc: 69.491525 val_fscore_macro: 0.572013\n",
      "Epoch:  79/200 Train steps: 155 Val steps: 34 0.37s loss: 0.159310 acc: 94.666667 fscore_macro: 0.953950 val_loss: 1.582397 val_acc: 67.796610 val_fscore_macro: 0.518436\n",
      "Epoch:  80/200 Train steps: 155 Val steps: 34 0.31s loss: 0.140600 acc: 95.434343 fscore_macro: 0.965190 val_loss: 1.556419 val_acc: 68.926554 val_fscore_macro: 0.563453\n",
      "Epoch:  81/200 Train steps: 155 Val steps: 34 0.34s loss: 0.123240 acc: 96.161616 fscore_macro: 0.962750 val_loss: 1.666177 val_acc: 66.478343 val_fscore_macro: 0.557425\n",
      "Epoch:  82/200 Train steps: 155 Val steps: 34 0.41s loss: 0.113501 acc: 96.565657 fscore_macro: 0.976885 val_loss: 1.716784 val_acc: 67.608286 val_fscore_macro: 0.565415\n",
      "Epoch:  83/200 Train steps: 155 Val steps: 34 0.34s loss: 0.118496 acc: 96.282828 fscore_macro: 0.972484 val_loss: 1.592942 val_acc: 69.868173 val_fscore_macro: 0.571808\n",
      "Epoch:  84/200 Train steps: 155 Val steps: 34 0.35s loss: 0.114609 acc: 96.404040 fscore_macro: 0.974223 val_loss: 1.613654 val_acc: 67.984934 val_fscore_macro: 0.562889\n",
      "Epoch:  85/200 Train steps: 155 Val steps: 34 0.33s loss: 0.109622 acc: 96.686869 fscore_macro: 0.974331 val_loss: 1.698012 val_acc: 67.608286 val_fscore_macro: 0.525796\n",
      "Epoch:  86/200 Train steps: 155 Val steps: 34 0.33s loss: 0.128473 acc: 95.838384 fscore_macro: 0.964706 val_loss: 1.740954 val_acc: 65.348399 val_fscore_macro: 0.489435\n",
      "Epoch:  87/200 Train steps: 155 Val steps: 34 0.33s loss: 0.147511 acc: 94.666667 fscore_macro: 0.954463 val_loss: 1.855850 val_acc: 66.101695 val_fscore_macro: 0.505418\n",
      "Epoch:  88/200 Train steps: 155 Val steps: 34 0.35s loss: 0.084431 acc: 97.737374 fscore_macro: 0.984071 val_loss: 1.830352 val_acc: 65.725047 val_fscore_macro: 0.559915\n",
      "Epoch:  89/200 Train steps: 155 Val steps: 34 0.35s loss: 0.082386 acc: 97.656566 fscore_macro: 0.982173 val_loss: 1.859472 val_acc: 65.913371 val_fscore_macro: 0.515795\n",
      "Epoch:  90/200 Train steps: 155 Val steps: 34 0.35s loss: 0.138227 acc: 94.828283 fscore_macro: 0.962145 val_loss: 1.847544 val_acc: 66.478343 val_fscore_macro: 0.571532\n",
      "Epoch:  91/200 Train steps: 155 Val steps: 34 0.33s loss: 0.098761 acc: 97.252525 fscore_macro: 0.977907 val_loss: 1.849078 val_acc: 67.608286 val_fscore_macro: 0.517649\n",
      "Epoch:  92/200 Train steps: 155 Val steps: 34 0.35s loss: 0.085264 acc: 97.737374 fscore_macro: 0.985111 val_loss: 2.043390 val_acc: 63.088512 val_fscore_macro: 0.548957\n",
      "Epoch:  93/200 Train steps: 155 Val steps: 34 0.36s loss: 0.098949 acc: 97.010101 fscore_macro: 0.979397 val_loss: 1.798445 val_acc: 67.984934 val_fscore_macro: 0.556333\n",
      "Epoch:  94/200 Train steps: 155 Val steps: 34 0.36s loss: 0.047765 acc: 99.070707 fscore_macro: 0.993843 val_loss: 1.822212 val_acc: 67.419962 val_fscore_macro: 0.550772\n",
      "Epoch:  95/200 Train steps: 155 Val steps: 34 0.36s loss: 0.060826 acc: 98.626263 fscore_macro: 0.990469 val_loss: 1.975696 val_acc: 66.290019 val_fscore_macro: 0.510586\n",
      "Epoch:  96/200 Train steps: 155 Val steps: 34 0.35s loss: 0.112715 acc: 96.121212 fscore_macro: 0.973956 val_loss: 1.845584 val_acc: 67.419962 val_fscore_macro: 0.507978\n",
      "Epoch:  97/200 Train steps: 155 Val steps: 34 0.33s loss: 0.075373 acc: 98.060606 fscore_macro: 0.986348 val_loss: 1.977285 val_acc: 65.536723 val_fscore_macro: 0.552086\n",
      "Epoch:  98/200 Train steps: 155 Val steps: 34 0.37s loss: 0.056298 acc: 98.585859 fscore_macro: 0.990520 val_loss: 1.950570 val_acc: 67.231638 val_fscore_macro: 0.553891\n",
      "Epoch:  99/200 Train steps: 155 Val steps: 34 0.33s loss: 0.071057 acc: 98.101010 fscore_macro: 0.986636 val_loss: 1.914750 val_acc: 68.173258 val_fscore_macro: 0.547917\n",
      "Epoch: 100/200 Train steps: 155 Val steps: 34 0.35s loss: 0.055083 acc: 98.707071 fscore_macro: 0.989699 val_loss: 1.960144 val_acc: 67.043315 val_fscore_macro: 0.550920\n",
      "Epoch: 101/200 Train steps: 155 Val steps: 34 0.33s loss: 0.104720 acc: 96.363636 fscore_macro: 0.966408 val_loss: 2.055598 val_acc: 68.361582 val_fscore_macro: 0.496029\n",
      "Epoch: 102/200 Train steps: 155 Val steps: 34 0.33s loss: 0.127584 acc: 95.313131 fscore_macro: 0.943686 val_loss: 2.035681 val_acc: 67.608286 val_fscore_macro: 0.499892\n",
      "Epoch: 103/200 Train steps: 155 Val steps: 34 0.96s loss: 0.103835 acc: 96.242424 fscore_macro: 0.968160 val_loss: 2.008348 val_acc: 68.361582 val_fscore_macro: 0.567119\n",
      "Epoch: 104/200 Train steps: 155 Val steps: 34 0.62s loss: 0.111218 acc: 95.919192 fscore_macro: 0.954448 val_loss: 2.032325 val_acc: 66.478343 val_fscore_macro: 0.568835\n",
      "Epoch: 105/200 Train steps: 155 Val steps: 34 0.43s loss: 0.102385 acc: 96.929293 fscore_macro: 0.974507 val_loss: 2.049413 val_acc: 66.666667 val_fscore_macro: 0.510769\n",
      "Epoch: 106/200 Train steps: 155 Val steps: 34 0.62s loss: 0.059610 acc: 98.303030 fscore_macro: 0.988244 val_loss: 2.050510 val_acc: 66.666667 val_fscore_macro: 0.530340\n",
      "Epoch: 107/200 Train steps: 155 Val steps: 34 0.51s loss: 0.030601 acc: 99.676768 fscore_macro: 0.997784 val_loss: 2.044480 val_acc: 68.361582 val_fscore_macro: 0.563675\n",
      "Epoch: 108/200 Train steps: 155 Val steps: 34 0.38s loss: 0.018002 acc: 99.878788 fscore_macro: 0.999309 val_loss: 2.110655 val_acc: 67.419962 val_fscore_macro: 0.552002\n",
      "Epoch: 109/200 Train steps: 155 Val steps: 34 0.32s loss: 0.024884 acc: 99.717172 fscore_macro: 0.997282 val_loss: 2.098506 val_acc: 65.913371 val_fscore_macro: 0.495490\n",
      "Epoch: 110/200 Train steps: 155 Val steps: 34 0.36s loss: 0.105583 acc: 96.404040 fscore_macro: 0.964655 val_loss: 2.295519 val_acc: 65.536723 val_fscore_macro: 0.479165\n",
      "Epoch: 111/200 Train steps: 155 Val steps: 34 0.35s loss: 0.120783 acc: 95.555556 fscore_macro: 0.943495 val_loss: 2.072424 val_acc: 67.043315 val_fscore_macro: 0.517528\n",
      "Epoch: 112/200 Train steps: 155 Val steps: 34 0.35s loss: 0.081453 acc: 97.696970 fscore_macro: 0.972784 val_loss: 2.110197 val_acc: 66.854991 val_fscore_macro: 0.512068\n",
      "Epoch: 113/200 Train steps: 155 Val steps: 34 0.32s loss: 0.078909 acc: 97.333333 fscore_macro: 0.977689 val_loss: 2.205530 val_acc: 68.173258 val_fscore_macro: 0.529940\n",
      "Epoch: 114/200 Train steps: 155 Val steps: 34 0.37s loss: 0.029843 acc: 99.474747 fscore_macro: 0.996272 val_loss: 2.115078 val_acc: 67.231638 val_fscore_macro: 0.543726\n",
      "Epoch: 115/200 Train steps: 155 Val steps: 34 0.33s loss: 0.071647 acc: 97.616162 fscore_macro: 0.982389 val_loss: 2.164080 val_acc: 68.173258 val_fscore_macro: 0.556874\n",
      "Epoch: 116/200 Train steps: 155 Val steps: 34 0.33s loss: 0.019759 acc: 99.797980 fscore_macro: 0.998444 val_loss: 2.197481 val_acc: 66.854991 val_fscore_macro: 0.552639\n",
      "Epoch: 117/200 Train steps: 155 Val steps: 34 0.32s loss: 0.012384 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.116792 val_acc: 69.303202 val_fscore_macro: 0.590036\n",
      "Epoch: 118/200 Train steps: 155 Val steps: 34 0.34s loss: 0.009889 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.295889 val_acc: 66.290019 val_fscore_macro: 0.503967\n",
      "Epoch: 119/200 Train steps: 155 Val steps: 34 0.32s loss: 0.012525 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.151779 val_acc: 67.608286 val_fscore_macro: 0.550058\n",
      "Epoch: 120/200 Train steps: 155 Val steps: 34 0.35s loss: 0.011067 acc: 99.959596 fscore_macro: 0.999820 val_loss: 2.158431 val_acc: 66.854991 val_fscore_macro: 0.547712\n",
      "Epoch: 121/200 Train steps: 155 Val steps: 34 0.33s loss: 0.011162 acc: 99.959596 fscore_macro: 0.999733 val_loss: 2.169417 val_acc: 67.796610 val_fscore_macro: 0.566492\n",
      "Epoch: 122/200 Train steps: 155 Val steps: 34 0.36s loss: 0.008953 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.206409 val_acc: 68.361582 val_fscore_macro: 0.568546\n",
      "Epoch: 123/200 Train steps: 155 Val steps: 34 0.39s loss: 0.254886 acc: 92.848485 fscore_macro: 0.934288 val_loss: 2.807937 val_acc: 59.510358 val_fscore_macro: 0.437276\n",
      "Epoch: 124/200 Train steps: 155 Val steps: 34 0.40s loss: 0.353050 acc: 88.202020 fscore_macro: 0.852896 val_loss: 2.231254 val_acc: 66.290019 val_fscore_macro: 0.503659\n",
      "Epoch: 125/200 Train steps: 155 Val steps: 34 0.34s loss: 0.051367 acc: 98.787879 fscore_macro: 0.988509 val_loss: 2.281141 val_acc: 64.783428 val_fscore_macro: 0.561294\n",
      "Epoch: 126/200 Train steps: 155 Val steps: 34 0.33s loss: 0.021033 acc: 99.959596 fscore_macro: 0.999820 val_loss: 2.123398 val_acc: 68.738230 val_fscore_macro: 0.565470\n",
      "Epoch: 127/200 Train steps: 155 Val steps: 34 0.34s loss: 0.014432 acc: 99.959596 fscore_macro: 0.999610 val_loss: 2.188645 val_acc: 68.926554 val_fscore_macro: 0.528483\n",
      "Epoch: 128/200 Train steps: 155 Val steps: 34 0.32s loss: 0.016738 acc: 99.797980 fscore_macro: 0.998336 val_loss: 2.315164 val_acc: 67.419962 val_fscore_macro: 0.515132\n",
      "Epoch: 129/200 Train steps: 155 Val steps: 34 0.34s loss: 0.013649 acc: 99.959596 fscore_macro: 0.999820 val_loss: 2.243801 val_acc: 68.173258 val_fscore_macro: 0.512541\n",
      "Epoch: 130/200 Train steps: 155 Val steps: 34 0.34s loss: 0.011420 acc: 99.959596 fscore_macro: 0.999820 val_loss: 2.327957 val_acc: 66.854991 val_fscore_macro: 0.501285\n",
      "Epoch: 131/200 Train steps: 155 Val steps: 34 0.34s loss: 0.008965 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.254615 val_acc: 67.984934 val_fscore_macro: 0.513624\n",
      "Epoch: 132/200 Train steps: 155 Val steps: 34 0.35s loss: 0.008650 acc: 99.959596 fscore_macro: 0.999820 val_loss: 2.257238 val_acc: 68.173258 val_fscore_macro: 0.556794\n",
      "Epoch: 133/200 Train steps: 155 Val steps: 34 0.33s loss: 0.009367 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.262522 val_acc: 67.043315 val_fscore_macro: 0.509459\n",
      "Epoch: 134/200 Train steps: 155 Val steps: 34 0.34s loss: 0.009495 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.303256 val_acc: 68.173258 val_fscore_macro: 0.567081\n",
      "Epoch: 135/200 Train steps: 155 Val steps: 34 0.36s loss: 0.108592 acc: 96.606061 fscore_macro: 0.965820 val_loss: 2.895694 val_acc: 64.218456 val_fscore_macro: 0.475939\n",
      "Epoch: 136/200 Train steps: 155 Val steps: 34 0.34s loss: 0.339792 acc: 88.080808 fscore_macro: 0.868071 val_loss: 2.365254 val_acc: 60.640301 val_fscore_macro: 0.470378\n",
      "Epoch: 137/200 Train steps: 155 Val steps: 34 0.34s loss: 0.147072 acc: 94.989899 fscore_macro: 0.950328 val_loss: 2.382912 val_acc: 66.478343 val_fscore_macro: 0.508931\n",
      "Epoch: 138/200 Train steps: 155 Val steps: 34 0.35s loss: 0.050553 acc: 98.828283 fscore_macro: 0.991509 val_loss: 2.210432 val_acc: 67.043315 val_fscore_macro: 0.570523\n",
      "Epoch: 139/200 Train steps: 155 Val steps: 34 0.34s loss: 0.020577 acc: 99.797980 fscore_macro: 0.998386 val_loss: 2.277664 val_acc: 67.796610 val_fscore_macro: 0.562586\n",
      "Epoch: 140/200 Train steps: 155 Val steps: 34 0.35s loss: 0.021909 acc: 99.717172 fscore_macro: 0.998441 val_loss: 2.286963 val_acc: 67.984934 val_fscore_macro: 0.562642\n",
      "Epoch: 141/200 Train steps: 155 Val steps: 34 0.37s loss: 0.012906 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.318741 val_acc: 67.984934 val_fscore_macro: 0.575936\n",
      "Epoch: 142/200 Train steps: 155 Val steps: 34 0.38s loss: 0.008284 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.336712 val_acc: 68.926554 val_fscore_macro: 0.536270\n",
      "Epoch: 143/200 Train steps: 155 Val steps: 34 0.36s loss: 0.010946 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.319710 val_acc: 67.796610 val_fscore_macro: 0.517022\n",
      "Epoch: 144/200 Train steps: 155 Val steps: 34 0.35s loss: 0.008266 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.321466 val_acc: 68.361582 val_fscore_macro: 0.567374\n",
      "Epoch: 145/200 Train steps: 155 Val steps: 34 0.35s loss: 0.007678 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.329134 val_acc: 67.796610 val_fscore_macro: 0.516505\n",
      "Epoch: 146/200 Train steps: 155 Val steps: 34 0.35s loss: 0.005855 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.328256 val_acc: 69.303202 val_fscore_macro: 0.579331\n",
      "Epoch: 147/200 Train steps: 155 Val steps: 34 0.33s loss: 0.007360 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.386355 val_acc: 68.361582 val_fscore_macro: 0.569215\n",
      "Epoch: 148/200 Train steps: 155 Val steps: 34 0.35s loss: 0.006350 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.366435 val_acc: 67.608286 val_fscore_macro: 0.521059\n",
      "Epoch: 149/200 Train steps: 155 Val steps: 34 0.34s loss: 0.005349 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.391136 val_acc: 66.666667 val_fscore_macro: 0.503876\n",
      "Epoch: 150/200 Train steps: 155 Val steps: 34 0.36s loss: 0.005559 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.441548 val_acc: 67.984934 val_fscore_macro: 0.520337\n",
      "Epoch: 151/200 Train steps: 155 Val steps: 34 0.36s loss: 0.598140 acc: 85.818182 fscore_macro: 0.837200 val_loss: 2.229655 val_acc: 63.465160 val_fscore_macro: 0.495546\n",
      "Epoch: 152/200 Train steps: 155 Val steps: 34 0.33s loss: 0.297685 acc: 89.292929 fscore_macro: 0.884583 val_loss: 2.228103 val_acc: 65.913371 val_fscore_macro: 0.517038\n",
      "Epoch: 153/200 Train steps: 155 Val steps: 34 0.34s loss: 0.107892 acc: 96.080808 fscore_macro: 0.959891 val_loss: 2.168679 val_acc: 65.160075 val_fscore_macro: 0.501165\n",
      "Epoch: 154/200 Train steps: 155 Val steps: 34 0.34s loss: 0.067012 acc: 98.020202 fscore_macro: 0.979610 val_loss: 2.401965 val_acc: 64.595104 val_fscore_macro: 0.546030\n",
      "Epoch: 155/200 Train steps: 155 Val steps: 34 0.37s loss: 0.034659 acc: 99.272727 fscore_macro: 0.995230 val_loss: 2.187425 val_acc: 66.290019 val_fscore_macro: 0.503165\n",
      "Epoch: 156/200 Train steps: 155 Val steps: 34 0.35s loss: 0.017811 acc: 99.878788 fscore_macro: 0.998953 val_loss: 2.258909 val_acc: 67.608286 val_fscore_macro: 0.560425\n",
      "Epoch: 157/200 Train steps: 155 Val steps: 34 0.37s loss: 0.016228 acc: 99.959596 fscore_macro: 0.999698 val_loss: 2.325804 val_acc: 66.666667 val_fscore_macro: 0.555640\n",
      "Epoch: 158/200 Train steps: 155 Val steps: 34 0.33s loss: 0.011061 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.293158 val_acc: 67.796610 val_fscore_macro: 0.574196\n",
      "Epoch: 159/200 Train steps: 155 Val steps: 34 0.37s loss: 0.011408 acc: 99.959596 fscore_macro: 0.999820 val_loss: 2.310504 val_acc: 66.290019 val_fscore_macro: 0.507116\n",
      "Epoch: 160/200 Train steps: 155 Val steps: 34 0.34s loss: 0.012189 acc: 99.919192 fscore_macro: 0.999641 val_loss: 2.305928 val_acc: 67.043315 val_fscore_macro: 0.513777\n",
      "Epoch: 161/200 Train steps: 155 Val steps: 34 0.35s loss: 0.008960 acc: 99.959596 fscore_macro: 0.999733 val_loss: 2.371723 val_acc: 66.666667 val_fscore_macro: 0.512026\n",
      "Epoch: 162/200 Train steps: 155 Val steps: 34 0.33s loss: 0.007779 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.402417 val_acc: 66.478343 val_fscore_macro: 0.550215\n",
      "Epoch: 163/200 Train steps: 155 Val steps: 34 0.33s loss: 0.007680 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.383428 val_acc: 66.854991 val_fscore_macro: 0.549560\n",
      "Epoch: 164/200 Train steps: 155 Val steps: 34 0.38s loss: 0.007339 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.390310 val_acc: 67.043315 val_fscore_macro: 0.558577\n",
      "Epoch: 165/200 Train steps: 155 Val steps: 34 0.35s loss: 0.006463 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.425292 val_acc: 67.984934 val_fscore_macro: 0.562787\n",
      "Epoch: 166/200 Train steps: 155 Val steps: 34 0.34s loss: 0.005889 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.385086 val_acc: 67.231638 val_fscore_macro: 0.563108\n",
      "Epoch: 167/200 Train steps: 155 Val steps: 34 0.35s loss: 0.006543 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.438997 val_acc: 68.361582 val_fscore_macro: 0.577110\n",
      "Epoch: 168/200 Train steps: 155 Val steps: 34 0.37s loss: 0.005546 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.500685 val_acc: 67.043315 val_fscore_macro: 0.510341\n",
      "Epoch: 169/200 Train steps: 155 Val steps: 34 0.36s loss: 0.428812 acc: 87.676768 fscore_macro: 0.864647 val_loss: 2.837220 val_acc: 55.178908 val_fscore_macro: 0.406850\n",
      "Epoch: 170/200 Train steps: 155 Val steps: 34 0.36s loss: 0.282854 acc: 90.787879 fscore_macro: 0.895671 val_loss: 2.280230 val_acc: 64.971751 val_fscore_macro: 0.530400\n",
      "Epoch: 171/200 Train steps: 155 Val steps: 34 0.34s loss: 0.091609 acc: 96.848485 fscore_macro: 0.952080 val_loss: 2.466066 val_acc: 63.465160 val_fscore_macro: 0.484129\n",
      "Epoch: 172/200 Train steps: 155 Val steps: 34 0.34s loss: 0.035075 acc: 99.474747 fscore_macro: 0.990308 val_loss: 2.283293 val_acc: 67.231638 val_fscore_macro: 0.566648\n",
      "Epoch: 173/200 Train steps: 155 Val steps: 34 0.32s loss: 0.031819 acc: 99.191919 fscore_macro: 0.993222 val_loss: 2.324425 val_acc: 66.854991 val_fscore_macro: 0.489240\n",
      "Epoch: 174/200 Train steps: 155 Val steps: 34 0.36s loss: 0.015839 acc: 99.959596 fscore_macro: 0.999820 val_loss: 2.364608 val_acc: 65.160075 val_fscore_macro: 0.526046\n",
      "Epoch: 175/200 Train steps: 155 Val steps: 34 0.38s loss: 0.011516 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.296501 val_acc: 67.043315 val_fscore_macro: 0.562202\n",
      "Epoch: 176/200 Train steps: 155 Val steps: 34 0.35s loss: 0.010995 acc: 99.959596 fscore_macro: 0.999610 val_loss: 2.347091 val_acc: 66.854991 val_fscore_macro: 0.547135\n",
      "Epoch: 177/200 Train steps: 155 Val steps: 34 0.35s loss: 0.008657 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.369468 val_acc: 66.854991 val_fscore_macro: 0.508225\n",
      "Epoch: 178/200 Train steps: 155 Val steps: 34 0.33s loss: 0.008577 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.364378 val_acc: 67.984934 val_fscore_macro: 0.576916\n",
      "Epoch: 179/200 Train steps: 155 Val steps: 34 0.35s loss: 0.009962 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.335683 val_acc: 66.854991 val_fscore_macro: 0.501693\n",
      "Epoch: 180/200 Train steps: 155 Val steps: 34 0.35s loss: 0.007930 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.404986 val_acc: 67.796610 val_fscore_macro: 0.578656\n",
      "Epoch: 181/200 Train steps: 155 Val steps: 34 0.32s loss: 0.006700 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.413652 val_acc: 67.231638 val_fscore_macro: 0.505496\n",
      "Epoch: 182/200 Train steps: 155 Val steps: 34 0.35s loss: 0.006076 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.408159 val_acc: 66.666667 val_fscore_macro: 0.508182\n",
      "Epoch: 183/200 Train steps: 155 Val steps: 34 0.34s loss: 0.006006 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.424651 val_acc: 67.419962 val_fscore_macro: 0.517510\n",
      "Epoch: 184/200 Train steps: 155 Val steps: 34 0.34s loss: 0.005808 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.436440 val_acc: 67.419962 val_fscore_macro: 0.515773\n",
      "Epoch: 185/200 Train steps: 155 Val steps: 34 0.35s loss: 0.005197 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.447651 val_acc: 66.478343 val_fscore_macro: 0.501965\n",
      "Epoch: 186/200 Train steps: 155 Val steps: 34 0.34s loss: 0.005574 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.431398 val_acc: 67.608286 val_fscore_macro: 0.568635\n",
      "Epoch: 187/200 Train steps: 155 Val steps: 34 0.33s loss: 0.005464 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.463075 val_acc: 67.043315 val_fscore_macro: 0.505244\n",
      "Epoch: 188/200 Train steps: 155 Val steps: 34 0.33s loss: 0.004556 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.465153 val_acc: 66.854991 val_fscore_macro: 0.551741\n",
      "Epoch: 189/200 Train steps: 155 Val steps: 34 0.34s loss: 0.007605 acc: 99.959596 fscore_macro: 0.999669 val_loss: 2.481700 val_acc: 67.231638 val_fscore_macro: 0.547708\n",
      "Epoch: 190/200 Train steps: 155 Val steps: 34 0.35s loss: 0.537032 acc: 85.777778 fscore_macro: 0.829056 val_loss: 2.164220 val_acc: 63.088512 val_fscore_macro: 0.504126\n",
      "Epoch: 191/200 Train steps: 155 Val steps: 34 0.33s loss: 0.159333 acc: 94.464646 fscore_macro: 0.933403 val_loss: 2.490185 val_acc: 64.406780 val_fscore_macro: 0.495980\n",
      "Epoch: 192/200 Train steps: 155 Val steps: 34 0.40s loss: 0.052833 acc: 98.383838 fscore_macro: 0.982855 val_loss: 2.382160 val_acc: 66.478343 val_fscore_macro: 0.495965\n",
      "Epoch: 193/200 Train steps: 155 Val steps: 34 0.36s loss: 0.021030 acc: 99.757576 fscore_macro: 0.997552 val_loss: 2.377290 val_acc: 67.043315 val_fscore_macro: 0.504059\n",
      "Epoch: 194/200 Train steps: 155 Val steps: 34 0.34s loss: 0.011624 acc: 99.959596 fscore_macro: 0.999732 val_loss: 2.404411 val_acc: 66.478343 val_fscore_macro: 0.514466\n",
      "Epoch: 195/200 Train steps: 155 Val steps: 34 0.36s loss: 0.010761 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.402366 val_acc: 66.478343 val_fscore_macro: 0.506216\n",
      "Epoch: 196/200 Train steps: 155 Val steps: 34 0.35s loss: 0.008438 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.440301 val_acc: 66.101695 val_fscore_macro: 0.496945\n",
      "Epoch: 197/200 Train steps: 155 Val steps: 34 0.34s loss: 0.006829 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.475053 val_acc: 66.666667 val_fscore_macro: 0.497183\n",
      "Epoch: 198/200 Train steps: 155 Val steps: 34 0.33s loss: 0.007102 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.455473 val_acc: 67.231638 val_fscore_macro: 0.502356\n",
      "Epoch: 199/200 Train steps: 155 Val steps: 34 0.35s loss: 0.006601 acc: 100.000000 fscore_macro: 1.000000 val_loss: 2.485780 val_acc: 66.666667 val_fscore_macro: 0.492511\n",
      "Epoch: 200/200 Train steps: 155 Val steps: 34 0.34s loss: 0.011146 acc: 99.959596 fscore_macro: 0.999732 val_loss: 2.432829 val_acc: 65.348399 val_fscore_macro: 0.487408\n",
      "Restoring data from model/average_embedding_mlp/checkpoint_epoch_35.ckpt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging = experiment.train(train_dataloader, valid_dataloader, epochs=200, disable_tensorboard=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Évaluation et analyse de résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found best checkpoint at epoch: 35\n",
      "lr: 0.001, loss: 0.469844, acc: 83.8788, fscore_macro: 0.849488, val_loss: 1.02859, val_acc: 70.9981, val_fscore_macro: 0.545839\n",
      "Loading checkpoint model/average_embedding_mlp/checkpoint_epoch_35.ckpt\n",
      "Running test\n",
      "Test steps: 34 0.10s test_loss: 1.041696 test_acc: 68.738230 test_fscore_macro: 0.555658    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'time': 0.09963362501002848,\n",
       " 'test_loss': 1.0416964147127674,\n",
       " 'test_acc': 68.7382297551789,\n",
       " 'test_fscore_macro': 0.5556579828262329}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.test(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
