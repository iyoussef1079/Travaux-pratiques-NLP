{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tâche #1 : Classification d'incidents avec un réseau *feedforward* et des *embeddings* Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On reprend la classification des descriptions d’accidents du premier travail. Le corpus de textes contient 3 partitions : \n",
    "-\tun fichier d’entraînement -  data/incidents_train.json\n",
    "-\tun fichier de validation -  data/incidents_dev.json\n",
    "-\tun fichier de test - data/incidents_test.json\n",
    "\n",
    "Entraînez un modèle de réseau de neurones de type feedforward multicouche (MLP) avec plongements de mots pour déterminer le type d’un incident à partir de sa description. \n",
    "\n",
    "Voici les consignes pour cette tâche : \n",
    "\n",
    "-\tNom du notebook : mlp.ipynb\n",
    "-\tTokenisation : Utilisation de Spacy. \n",
    "-\tPlongements de mots : Ceux de Spacy. \n",
    "-\tNormalisation : Aucune normalisation. \n",
    "-\tAgrégation des plongements de mots : Comparer les approches max, average et min pooling. \n",
    "-\tStructure du réseau : 1 seule couche cachée dont vous choisirez la taille (à expliquer). \n",
    "-\tPrésentez clairement vos résultats et faites-en l’analyse. En cas de doute, inspirez-vous de ce qui a été fait dans le travail pratique #1. \n",
    "\n",
    "Vous pouvez ajouter au *notebook* toutes les cellules dont vous avez besoin pour votre code, vos explications ou la présentation de vos résultats. Vous pouvez également ajouter des sous-sections (par ex. des sous-sections 1.1, 1.2 etc.) si cela améliore la lisibilité.\n",
    "\n",
    "Notes :\n",
    "- Évitez les bouts de code trop longs ou trop complexes. Par exemple, il est difficile de comprendre 4-5 boucles ou conditions imbriquées. Si c'est le cas, définissez des sous-fonctions pour refactoriser et simplifier votre code. \n",
    "- Expliquez sommairement votre démarche.\n",
    "- Expliquez les choix que vous faites au niveau de la programmation et des modèles (si non trivial).\n",
    "- Analyser vos résultats. Indiquez ce que vous observez, si c'est bon ou non, si c'est surprenant, etc. \n",
    "- Une analyse quantitative et qualitative d'erreurs est intéressante et permet de mieux comprendre le comportement d'un modèle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Création du jeu de données (*dataset*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_model = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train data: text_size 2475, target_size 2475'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Dev data: text_size 531, target_size 531'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Test data: text_size 531, target_size 531'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' At approximately 8:50 a.m. on October 29  1997  Employee #1 was painting a  single story house at 2657 7th Ave  Sacramento  CA. He was caulking around the  peak of the roof line on the west side of the house  20 ft above the ground.  He was working off of a 24 ft aluminum extension ladder so that his feet were  approximately 12 to 13 feet above the ground. Employee #1 fell and suffered a  concussion and two dislocated discs in his lower back and was hospitalized.  The ladder was not secured to prevent movement.                                 '"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "# Assurez-vous que le modèle de langue de spacy est téléchargé\n",
    "# python -m spacy download fr_core_news_md (par exemple pour le français)\n",
    "\n",
    "# Charger le modèle de langue de spacy\n",
    "\n",
    "# Définition des chemins vers les fichiers de données\n",
    "train_data_path = './data/incidents_train.json'\n",
    "dev_data_path = './data/incidents_dev.json'\n",
    "test_data_path = './data/incidents_test.json'\n",
    "\n",
    "def load_incident_dataset(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        incident_list = json.load(fp)\n",
    "        \n",
    "        text = [item[\"text\"] for item in incident_list]\n",
    "        target = np.array([int(item[\"label\"]) for item in incident_list])\n",
    "         \n",
    "    return text, target\n",
    "\n",
    "\n",
    "# Créer les DataFrames pour chaque partition de données\n",
    "train_list, train_target = load_incident_dataset(train_data_path)\n",
    "dev_list, dev_target = load_incident_dataset(dev_data_path)\n",
    "test_list, test_target = load_incident_dataset(test_data_path)\n",
    "\n",
    "# Affichage de l'information de base sur les DataFrames\n",
    "display(f\"Train data: text_size {len(train_list)}, target_size {len(train_target)}\")\n",
    "display(f\"Dev data: text_size {len(dev_list)}, target_size {len(dev_target)}\")\n",
    "display(f\"Test data: text_size {len(test_list)}, target_size {len(test_target)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Vérification des premiers enregistrements dans l'ensemble d'entraînement\n",
    "train_list[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce script utilise les bibliothèques pandas, json et numpy pour manipuler les données. Il charge les données d'incidents à partir de fichiers JSON, les organise dans des ensembles de textes et d'étiquettes, crée des DataFrames pour chaque ensemble de données, affiche des informations de base sur ces ensembles, puis vérifie le premier enregistrement dans l'ensemble d'entraînement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gestion de plongements de mots (*embeddings*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_embeddings = {}\n",
    "\n",
    "# for train in train_list:\n",
    "#     doc = spacy_model(train['text'])\n",
    "#     for token in doc:\n",
    "#         word_embeddings[token.text] = token.vector\n",
    "\n",
    "# nb_dim = 50\n",
    "# word = \"painting\"\n",
    "# print(\"\\nLes {} premières valeurs du vecteur de plongement du mot \\\"{}\\\": \\n{}...\".format(nb_dim, word, word_embeddings[word][:nb_dim]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for train in train_list:\n",
    "    docs.append(spacy_model(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Fonction pour calculer l'embedding moyen d'une phrase\n",
    "def average_embedding(sentence, nlp_model=spacy_model):\n",
    "    tokenised_sentence = nlp_model(sentence)      # Tokenisation de la phrase\n",
    "    nb_column = len(tokenised_sentence)     # Nombre de colonnes dans la matrice d'embedding (nombre de tokens)\n",
    "    nb_rows =  nlp_model.vocab.vectors_length  # Nombre de lignes dans la matrice d'embedding (taille de l'espace d'embedding)\n",
    "    sentence_embedding_matrix = np.zeros((nb_rows, nb_column))                                  \n",
    "    for index, token in enumerate(tokenised_sentence):  # Remplissage de la matrice d'embedding avec les vecteurs d'embedding de chaque token\n",
    "        sentence_embedding_matrix[:, index] = token.vector\n",
    "    return np.average(sentence_embedding_matrix, axis=1)  # Calcul de l'embedding moyen en prenant la moyenne le long de l'axe des colonnes\n",
    "\n",
    "# Fonction pour calculer l'embedding avec la technique du max pooling d'une phrase\n",
    "def maxpool_embedding(sentence, nlp_model=spacy_model): \n",
    "    tokenised_sentence = nlp_model(sentence)\n",
    "    nb_column = len(tokenised_sentence)\n",
    "    nb_rows =  nlp_model.vocab.vectors_length \n",
    "    sentence_embedding_matrix = np.zeros((nb_rows, nb_column))                                    \n",
    "    for index, token in enumerate(tokenised_sentence):\n",
    "        sentence_embedding_matrix[:, index] = token.vector\n",
    "    return np.max(sentence_embedding_matrix, axis=1)\n",
    "\n",
    "# Fonction pour calculer l'embedding avec la technique du min pooling d'une phrase\n",
    "def minpool_embedding(sentence, nlp_model=spacy_model):\n",
    "    tokenised_sentence = nlp_model(sentence)\n",
    "    nb_column = len(tokenised_sentence)\n",
    "    nb_rows =  nlp_model.vocab.vectors_length \n",
    "    sentence_embedding_matrix = np.zeros((nb_rows, nb_column))                                    \n",
    "    for index, token in enumerate(tokenised_sentence):\n",
    "        sentence_embedding_matrix[:, index] = token.vector\n",
    "    return np.min(sentence_embedding_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces fonctions prennent une phrase en entrée, la tokenisent à l'aide d'un modèle spaCy (nlp_model), puis calculent l'embedding moyen, l'embedding avec la technique du max pooling, ou l'embedding avec la technique du min pooling en utilisant les vecteurs d'embedding des tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.96873228e+00, -5.10729270e-01, -2.35349378e+00,  4.97672888e-01,\n",
       "        4.88353263e+00, -5.49114923e-01,  5.77439719e-01,  4.37476939e+00,\n",
       "        9.82608505e-01,  2.76181211e-01,  3.32291929e+00,  1.83452836e+00,\n",
       "       -2.19539340e+00,  7.13141577e-01,  4.67925282e-01,  1.69569208e+00,\n",
       "       -4.15554943e-01,  5.46220097e-01, -8.17133067e-01, -1.81754243e+00,\n",
       "        1.45045962e+00, -1.64859988e-01,  7.35189274e-01,  1.39185669e+00,\n",
       "       -2.16275255e-01, -1.53836771e+00, -2.90574922e+00, -1.10088644e+00,\n",
       "       -2.45176188e-01,  1.23890522e+00,  7.16629948e-01,  2.70870438e-01,\n",
       "       -4.62952329e-01, -2.58849184e+00, -2.63352788e+00, -1.38033428e+00,\n",
       "       -6.32679274e-01,  1.24110528e+00,  4.78200293e-01,  2.11488465e-02,\n",
       "        1.16837779e+00, -2.70511874e-01, -1.43659251e-01,  9.54270908e-01,\n",
       "       -1.01153907e+00,  1.14068031e+00,  1.59027040e-01, -2.24387173e-01,\n",
       "       -2.60219337e-03,  1.40369611e+00, -4.18182354e-01,  1.69740604e+00,\n",
       "       -1.48766517e+00, -2.45625057e+00,  1.64713725e-01,  1.28808839e+00,\n",
       "        8.62606266e-02,  4.15198956e-01,  4.98442410e-01, -5.37761597e-01,\n",
       "       -1.20520101e+00, -1.13710915e+00, -6.59722981e-01, -4.61622877e-01,\n",
       "        9.79431047e-01,  7.08585172e-01, -1.01729620e+00, -1.13611561e+00,\n",
       "        7.19882042e-01,  1.85753255e+00, -5.66562619e-01, -1.42907024e+00,\n",
       "       -1.65274502e+00, -1.15747813e-01,  9.91672008e-02,  1.03055562e+00,\n",
       "       -1.64221492e+00,  2.05763068e+00, -2.56929823e+00,  7.85430761e-02,\n",
       "       -2.75466940e+00, -5.84539363e-01, -1.63984744e-01, -3.80676353e-01,\n",
       "        1.95624249e+00,  1.70345415e-01, -1.26160681e+00, -1.61548122e+00,\n",
       "       -8.32527352e-02, -5.94322507e-01, -1.27867962e+00,  5.98755919e-01,\n",
       "        1.62920096e+00, -1.44040656e+00,  1.08236223e+00, -2.66479838e-01,\n",
       "        1.63859324e+00, -1.74873323e-01,  1.54814026e+00,  1.17597607e+00,\n",
       "        1.35302079e+00,  7.78074987e-01,  3.28567882e+00,  1.84762997e+00,\n",
       "        3.32296035e-01,  1.82446759e+00, -3.53985316e-01, -2.17733662e+00,\n",
       "       -1.34298582e+00, -3.08889922e+00, -1.42204216e-01,  5.42597832e-01,\n",
       "       -9.57087020e-01,  5.14289913e-01,  6.15424550e-01, -1.67098265e-01,\n",
       "       -8.12221410e-01, -2.05723570e-01,  4.63872684e-01, -1.08724221e+00,\n",
       "        3.09349468e-01, -2.82193602e+00,  2.87569028e-01,  6.95764417e-01,\n",
       "       -1.17024027e+00, -2.86958866e+00,  1.30798503e+00, -1.76611095e+00,\n",
       "        2.10508720e+00, -6.62218154e-01, -2.80313200e+00, -9.01863532e-02,\n",
       "        4.11104895e+00,  3.32299838e-01,  8.18393217e-01,  2.29702159e-01,\n",
       "       -9.21297337e-01,  2.66674112e-01,  1.53976315e+00, -1.20156723e+00,\n",
       "       -7.02824641e-01, -2.17315579e-02,  2.38082683e-01,  8.87372972e-01,\n",
       "        2.31748150e+00,  1.73541509e+00, -3.68527398e+00, -4.03775594e-01,\n",
       "        1.74377023e+00,  1.18632791e+00,  5.63058614e-01,  8.99596926e-01,\n",
       "        5.27105703e-01,  8.64436802e-01,  5.26340472e-03, -2.17649929e-01,\n",
       "        1.65743548e+00, -6.08410644e-01, -1.11949530e+00, -2.15546253e+00,\n",
       "       -1.34228401e+00, -2.18393996e+00,  1.87448469e+00,  1.21739417e+00,\n",
       "       -7.07873372e-01,  1.02100749e-01, -2.20846534e+00, -4.87494622e-01,\n",
       "        7.05138816e-01,  2.08219933e+00,  1.23224026e-01,  9.16752811e-01,\n",
       "        8.52226765e-01, -6.63847013e-01,  1.00772443e+00,  1.49246591e+00,\n",
       "       -6.47708071e-01, -1.90595963e+00, -2.46991214e+00, -2.19778659e+00,\n",
       "       -8.70971470e-01, -5.56545272e-02, -2.96860380e-01,  6.60292969e-01,\n",
       "       -1.10994549e+00,  1.72842832e-01, -7.74296783e-01,  5.30756227e-02,\n",
       "       -4.40770815e-01,  8.99063989e-01,  7.21476239e-01, -1.20376129e+00,\n",
       "        5.67012176e-01, -2.54384665e+00,  6.56247949e-01,  3.75476704e-01,\n",
       "       -1.60708201e+00, -1.09388633e+00, -9.74294109e-01,  4.37457085e-01,\n",
       "        9.66083330e-02, -2.26582406e+00,  4.12299641e-01, -2.51909335e+00,\n",
       "        3.52094564e+00,  2.73591996e-01, -1.86047153e+00,  1.39799713e+00,\n",
       "       -7.08549824e-01, -4.46750174e-02, -2.03775233e-01,  1.49421349e+00,\n",
       "       -1.35233676e+00,  1.60254189e+00,  1.09645992e+00,  7.91542614e-01,\n",
       "        1.68371097e+00, -1.30475499e+00, -1.44171668e+00, -1.45109134e+00,\n",
       "       -1.36317211e+00,  1.06909485e+00, -1.07901798e+00, -2.74204980e-01,\n",
       "       -1.91685147e+00, -1.88006396e+00,  1.66656842e-01,  2.78838851e+00,\n",
       "        1.10033130e+00,  1.01545347e-01,  2.59105533e+00, -2.50196448e+00,\n",
       "        1.76116530e-01,  2.32385743e+00,  1.92074370e+00,  1.91828095e+00,\n",
       "       -1.10651456e+00,  6.49842954e-01, -1.35461708e+00, -1.10663825e-01,\n",
       "        1.85675622e-01, -1.88465076e-01,  2.73124013e-01, -8.24189863e-02,\n",
       "       -6.73801658e-01,  2.94731682e-01, -1.83029973e+00,  1.50096592e+00,\n",
       "       -7.29662726e-03,  1.89332610e+00,  1.24892995e+00, -1.85086565e+00,\n",
       "       -3.89937046e+00,  4.03489439e-01, -7.45697393e-01, -3.57439990e-01,\n",
       "        1.60210832e+00,  1.03600199e-01,  1.42626886e+00, -7.72361350e-02,\n",
       "        1.24209665e+00,  4.33458589e+00,  3.11159260e+00,  1.16459602e+00,\n",
       "        1.24675497e+00, -2.16846758e+00,  2.87291458e-01,  1.50374453e+00,\n",
       "       -1.65664237e+00,  6.87693177e-01,  8.12787855e-01, -1.50196641e-02,\n",
       "       -3.78258125e-01, -2.21603893e+00,  4.29642058e-01,  5.79907377e-01,\n",
       "        6.13420526e-01, -6.29024197e-01, -1.58631465e-01,  7.08892060e-01,\n",
       "        2.77624264e+00,  1.02410194e+00,  1.71450248e+00,  1.58539266e+00,\n",
       "        2.15115849e+00, -1.24033993e+00, -1.63436324e+00, -1.55968347e-02,\n",
       "       -1.51300579e+00, -3.70410255e-01, -6.97573604e-01,  8.93375907e-01,\n",
       "       -9.00048171e-01,  1.25225933e+00, -9.34773770e-01,  1.24417414e+00,\n",
       "        1.14301540e+00, -1.98515399e+00, -1.44004622e+00,  1.03731580e+00])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(average_embedding(train_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Création de modèle(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    # Initialisation du modèle avec les paramètres d'entrée, de la couche cachée et de sortie\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size) :\n",
    "        super().__init__()\n",
    "        self.intput_layer = nn.Linear(input_size, hidden_layer_size)    # Définition de la couche d'entrée (input layer) avec une transformation linéaire\n",
    "        self.output_layer = nn.Linear(hidden_layer_size, output_size) # Définition de la couche de sortie (output layer) avec une transformation linéaire\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.intput_layer(x)\n",
    "        x = nn.functional.relu_(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette classe MultiLayerPerceptron hérite de la classe \"nn.Module\" de PyTorch et définit un modèle de perceptron multicouche avec une couche d'entrée, une couche cachée avec une fonction d'activation ReLU, et une couche de sortie. La méthode \"forward\" spécifie comment les données passent à travers le modèle lors de l'inférence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import FloatTensor, LongTensor\n",
    "from typing import List\n",
    "\n",
    "class SpacyDataset(Dataset):\n",
    "    # Initialisation de la classe avec les données du dataset, les cibles, et une fonction d'agrégation des phrases\n",
    "    def __init__(self, dataset: List[str] , target: np.array, sentence_aggregation_function):\n",
    "        self.dataset = dataset\n",
    "        self.doc_embeddings = [None for _ in range(len(dataset))]  # Initialisation d'une liste vide pour stocker les embeddings des phrases (spaCy)\n",
    "        self.sentence_aggregation_function = sentence_aggregation_function # Stockage de la fonction d'agrégation des phrases\n",
    "        self.target = target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.doc_embeddings[index] is None:\n",
    "            self.doc_embeddings[index] = self.sentence_aggregation_function(self.dataset[index])  \n",
    "        return FloatTensor(self.doc_embeddings[index]), LongTensor([self.target[index]]).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un dictionnaire pour choisir le type d'agrégation\n",
    "aggregation = {\n",
    "    \"average\" : average_embedding,\n",
    "    \"maxpool\" : maxpool_embedding,\n",
    "    \"minpool\" : minpool_embedding\n",
    "}\n",
    "\n",
    "# On détermine ici comment la classe SpacyDataset construit la représentation d'un texte\n",
    "# par l'agrégation des représentations de mots. \n",
    "# Choix possibles: \"average\", \"maxpool\", \"minpool\"\n",
    "aggregation_function = aggregation[\"average\"]  \n",
    "\n",
    "# On finalise la construction des 3 jeux de données et leurs dataloaders\n",
    "train_dataset = SpacyDataset(train_list, train_target, aggregation_function)\n",
    "valid_dataset = SpacyDataset(dev_list, dev_target, aggregation_function)\n",
    "test_dataset = SpacyDataset(test_list, test_target, aggregation_function)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code utilise un dictionnaire appelé 'aggregation' pour associer des chaînes de caractères à des fonctions d'agrégation, telles que 'average_embedding, maxpool_embedding, et minpool_embedding'. Il choisit ensuite une fonction d'agrégation spécifique (dans cet exemple, \"average\") pour être utilisée dans la construction des jeux de données (train_dataset, valid_dataset, test_dataset) en utilisant la classe SpacyDataset, et leurs DataLoader respectifs (train_dataloader, valid_dataloader, test_dataloader). Les DataLoader sont utilisés pour faciliter l'itération sur les jeux de données pendant l'entraînement et l'évaluation du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fonctions utilitaires\n",
    "\n",
    "Vous pouvez mettre ici toutes les fonctions qui seront utiles pour les sections suivantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement de modèle(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des plongements de Spacy: 300\n",
      "Nombre de classes: 9\n"
     ]
    }
   ],
   "source": [
    "embedding_size = spacy_model.meta['vectors']['width'] # La dimension des vecteurs d'embeddings de Spacy\n",
    "nb_classes = len([i for i in range(9)])\n",
    "\n",
    "print(\"Taille des plongements de Spacy:\", embedding_size)\n",
    "print(\"Nombre de classes:\", nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code utilise les métadonnées du modèle spaCy pour obtenir la dimension des vecteurs d'embedding (embedding_size). Ensuite, il définit le nombre de classes (nb_classes) en comptant le nombre d'entiers dans la plage de 0 à 8. Enfin, il affiche ces informations. Notez que le nombre de classes dans cet exemple est fixé à 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLayerPerceptron(\n",
       "  (intput_layer): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (output_layer): Linear(in_features=100, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from poutyne.framework import Experiment\n",
    "from poutyne import set_seeds\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "\n",
    "set_seeds(42) # Fixation des graines pour assurer la reproductibilité\n",
    "hidden_size = 100 # Définition de la taille de la couche cachée dans le modèle MLP\n",
    "optimizer = \"SGD\"\n",
    "\n",
    "directory_name = 'model/{}_mlp_optimizer{}'.format(aggregation_function.__name__, optimizer)  \n",
    "\n",
    "# Création du modèle MLP\n",
    "model = MultiLayerPerceptron(embedding_size, hidden_size, nb_classes)\n",
    "\n",
    "# Création d'une instance de la classe Experiment de Poutyne pour gérer l'entraînement et l'évaluation\n",
    "experiment = Experiment(directory_name, \n",
    "                        model, \n",
    "                        optimizer = optimizer, \n",
    "                        task=\"classification\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code utilise la bibliothèque Poutyne pour définir une expérience d'entraînement. Il crée un modèle MLP avec les paramètres spécifiés, puis utilise la classe 'Experiment' de Poutyne pour gérer l'entraînement et l'évaluation du modèle. Le répertoire spécifié (directory_name) sera utilisé pour sauvegarder le modèle et les résultats de l'expérienc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/50 Train steps: 155 Val steps: 34 0.21s loss: 1.683572 acc: 40.363636 fscore_macro: 0.094113 val_loss: 1.606964 val_acc: 40.677966 val_fscore_macro: 0.081529\n",
      "Epoch 1: val_acc improved from -inf to 40.67797, saving file to model/average_embedding_mlp_optimizerSGD/checkpoint_epoch_1.ckpt\n",
      "Epoch:  2/50 Train steps: 155 Val steps: 34 0.21s loss: 1.549883 acc: 44.080808 fscore_macro: 0.142711 val_loss: 1.504527 val_acc: 50.847458 val_fscore_macro: 0.191354\n",
      "Epoch 2: val_acc improved from 40.67797 to 50.84746, saving file to model/average_embedding_mlp_optimizerSGD/checkpoint_epoch_2.ckpt\n",
      "Epoch:  3/50 Train steps: 155 Val steps: 34 0.26s loss: 1.443486 acc: 48.848485 fscore_macro: 0.183284 val_loss: 1.437503 val_acc: 49.905838 val_fscore_macro: 0.193641\n",
      "Epoch:  4/50 Train steps: 155 Val steps: 34 0.21s loss: 1.377818 acc: 52.121212 fscore_macro: 0.209421 val_loss: 1.319665 val_acc: 51.977401 val_fscore_macro: 0.191257\n",
      "Epoch 4: val_acc improved from 50.84746 to 51.97740, saving file to model/average_embedding_mlp_optimizerSGD/checkpoint_epoch_4.ckpt\n",
      "Epoch:  5/50 Train steps: 155 Val steps: 34 0.20s loss: 1.291252 acc: 54.424242 fscore_macro: 0.237959 val_loss: 1.362467 val_acc: 49.529190 val_fscore_macro: 0.170599\n",
      "Epoch:  6/50 Train steps: 155 Val steps: 34 0.22s loss: 1.260844 acc: 55.272727 fscore_macro: 0.260923 val_loss: 1.279538 val_acc: 56.308851 val_fscore_macro: 0.269084\n",
      "Epoch 6: val_acc improved from 51.97740 to 56.30885, saving file to model/average_embedding_mlp_optimizerSGD/checkpoint_epoch_6.ckpt\n",
      "Epoch:  7/50 Train steps: 155 Val steps: 34 0.25s loss: 1.219186 acc: 57.575758 fscore_macro: 0.292939 val_loss: 1.250461 val_acc: 58.003767 val_fscore_macro: 0.277746\n",
      "Epoch 7: val_acc improved from 56.30885 to 58.00377, saving file to model/average_embedding_mlp_optimizerSGD/checkpoint_epoch_7.ckpt\n",
      "Epoch:  8/50 Train steps: 155 Val steps: 34 0.23s loss: 1.182093 acc: 57.858586 fscore_macro: 0.304071 val_loss: 1.188359 val_acc: 57.815443 val_fscore_macro: 0.267697\n",
      "Epoch:  9/50 Train steps: 155 Val steps: 34 0.30s loss: 1.145971 acc: 59.878788 fscore_macro: 0.325970 val_loss: 1.139805 val_acc: 60.451977 val_fscore_macro: 0.316266\n",
      "Epoch 9: val_acc improved from 58.00377 to 60.45198, saving file to model/average_embedding_mlp_optimizerSGD/checkpoint_epoch_9.ckpt\n",
      "Epoch: 10/50 Train steps: 155 Val steps: 34 0.23s loss: 1.108806 acc: 60.525253 fscore_macro: 0.338916 val_loss: 1.084109 val_acc: 64.595104 val_fscore_macro: 0.366328\n",
      "Epoch 10: val_acc improved from 60.45198 to 64.59510, saving file to model/average_embedding_mlp_optimizerSGD/checkpoint_epoch_10.ckpt\n",
      "Epoch: 11/50 Train steps: 155 Val steps: 34 0.32s loss: 1.074954 acc: 62.424242 fscore_macro: 0.373373 val_loss: 1.062727 val_acc: 62.711864 val_fscore_macro: 0.338242\n",
      "Epoch: 12/50 Train steps: 155 Val steps: 34 0.24s loss: 1.047099 acc: 63.515152 fscore_macro: 0.369867 val_loss: 1.022339 val_acc: 64.218456 val_fscore_macro: 0.352256\n",
      "Epoch: 13/50 Train steps: 155 Val steps: 34 0.19s loss: 1.038001 acc: 64.040404 fscore_macro: 0.391070 val_loss: 1.046904 val_acc: 62.900188 val_fscore_macro: 0.332340\n",
      "Epoch: 14/50 Train steps: 155 Val steps: 34 0.24s loss: 1.027746 acc: 63.353535 fscore_macro: 0.376947 val_loss: 1.044632 val_acc: 62.146893 val_fscore_macro: 0.341907\n",
      "Epoch: 15/50 Train steps: 155 Val steps: 34 0.23s loss: 0.998310 acc: 63.959596 fscore_macro: 0.397360 val_loss: 1.035921 val_acc: 63.088512 val_fscore_macro: 0.372604\n",
      "Epoch: 16/50 Train steps: 155 Val steps: 34 0.24s loss: 0.986548 acc: 64.686869 fscore_macro: 0.408108 val_loss: 1.033684 val_acc: 63.276836 val_fscore_macro: 0.355689\n",
      "Epoch: 17/50 Train steps: 155 Val steps: 34 0.21s loss: 0.965747 acc: 64.767677 fscore_macro: 0.413999 val_loss: 1.024620 val_acc: 65.348399 val_fscore_macro: 0.424241\n",
      "Epoch 17: val_acc improved from 64.59510 to 65.34840, saving file to model/average_embedding_mlp_optimizerSGD/checkpoint_epoch_17.ckpt\n",
      "Epoch: 18/50 Train steps: 155 Val steps: 34 0.19s loss: 0.967092 acc: 65.535354 fscore_macro: 0.424650 val_loss: 0.985130 val_acc: 64.030132 val_fscore_macro: 0.396444\n",
      "Epoch: 19/50 Train steps: 155 Val steps: 34 0.19s loss: 0.962975 acc: 65.777778 fscore_macro: 0.426974 val_loss: 0.978998 val_acc: 68.173258 val_fscore_macro: 0.447895\n",
      "Epoch 19: val_acc improved from 65.34840 to 68.17326, saving file to model/average_embedding_mlp_optimizerSGD/checkpoint_epoch_19.ckpt\n",
      "Epoch: 20/50 Train steps: 155 Val steps: 34 0.20s loss: 0.951393 acc: 65.818182 fscore_macro: 0.435758 val_loss: 0.953198 val_acc: 68.549906 val_fscore_macro: 0.475101\n",
      "Epoch 20: val_acc improved from 68.17326 to 68.54991, saving file to model/average_embedding_mlp_optimizerSGD/checkpoint_epoch_20.ckpt\n",
      "Epoch: 21/50 Train steps: 155 Val steps: 34 0.20s loss: 0.937792 acc: 66.909091 fscore_macro: 0.443918 val_loss: 1.223180 val_acc: 58.003766 val_fscore_macro: 0.374327\n",
      "Epoch: 22/50 Train steps: 155 Val steps: 34 0.31s loss: 0.928719 acc: 66.747475 fscore_macro: 0.438847 val_loss: 1.209447 val_acc: 55.743880 val_fscore_macro: 0.425587\n",
      "Epoch: 23/50 Train steps: 155 Val steps: 34 0.23s loss: 0.917892 acc: 67.717172 fscore_macro: 0.457023 val_loss: 0.998681 val_acc: 65.725047 val_fscore_macro: 0.449516\n",
      "Epoch: 24/50 Train steps: 155 Val steps: 34 0.19s loss: 0.892787 acc: 67.838384 fscore_macro: 0.452768 val_loss: 1.008661 val_acc: 65.913371 val_fscore_macro: 0.434348\n",
      "Epoch: 25/50 Train steps: 155 Val steps: 34 0.20s loss: 0.915894 acc: 67.111111 fscore_macro: 0.450662 val_loss: 0.995690 val_acc: 66.854991 val_fscore_macro: 0.446138\n",
      "Epoch: 26/50 Train steps: 155 Val steps: 34 0.20s loss: 0.871074 acc: 69.737374 fscore_macro: 0.472302 val_loss: 0.979829 val_acc: 66.854991 val_fscore_macro: 0.400853\n",
      "Epoch: 27/50 Train steps: 155 Val steps: 34 0.20s loss: 0.872525 acc: 68.808081 fscore_macro: 0.464829 val_loss: 0.919390 val_acc: 67.608286 val_fscore_macro: 0.465568\n",
      "Epoch: 28/50 Train steps: 155 Val steps: 34 0.18s loss: 0.863450 acc: 69.696970 fscore_macro: 0.494087 val_loss: 0.962428 val_acc: 66.101695 val_fscore_macro: 0.471621\n",
      "Epoch: 29/50 Train steps: 155 Val steps: 34 0.19s loss: 0.870446 acc: 69.333333 fscore_macro: 0.483123 val_loss: 1.024890 val_acc: 64.783428 val_fscore_macro: 0.434152\n",
      "Epoch: 30/50 Train steps: 155 Val steps: 34 0.23s loss: 0.861396 acc: 67.797980 fscore_macro: 0.468210 val_loss: 1.453498 val_acc: 47.080979 val_fscore_macro: 0.360275\n",
      "Epoch: 31/50 Train steps: 155 Val steps: 34 0.17s loss: 0.869252 acc: 68.848485 fscore_macro: 0.518718 val_loss: 0.993822 val_acc: 64.595104 val_fscore_macro: 0.379346\n",
      "Epoch: 32/50 Train steps: 155 Val steps: 34 0.19s loss: 0.847559 acc: 69.898990 fscore_macro: 0.497651 val_loss: 0.925445 val_acc: 68.549906 val_fscore_macro: 0.469268\n",
      "Epoch: 33/50 Train steps: 155 Val steps: 34 0.19s loss: 0.829783 acc: 71.030303 fscore_macro: 0.531160 val_loss: 0.993230 val_acc: 66.666667 val_fscore_macro: 0.490516\n",
      "Epoch: 34/50 Train steps: 155 Val steps: 34 0.21s loss: 0.834800 acc: 69.737374 fscore_macro: 0.524986 val_loss: 1.035610 val_acc: 64.783427 val_fscore_macro: 0.452958\n",
      "Epoch: 35/50 Train steps: 155 Val steps: 34 0.21s loss: 0.824502 acc: 69.939394 fscore_macro: 0.546754 val_loss: 1.027918 val_acc: 63.088512 val_fscore_macro: 0.416416\n",
      "Epoch: 36/50 Train steps: 155 Val steps: 34 0.28s loss: 0.817685 acc: 70.949495 fscore_macro: 0.547376 val_loss: 0.917182 val_acc: 68.549906 val_fscore_macro: 0.488315\n",
      "Epoch: 37/50 Train steps: 155 Val steps: 34 0.26s loss: 0.805445 acc: 72.080808 fscore_macro: 0.571616 val_loss: 0.993320 val_acc: 64.218456 val_fscore_macro: 0.429309\n",
      "Epoch: 38/50 Train steps: 155 Val steps: 34 0.22s loss: 0.817928 acc: 71.030303 fscore_macro: 0.545593 val_loss: 0.938892 val_acc: 66.854991 val_fscore_macro: 0.466095\n",
      "Epoch: 39/50 Train steps: 155 Val steps: 34 0.22s loss: 0.811601 acc: 70.666667 fscore_macro: 0.563779 val_loss: 0.938263 val_acc: 66.854991 val_fscore_macro: 0.451491\n",
      "Epoch: 40/50 Train steps: 155 Val steps: 34 0.19s loss: 0.815257 acc: 70.626263 fscore_macro: 0.534907 val_loss: 0.967872 val_acc: 66.478343 val_fscore_macro: 0.459716\n",
      "Epoch: 41/50 Train steps: 155 Val steps: 34 0.23s loss: 0.793977 acc: 71.353535 fscore_macro: 0.565887 val_loss: 0.956900 val_acc: 67.043315 val_fscore_macro: 0.454023\n",
      "Epoch: 42/50 Train steps: 155 Val steps: 34 0.17s loss: 0.781032 acc: 72.444444 fscore_macro: 0.572048 val_loss: 1.029533 val_acc: 64.595104 val_fscore_macro: 0.427798\n",
      "Epoch: 43/50 Train steps: 155 Val steps: 34 0.17s loss: 0.787311 acc: 72.121212 fscore_macro: 0.576985 val_loss: 0.953164 val_acc: 65.913371 val_fscore_macro: 0.468317\n",
      "Epoch: 44/50 Train steps: 155 Val steps: 34 0.19s loss: 0.780013 acc: 72.363636 fscore_macro: 0.571993 val_loss: 1.042247 val_acc: 64.030132 val_fscore_macro: 0.445688\n",
      "Epoch: 45/50 Train steps: 155 Val steps: 34 0.17s loss: 0.773051 acc: 72.808081 fscore_macro: 0.586379 val_loss: 0.892524 val_acc: 70.056497 val_fscore_macro: 0.539081\n",
      "Epoch 45: val_acc improved from 68.54991 to 70.05650, saving file to model/average_embedding_mlp_optimizerSGD/checkpoint_epoch_45.ckpt\n",
      "Epoch: 46/50 Train steps: 155 Val steps: 34 0.18s loss: 0.772816 acc: 72.242424 fscore_macro: 0.576572 val_loss: 0.924253 val_acc: 67.419962 val_fscore_macro: 0.484376\n",
      "Epoch: 47/50 Train steps: 155 Val steps: 34 0.18s loss: 0.772765 acc: 72.121212 fscore_macro: 0.579875 val_loss: 0.951206 val_acc: 67.419962 val_fscore_macro: 0.515132\n",
      "Epoch: 48/50 Train steps: 155 Val steps: 34 0.20s loss: 0.752015 acc: 73.010101 fscore_macro: 0.612310 val_loss: 0.994724 val_acc: 66.101695 val_fscore_macro: 0.429585\n",
      "Epoch: 49/50 Train steps: 155 Val steps: 34 0.20s loss: 0.764506 acc: 73.010101 fscore_macro: 0.625477 val_loss: 0.951741 val_acc: 69.679849 val_fscore_macro: 0.484358\n",
      "Epoch: 50/50 Train steps: 155 Val steps: 34 0.17s loss: 0.742550 acc: 73.373737 fscore_macro: 0.582926 val_loss: 0.924539 val_acc: 69.868173 val_fscore_macro: 0.552072\n",
      "Restoring data from model/average_embedding_mlp_optimizerSGD/checkpoint_epoch_45.ckpt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging = experiment.train(train_dataloader, valid_dataloader, epochs=50, disable_tensorboard=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Évaluation et analyse de résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found best checkpoint at epoch: 38\n",
      "lr: 0.001, loss: 0.581567, acc: 79.5152, fscore_macro: 0.799569, val_loss: 0.92777, val_acc: 70.9981, val_fscore_macro: 0.548251\n",
      "Loading checkpoint model/average_embedding_mlp_optimizerADAM/checkpoint_epoch_38.ckpt\n",
      "Running test\n",
      "Test steps: 34 0.03s test_loss: 0.930146 test_acc: 68.926554 test_fscore_macro: 0.636161    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'time': 0.026028541033156216,\n",
       " 'test_loss': 0.930146022252528,\n",
       " 'test_acc': 68.92655367231639,\n",
       " 'test_fscore_macro': 0.6361610293388367}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Évaluation du modèle sur l'ensemble de test à l'aide du DataLoader test_dataloader\n",
    "experiment.test(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
