{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65179fa",
   "metadata": {},
   "source": [
    "# *Notebook* à utiliser pour faire le travail pratique # 3 sur l'analyse d'incidents.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du jeu de donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2233dd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "train_data_path = './data/dev_examples.json'\n",
    "new_exemples_path = './data/new_examples.json'\n",
    "test_data_path = './data/test_examples.json'\n",
    "\n",
    "def load_incident_dataset(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        incident_list = json.load(fp)\n",
    "\n",
    "    return incident_list\n",
    "\n",
    "# Load datasets\n",
    "train_data = load_incident_dataset(train_data_path)\n",
    "new_examples = load_incident_dataset(new_exemples_path)\n",
    "test_data_path = load_incident_dataset(test_data_path)\n",
    "\n",
    "# Merge datasets\n",
    "merged_data = train_data\n",
    "\n",
    "# merged_data = merged_data[:20]\n",
    "\n",
    "len(merged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer from transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode texts from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34d8bd93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_texts(tokenizer, texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "texts = [incident['text'] for incident in merged_data]\n",
    "encoded_texts = encode_texts(tokenizer, texts)\n",
    "encoded_texts.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceb24f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-EVENT': 0, 'I-EVENT': 1, 'B-ACTIVITY': 2, 'I-ACTIVITY': 3, 'B-WHO': 4, 'I-WHO': 5, 'B-WHERE': 6, 'I-WHERE': 7, 'B-WHEN': 8, 'I-WHEN': 9, 'B-CAUSE': 10, 'I-CAUSE': 11, 'B-EQUIPMENT': 12, 'I-EQUIPMENT': 13, 'B-INJURY': 14, 'I-INJURY': 15, 'B-INJURED': 16, 'I-INJURED': 17, 'B-BODY-PARTS': 18, 'I-BODY-PARTS': 19, 'B-DEATH': 20, 'I-DEATH': 21, 'O': 22}\n"
     ]
    }
   ],
   "source": [
    "slot_labels = [\"B-EVENT\", \"I-EVENT\", \"B-ACTIVITY\", \"I-ACTIVITY\", \"B-WHO\", \"I-WHO\", \"B-WHERE\", \"I-WHERE\", \"B-WHEN\", \"I-WHEN\", \"B-CAUSE\", \"I-CAUSE\", \"B-EQUIPMENT\", \"I-EQUIPMENT\", \"B-INJURY\", \"I-INJURY\", \"B-INJURED\", \"I-INJURED\", \"B-BODY-PARTS\", \"I-BODY-PARTS\", \"B-DEATH\", \"I-DEATH\", \"O\"]\n",
    "slot_label_to_id = {label: i for i, label in enumerate(slot_labels)}\n",
    "\n",
    "print(slot_label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a0b0d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I-CAUSE']\n"
     ]
    }
   ],
   "source": [
    "def get_slot_from_word(word, data):\n",
    "    found_in = []\n",
    "    for argument, values in data['arguments'].items():\n",
    "        # Check if the word is in any of the values for this argument\n",
    "        for value in values:\n",
    "            if word in value:\n",
    "                # find index of word in value\n",
    "                word_index = value.index(word)\n",
    "                found_in.append(\"B-\" + argument) if word_index == 0 else found_in.append('I-' + argument)\n",
    "\n",
    "    return found_in\n",
    "\n",
    "print(get_slot_from_word('driver', train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def align_tokens_with_all_slots_bert(data, slot_label_to_id, tokenizer):\n",
    "    number_slots = len(slot_label_to_id)\n",
    "    other_array = np.zeros(number_slots)\n",
    "    other_array[-1] = 1 # O slot\n",
    "\n",
    "    aligned_token_slots = []\n",
    "    words = data[\"text\"].split()\n",
    "\n",
    "    aligned_token_slots.append(other_array) # [CLS] token\n",
    "\n",
    "    for word in words:\n",
    "        tokens = tokenizer.tokenize(word)\n",
    "        expected_slots = get_slot_from_word(word, data)  # This can now be a list of slots\n",
    "\n",
    "        for bert_token in tokens:\n",
    "            # Here, each token is represented by a list, with the first element being the token\n",
    "            # and the subsequent elements being flags for each slot\n",
    "            token_with_slots = []\n",
    "\n",
    "            # Adding flags for each slot\n",
    "            for slot_label in slot_label_to_id.keys():\n",
    "                slot_flag = 1 if slot_label in expected_slots else 0\n",
    "                token_with_slots.append(slot_flag)\n",
    "\n",
    "            # if token_with_slots does not contain any slot, then it is an O token\n",
    "            if sum(token_with_slots) == 0:\n",
    "                token_with_slots[-1] = 1 # -1 is the index of the O slot (last slot)\n",
    "\n",
    "            aligned_token_slots.append(token_with_slots)\n",
    "\n",
    "    aligned_token_slots.append(other_array) # [SEP] token\n",
    "\n",
    "    return aligned_token_slots\n",
    "\n",
    "\n",
    "exple = {\n",
    "        \"text\": \"John had an accident at the construction site while walking.\",\n",
    "        \"arguments\": {\n",
    "            \"EVENT\": [\"accident\"],\n",
    "            \"ACTIVITY\": [\"walking\"],\n",
    "            \"WHO\": [\"John\"],\n",
    "            \"WHERE\": [\"construction site\"]\n",
    "        }\n",
    "    }\n",
    "encoded_slots_matrix = align_tokens_with_all_slots_bert(exple, slot_label_to_id, tokenizer)\n",
    "print(encoded_slots_matrix[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "torch.Size([512, 23])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class SlotDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, slot_label_to_id, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.slot_label_to_id = slot_label_to_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx][\"text\"]\n",
    "        arguments = self.texts[idx][\"arguments\"]\n",
    "\n",
    "        # Tokenize text and align labels\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        labels = align_tokens_with_all_slots_bert(self.texts[idx], self.slot_label_to_id, self.tokenizer)\n",
    "\n",
    "        # Adjust the labels to match the length of the tokenized input\n",
    "        # Truncate or pad the labels\n",
    "        padded_labels = []\n",
    "        for label in labels:\n",
    "            if len(padded_labels) < self.max_len:\n",
    "                padded_labels.append(label)\n",
    "            else:\n",
    "                break\n",
    "        while len(padded_labels) < self.max_len:\n",
    "            padded_labels.append([0] * len(self.slot_label_to_id))  # Padding\n",
    "\n",
    "        padded_labels = np.array(padded_labels)\n",
    "        padded_labels = torch.tensor(padded_labels, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': padded_labels\n",
    "        }\n",
    "\n",
    "# Your dataset\n",
    "# texts = [exple]\n",
    "\n",
    "# Create dataset\n",
    "dataset = SlotDataset(merged_data, tokenizer, slot_label_to_id)\n",
    "eval_dataset = SlotDataset(new_examples, tokenizer, slot_label_to_id)\n",
    "\n",
    "print(dataset[0][\"input_ids\"].shape)\n",
    "print(dataset[0][\"labels\"].shape)\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46fa5340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, BertTokenizer, AdamW\n",
    "import torch\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "num_labels = len(slot_label_to_id)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b87bef1a5648beb46d42effd0265ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 17.73 GB, other allocations: 399.22 MB, max allowed: 18.13 GB). Tried to allocate 12.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/admin/Library/CloudStorage/OneDrive-UniversitéLaval/Ulaval/A23/NLP/Travaux-pratiques-NLP/tp3_2023/incident_analysis.ipynb Cellule 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/admin/Library/CloudStorage/OneDrive-Universit%C3%A9Laval/Ulaval/A23/NLP/Travaux-pratiques-NLP/tp3_2023/incident_analysis.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/admin/Library/CloudStorage/OneDrive-Universit%C3%A9Laval/Ulaval/A23/NLP/Travaux-pratiques-NLP/tp3_2023/incident_analysis.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m'\u001b[39m,          \u001b[39m# output directory\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/admin/Library/CloudStorage/OneDrive-Universit%C3%A9Laval/Ulaval/A23/NLP/Travaux-pratiques-NLP/tp3_2023/incident_analysis.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,              \u001b[39m# total number of training epochs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Library/CloudStorage/OneDrive-Universit%C3%A9Laval/Ulaval/A23/NLP/Travaux-pratiques-NLP/tp3_2023/incident_analysis.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     optim\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madamw_torch\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Library/CloudStorage/OneDrive-Universit%C3%A9Laval/Ulaval/A23/NLP/Travaux-pratiques-NLP/tp3_2023/incident_analysis.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Library/CloudStorage/OneDrive-Universit%C3%A9Laval/Ulaval/A23/NLP/Travaux-pratiques-NLP/tp3_2023/incident_analysis.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Library/CloudStorage/OneDrive-Universit%C3%A9Laval/Ulaval/A23/NLP/Travaux-pratiques-NLP/tp3_2023/incident_analysis.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,                         \u001b[39m# the instantiated Transformers model to be trained\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Library/CloudStorage/OneDrive-Universit%C3%A9Laval/Ulaval/A23/NLP/Travaux-pratiques-NLP/tp3_2023/incident_analysis.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,                  \u001b[39m# training arguments, defined above\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Library/CloudStorage/OneDrive-Universit%C3%A9Laval/Ulaval/A23/NLP/Travaux-pratiques-NLP/tp3_2023/incident_analysis.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mdataset,         \u001b[39m# training dataset\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Library/CloudStorage/OneDrive-Universit%C3%A9Laval/Ulaval/A23/NLP/Travaux-pratiques-NLP/tp3_2023/incident_analysis.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39meval_dataset            \u001b[39m# evaluation dataset\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Library/CloudStorage/OneDrive-Universit%C3%A9Laval/Ulaval/A23/NLP/Travaux-pratiques-NLP/tp3_2023/incident_analysis.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/admin/Library/CloudStorage/OneDrive-Universit%C3%A9Laval/Ulaval/A23/NLP/Travaux-pratiques-NLP/tp3_2023/incident_analysis.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[39m=\u001b[39margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1837\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1834\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1836\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1837\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1839\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1840\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1841\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1842\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1843\u001b[0m ):\n\u001b[1;32m   1844\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1845\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2682\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2679\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2681\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2682\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   2684\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2685\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2707\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2705\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2706\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2707\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[1;32m   2708\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2709\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2710\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1756\u001b[0m, in \u001b[0;36mBertForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m \u001b[39m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1756\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert(\n\u001b[1;32m   1757\u001b[0m     input_ids,\n\u001b[1;32m   1758\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   1759\u001b[0m     token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1760\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   1761\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[1;32m   1762\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m   1763\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1764\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1765\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1766\u001b[0m )\n\u001b[1;32m   1768\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1770\u001b[0m sequence_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m   1023\u001b[0m     embedding_output,\n\u001b[1;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[1;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[1;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1033\u001b[0m )\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    605\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    613\u001b[0m         hidden_states,\n\u001b[1;32m    614\u001b[0m         attention_mask,\n\u001b[1;32m    615\u001b[0m         layer_head_mask,\n\u001b[1;32m    616\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    617\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    618\u001b[0m         past_key_value,\n\u001b[1;32m    619\u001b[0m         output_attentions,\n\u001b[1;32m    620\u001b[0m     )\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\n\u001b[1;32m    498\u001b[0m         hidden_states,\n\u001b[1;32m    499\u001b[0m         attention_mask,\n\u001b[1;32m    500\u001b[0m         head_mask,\n\u001b[1;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39mself_attn_past_key_value,\n\u001b[1;32m    503\u001b[0m     )\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself(\n\u001b[1;32m    428\u001b[0m         hidden_states,\n\u001b[1;32m    429\u001b[0m         attention_mask,\n\u001b[1;32m    430\u001b[0m         head_mask,\n\u001b[1;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    433\u001b[0m         past_key_value,\n\u001b[1;32m    434\u001b[0m         output_attentions,\n\u001b[1;32m    435\u001b[0m     )\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:365\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m     attention_probs \u001b[39m=\u001b[39m attention_probs \u001b[39m*\u001b[39m head_mask\n\u001b[0;32m--> 365\u001b[0m context_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(attention_probs, value_layer)\n\u001b[1;32m    367\u001b[0m context_layer \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    368\u001b[0m new_context_layer_shape \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_head_size,)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 17.73 GB, other allocations: 399.22 MB, max allowed: 18.13 GB). Tried to allocate 12.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    optim='adamw_torch',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset            # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef3fdd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "# num_epochs = 5\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a19c165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 0.2874, Accuracy: 0.9387\n",
      "Epoch 2/5 - Loss: 0.1527, Accuracy: 0.9763\n",
      "Epoch 3/5 - Loss: 0.1316, Accuracy: 0.9763\n",
      "Epoch 4/5 - Loss: 0.1195, Accuracy: 0.9763\n",
      "Epoch 5/5 - Loss: 0.1103, Accuracy: 0.9763\n"
     ]
    }
   ],
   "source": [
    "# from torch.nn import BCEWithLogitsLoss\n",
    "# import torch\n",
    "\n",
    "# def calculate_accuracy(logits, labels):\n",
    "#     # Applying sigmoid to logits and rounding to get predictions\n",
    "#     preds = torch.sigmoid(logits) > 0.5\n",
    "#     correct_preds = (preds == labels).float()\n",
    "#     accuracy = correct_preds.sum() / correct_preds.numel()\n",
    "#     return accuracy.item()\n",
    "\n",
    "# model.train()\n",
    "# loss_fn = BCEWithLogitsLoss()\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     total_loss = 0\n",
    "#     total_accuracy = 0\n",
    "\n",
    "#     for batch in data_loader:\n",
    "#         # Forward pass\n",
    "#         outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "#         logits = outputs.logits\n",
    "\n",
    "#         # Convert labels to float\n",
    "#         labels = batch['labels'].float()\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = loss_fn(logits.view(-1, num_labels), labels.view(-1, num_labels))\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Calculate accuracy\n",
    "#         accuracy = calculate_accuracy(logits, labels)\n",
    "#         total_accuracy += accuracy\n",
    "\n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     # Compute average loss and accuracy\n",
    "#     avg_loss = total_loss / len(data_loader)\n",
    "#     avg_accuracy = total_accuracy / len(data_loader)\n",
    "\n",
    "#     # Print metrics\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_slots(text, model, tokenizer, slot_label_to_id, threshold=0.5):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    # Run model to get logits\n",
    "    model.eval()  # Put model in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Apply sigmoid and threshold\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs > threshold).int()\n",
    "\n",
    "    # Convert predictions to label names\n",
    "    id_to_label = {v: k for k, v in slot_label_to_id.items()}\n",
    "    predicted_labels = [[id_to_label.get(idx) for idx, val in enumerate(row) if val == 1] for row in preds.squeeze().tolist()]\n",
    "\n",
    "    # Tokenized text for reference\n",
    "    tokenized_text = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "\n",
    "    return list(zip(tokenized_text, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On August 27  2013  Employees #1 and #2  of Templar Inc.  a construction  company specializing in fiber optic installation and services  were working  along a highway. The highway speed limit was posted at 55 miles per hour.  Employee #1 was marking the location of an underground line that ran below the  turn lane. Employee #2 was next to Employee #1 and performing the duties of a  flagger. A privately owned vehicle was travelling in the travel/through lane.  The vehicle veered to the right  entered the turn lane  and struck both  workers. Emergency medical services were called. Employee #1 was declared dead  at the scene. Employee #2 refused emergency medical treatment for the bruises  he received when struck.\n",
      "[('[CLS]', []), ('on', []), ('august', []), ('27', []), ('2013', []), ('employees', []), ('#', []), ('1', []), ('and', []), ('#', []), ('2', []), ('of', []), ('templar', []), ('inc', []), ('.', []), ('a', []), ('construction', []), ('company', []), ('specializing', []), ('in', []), ('fiber', []), ('optic', []), ('installation', []), ('and', []), ('services', []), ('were', []), ('working', []), ('along', []), ('a', []), ('highway', []), ('.', ['O']), ('the', []), ('highway', []), ('speed', []), ('limit', []), ('was', []), ('posted', []), ('at', []), ('55', []), ('miles', []), ('per', []), ('hour', []), ('.', ['O']), ('employee', []), ('#', []), ('1', []), ('was', []), ('marking', []), ('the', []), ('location', []), ('of', []), ('an', []), ('underground', []), ('line', []), ('that', []), ('ran', []), ('below', []), ('the', []), ('turn', []), ('lane', []), ('.', []), ('employee', []), ('#', []), ('2', []), ('was', []), ('next', []), ('to', []), ('employee', []), ('#', []), ('1', []), ('and', []), ('performing', []), ('the', []), ('duties', []), ('of', []), ('a', []), ('flag', []), ('##ger', []), ('.', []), ('a', []), ('privately', []), ('owned', []), ('vehicle', []), ('was', []), ('travelling', []), ('in', []), ('the', []), ('travel', []), ('/', []), ('through', []), ('lane', []), ('.', []), ('the', []), ('vehicle', []), ('ve', []), ('##ered', []), ('to', []), ('the', []), ('right', []), ('entered', []), ('the', []), ('turn', []), ('lane', []), ('and', []), ('struck', []), ('both', []), ('workers', []), ('.', []), ('emergency', []), ('medical', []), ('services', []), ('were', []), ('called', []), ('.', []), ('employee', []), ('#', []), ('1', []), ('was', []), ('declared', []), ('dead', []), ('at', []), ('the', []), ('scene', []), ('.', []), ('employee', []), ('#', []), ('2', []), ('refused', []), ('emergency', []), ('medical', []), ('treatment', []), ('for', []), ('the', []), ('bruises', []), ('he', []), ('received', []), ('when', []), ('struck', []), ('.', []), ('[SEP]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', []), ('[PAD]', [])]\n"
     ]
    }
   ],
   "source": [
    "def construct_response(predicted_slots):\n",
    "    # Aggregate slot values\n",
    "    slot_values = {}\n",
    "    for token, slots in predicted_slots:\n",
    "        for slot in slots:\n",
    "            if slot not in slot_values:\n",
    "                slot_values[slot] = []\n",
    "            slot_values[slot].append(token)\n",
    "\n",
    "    # Format the response\n",
    "    response = \"Identified Slots:\\n\"\n",
    "    for slot, values in slot_values.items():\n",
    "        # Join the tokens for each slot and add to the response\n",
    "        token_str = \" \".join(values).replace(' ##', '')  # Handling subword tokens\n",
    "        response += f\"- {slot}: {token_str}\\n\"\n",
    "\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "text = test_data_path[0][\"text\"]\n",
    "print(text)\n",
    "\n",
    "predicted_slots = infer_slots(text, model, tokenizer, slot_label_to_id)\n",
    "response = construct_response(predicted_slots)\n",
    "print(predicted_slots)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
